{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from model import Graph_OurConvNet\n",
    "from util.helper import calculate_avg_accuracy, update_lr, print_results, load_variables\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "# parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "# parser.add_argument('--epochs', type=int, default=10000, help='Number of epochs to train.')\n",
    "# parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate.')\n",
    "# parser.add_argument('--l2', type=float, default=0.005, help='Weight decay (L2 loss on parameters).')\n",
    "# parser.add_argument('--hidden', type=int, default=50, help='Number of hidden units.')\n",
    "# parser.add_argument('--layers', type=int, default=6, help='Number of conv layers.')\n",
    "# parser.add_argument('--d_conv', type=float, default=0.5, help='Dropout Conv rate (1 - keep probability).')\n",
    "# parser.add_argument('--d_edge', type=float, default=0.5, help='Dropout Edge rate (1 - keep probability).')\n",
    "# parser.add_argument('--d_in', type=float, default=0.0075, help='Dropin rate (1 - keep probability).')\n",
    "# parser.add_argument('--dataset', default=\"cora\", help='Name of Dataset')\n",
    "# parser.add_argument('--save_path', default=\"model_states/state\", help='Name of Dataset')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# args.cuda = True\n",
    "\n",
    "np.random.seed(42)\n",
    "print('cuda available')\n",
    "dtypeFloat = torch.cuda.FloatTensor\n",
    "dtypeLong = torch.cuda.LongTensor\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, lr, l2, batch_iters, nb_classes, early_stopping, SAVE_PATH, verbose=False, max_iters = 1000):\n",
    "    ### optimization parameters\n",
    "    decay_rate = 1.25\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = net.update(lr, l2) \n",
    "    t_start = time.time()\n",
    "    t_start_total = time.time()\n",
    "    average_loss_old = torch.tensor(1e4).cuda()\n",
    "    best = running_train_acc = running_train_loss = running_val_loss = 0.0\n",
    "    tab_results = []\n",
    "\n",
    "    for iteration in range(1, max_iters):  # loop over the dataset multiple times\n",
    "        # forward, loss\n",
    "        net.train()\n",
    "        pred_y = net.forward(features_x, E_start, E_end, E_identity, E_dropin)\n",
    "        loss = net.loss(pred_y[idx_train], train_y[idx_train], None) \n",
    "        train_acc = calculate_avg_accuracy(nb_classes, train_y[idx_train], pred_y[idx_train]) # training acc\n",
    "        running_train_acc += train_acc    \n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "        # backward, update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # validation eval\n",
    "        net.eval()\n",
    "        y_eval = net.forward(features_x, E_start, E_end, E_identity, E_dropin)\n",
    "        val_loss = net.loss(y_eval[idx_val], train_y[idx_val], None) \n",
    "        running_val_loss += val_loss.item()\n",
    "\n",
    "        # learning rate, print results\n",
    "        if not iteration%batch_iters:\n",
    "            val_accuracy = calculate_avg_accuracy(nb_classes, train_y[idx_val], y_eval[idx_val])\n",
    "            average_val_loss = running_val_loss/ batch_iters\n",
    "            avg_train_acc = running_train_acc/ batch_iters\n",
    "\n",
    "            # update learning rate \n",
    "            if val_accuracy < avg_train_acc and avg_train_acc > 0.8:\n",
    "                optimizer, lr = update_lr(net, optimizer, average_val_loss, average_loss_old, \n",
    "                                          lr, decay_rate, early_stopping, verbose)\n",
    "\n",
    "            # save intermediate results\n",
    "            if val_accuracy > best:\n",
    "                torch.save(net.state_dict(), SAVE_PATH)\n",
    "                best = val_accuracy\n",
    "            tab_results.append([iteration,average_val_loss,100* val_accuracy, time.time()-t_start_total])\n",
    "\n",
    "            if verbose:\n",
    "                print_results(iteration, batch_iters, avg_train_acc, running_train_loss, val_accuracy, lr, t_start)\n",
    "            if lr < torch.tensor(early_stopping).cuda() and avg_train_acc - val_accuracy > 0.05:\n",
    "                print(\"Early Stopping at %d. Highest Val: %.3f \" % (iteration, max([tab_results[i][2] for i in range(len(tab_results))])))\n",
    "                return max([tab_results[i][2] for i in range(len(tab_results))])\n",
    "                break\n",
    "\n",
    "            # reset counters\n",
    "            t_start = time.time()\n",
    "            running_train_acc = running_train_loss = running_val_loss = 0.0\n",
    "            average_loss_old = average_val_loss\n",
    "    return max([tab_results[i][2] for i in range(len(tab_results))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm = torch.sum(E_end.t(), 1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm = torch.max(norm, torch.ones(norm.shape).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3327, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerry/torch/graph-neural-networks/util/utils.py:185: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on citeseer\n",
      "Setting Layers=4, L2=0.00500, LR=0.001000, Dropout_FC=0.400, Dropout_edge=0.400, Dropin=0.00100\n",
      "\n",
      "iteration= 20, train loss(20iter)= nan, lr= 0.0010000, time(20iter)= 19.38\n",
      "val accuracy= 5.800\n",
      "train accuracy= 16.667\n",
      "\n",
      "iteration= 40, train loss(20iter)= nan, lr= 0.0010000, time(20iter)= 19.35\n",
      "val accuracy= 5.800\n",
      "train accuracy= 16.667\n",
      "\n",
      "iteration= 60, train loss(20iter)= nan, lr= 0.0010000, time(20iter)= 19.32\n",
      "val accuracy= 5.800\n",
      "train accuracy= 16.667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-afa22d459e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Setting Layers=%d, L2=%.5f, LR=%f, Dropout_FC=%.3f, Dropout_edge=%.3f, Dropin=%.5f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnet_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dropout_fc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dropout_edge'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dropout_in'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nb_clusters_target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAVE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0d8d20f0868a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, lr, l2, batch_iters, nb_classes, early_stopping, SAVE_PATH, verbose, max_iters)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# forward, loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_identity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_dropin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_avg_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# training acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jerry/torch/graph-neural-networks/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, E_start, E_end, E_identity, E_dropin)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;31m# Edge Start+End Dropout for all layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mnum_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mdropout_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_edges\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mE_identity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mE_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jerry/torch/graph-neural-networks/model.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;31m# Edge Start+End Dropout for all layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mnum_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mdropout_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_edges\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mE_identity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mE_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features_x, train_y, E_start, E_end, E_identity, E_dropin, idx_train, idx_val, idx_test = load_variables('citeseer', 1)\n",
    "\n",
    "net_parameters = {}\n",
    "net_parameters['features'] = features_x.shape[1]\n",
    "net_parameters['nb_clusters_target'] = torch.max(train_y).item()+1\n",
    "net_parameters['H'] = 50\n",
    "net_parameters['L'] = 4\n",
    "net_parameters['Dropout_fc'] = 0.4\n",
    "net_parameters['Dropout_edge'] = 0.4\n",
    "net_parameters['Dropout_in'] = 0.001\n",
    "lr = 0.001\n",
    "l2 = 0.005\n",
    "batch_iters = 20\n",
    "early_stopping = 50e-6\n",
    "SAVE_PATH = 'model_states/citeseer'\n",
    "verbose = True\n",
    "test_acc = np.zeros((10,1))\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    net = Graph_OurConvNet(net_parameters, 1, 1)\n",
    "    net.cuda()\n",
    "        \n",
    "    print(\"Training on\", 'citeseer')\n",
    "    print(\"Setting Layers=%d, L2=%.5f, LR=%f, Dropout_FC=%.3f, Dropout_edge=%.3f, Dropin=%.5f\" % (net_parameters['L'], l2, lr, net_parameters['Dropout_fc'], net_parameters['Dropout_edge'], net_parameters['Dropout_in']))\n",
    "    \n",
    "    train(net, lr, l2, batch_iters, net_parameters['nb_clusters_target'],  early_stopping, SAVE_PATH, verbose, 500)\n",
    "    \n",
    "    # Eval\n",
    "    net.load_state_dict(torch.load(SAVE_PATH))\n",
    "    net.eval()\n",
    "    y_eval = net.forward(features_x, E_start, E_end, E_identity, E_dropin)\n",
    "\n",
    "    loss = net.loss(y_eval[idx_test], train_y[idx_test], None) \n",
    "    accuracy = calculate_avg_accuracy(net_parameters['nb_clusters_target'], \n",
    "                                      train_y[idx_test], y_eval[idx_test])\n",
    "    print('\\ntest loss = %.3f, test accuracy = %.3f' % (loss.item(), 100* accuracy))\n",
    "    test_acc[i] += accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
