{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pdb \n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sys\n",
    "import os \n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    torch.cuda.manual_seed(seed)\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "# Helper methods for loading CORA Graph\n",
    "from utils import load_data2,load_data3,accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Cora Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data (GCN)\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, labels = load_data3('cora')\n",
    "adj = adj.toarray().astype(float)\n",
    "adj += np.eye(adj.shape[0])\n",
    "idx_train = np.argwhere(train_mask).reshape(-1)\n",
    "idx_val = np.argwhere(val_mask).reshape(-1)\n",
    "idx_test = np.argwhere(test_mask).reshape(-1)\n",
    "labels = torch.LongTensor(np.where(labels)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_edges = []\n",
    "for v1 in idx_train:\n",
    "    for v2 in idx_train:\n",
    "        if v1 != v2 and adj[v1, v2] != 1: # and labels[v1] == labels[v2]:\n",
    "            new_edges.append((v1,v2))\n",
    "new_edges = np.array(new_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropin(new_edges, rate, dim=2708):\n",
    "    np.random.shuffle(new_edges)\n",
    "    v = new_edges.shape[0]\n",
    "    E_start = np.zeros((v, dim))\n",
    "    E_end = np.zeros((v, dim))\n",
    "    for i in range(0, int(v*rate), 2):\n",
    "        v1, v2 = new_edges[i]\n",
    "        E_start[i,v1] = E_end[i,v2] = E_start[i+1,v1] = E_end[i+1,v2] = 1\n",
    "    E_start = Variable(torch.from_numpy(E_start[:i+2,:]).float())\n",
    "    E_end = Variable(torch.from_numpy(E_end[:i+2,:]).float())\n",
    "    return E_start.cuda(), E_end.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data (pyGCN)\n",
    "# adj, features, labels, idx_train, idx_val, idx_test = load_data2()\n",
    "\n",
    "# adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "# adj = adj.toarray()\n",
    "# adj += np.eye(adj.shape[0])\n",
    "\n",
    "cora_Estart = np.zeros((20000, 2708))\n",
    "cora_Eend = np.zeros((20000, 2708))\n",
    "cora_Eidentity = [] # idx of identity edges\n",
    "\n",
    "# converting adjacency matrix to edge-to-start, edge-to-end vertex matrix\n",
    "count = 0\n",
    "for i in range(adj.shape[0]):\n",
    "    for j in range(adj.shape[1]):\n",
    "        if adj[i,j] == 1:\n",
    "            cora_Estart[count,i] = 1\n",
    "            cora_Eend[count,j] = 1\n",
    "            if i == j:\n",
    "                cora_Eidentity.append(count)\n",
    "            count += 1\n",
    "cora_Estart = cora_Estart[:count]\n",
    "cora_Eend = cora_Eend[:count]\n",
    "\n",
    "def get_cora_dataset():\n",
    "    x = Variable(features, requires_grad=False)\n",
    "    y = Variable(labels)\n",
    "    E_start = Variable(torch.from_numpy(cora_Estart).float())\n",
    "    E_end = Variable(torch.from_numpy(cora_Eend).float())\n",
    "    \n",
    "    return x.cuda(), y.cuda(), E_start.cuda(), E_end.cuda(), cora_Eidentity, new_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OurConvNetcell(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dropout_fc=0, dropout_edge=0):\n",
    "        super(OurConvNetcell, self).__init__()\n",
    "    \n",
    "        # conv1\n",
    "        self.Ui1 = nn.Linear(dim_in, dim_out, bias=False) \n",
    "        self.Vi1 = nn.Linear(dim_in, dim_out, bias=False) \n",
    "        self.Vj1 = nn.Linear(dim_in, dim_out, bias=False)  \n",
    "        self.bu1 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        self.bv1 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        \n",
    "        self.dropout_fc = dropout_fc\n",
    "        self.dropout_edge = dropout_edge\n",
    "        \n",
    "        # conv2\n",
    "        self.Ui2 = nn.Linear(dim_out, dim_out, bias=False) \n",
    "        self.Vi2 = nn.Linear(dim_out, dim_out, bias=False) \n",
    "        self.Vj2 = nn.Linear(dim_out, dim_out, bias=False)  \n",
    "        self.bu2 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        self.bv2 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        \n",
    "        # bn1, bn2\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim_out)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim_out)\n",
    "        \n",
    "        # resnet\n",
    "        self.R = nn.Linear(dim_in, dim_out, bias=False) \n",
    "        \n",
    "        # init\n",
    "        self.init_weights_OurConvNetcell(dim_in, dim_out, 1)\n",
    "        \n",
    "         \n",
    "    def init_weights_OurConvNetcell(self, dim_in, dim_out, gain):   \n",
    "        # conv1\n",
    "        scale = gain* np.sqrt( 2.0/ dim_in )\n",
    "        self.Ui1.weight.data.uniform_(-scale, scale) \n",
    "        self.Vi1.weight.data.uniform_(-scale, scale) \n",
    "        self.Vj1.weight.data.uniform_(-scale, scale) \n",
    "        self.bu1.data.fill_(0)\n",
    "        self.bv1.data.fill_(0)\n",
    "        \n",
    "        # conv2\n",
    "        scale = gain* np.sqrt( 2.0/ dim_out )\n",
    "        self.Ui2.weight.data.uniform_(-scale, scale) \n",
    "        self.Vi2.weight.data.uniform_(-scale, scale) \n",
    "        self.Vj2.weight.data.uniform_(-scale, scale) \n",
    "        self.bu2.data.fill_(0)\n",
    "        self.bv2.data.fill_(0)\n",
    "        \n",
    "        # RN\n",
    "        scale = gain* np.sqrt( 2.0/ dim_in )\n",
    "        self.R.weight.data.uniform_(-scale, scale)  \n",
    "        \n",
    "        \n",
    "    def forward(self, x, E_start, E_end):\n",
    "        x = F.dropout(x, self.dropout_fc, training=self.training)\n",
    "        xin = x\n",
    "        \n",
    "        # edge norm\n",
    "        norm = torch.sum(E_end.t(), 1).reshape(-1,1)\n",
    "#         norm = torch.max(norm, torch.ones(norm.shape).cuda())\n",
    "\n",
    "        # conv1\n",
    "        Uix = self.Ui1(x)  #  V x H_out\n",
    "        Vix = self.Vi1(x)  #  V x H_out\n",
    "        Vjx = self.Vj1(x)  #  V x H_out\n",
    "        x1 = torch.mm(E_end,Vix) + torch.mm(E_start,Vjx) + self.bv1  # E x H_out\n",
    "        x1 = torch.sigmoid(x1)\n",
    "#         x1 = F.dropout(x1, self.dropout_fc, training=self.training)\n",
    "\n",
    "        x2 = torch.mm(E_start, Uix)  #  E x H_out\n",
    "        x = torch.mm(E_end.t(), x1*x2) + self.bu1 #  V x H_out\n",
    "        \n",
    "        x = torch.div(x, norm)# norm\n",
    "        x = self.bn1(x) # bn1\n",
    "        x = torch.nn.LeakyReLU(0.2)(x) # relu1\n",
    "        \n",
    "        # conv2\n",
    "        Uix = self.Ui2(x)  #  V x H_out\n",
    "        Vix = self.Vi2(x)  #  V x H_out\n",
    "        Vjx = self.Vj2(x)  #  V x H_out\n",
    "        x1 = torch.mm(E_end,Vix) + torch.mm(E_start,Vjx) + self.bv2  # E x H_out\n",
    "        x1 = torch.sigmoid(x1)\n",
    "#         x1 = F.dropout(x1, self.dropout_fc, training=self.training)\n",
    "        \n",
    "        x2 = torch.mm(E_start, Uix)  #  V x H_out        \n",
    "        x = torch.mm(E_end.t(), x1*x2) + self.bu2 #  V x H_out\n",
    "        \n",
    "        x = torch.div(x, norm) # normalization\n",
    "        \n",
    "        x = self.bn2(x) # bn2\n",
    "        x = x + self.R(xin) # addition\n",
    "        x = torch.nn.LeakyReLU(0.2)(x) # relu2\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class Graph_OurConvNet(nn.Module):\n",
    "    def __init__(self, net_parameters, cora=False):\n",
    "        super(Graph_OurConvNet, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        Voc = net_parameters['Voc']\n",
    "        D = net_parameters['D']\n",
    "        nb_clusters_target = net_parameters['nb_clusters_target']\n",
    "        H = net_parameters['H']\n",
    "        L = net_parameters['L']\n",
    "        self.cora = cora\n",
    "        self.dropout_fc = net_parameters['Dropout_fc']\n",
    "        self.dropout_edge = net_parameters['Dropout_edge']\n",
    "        self.drop_in = net_parameters['Dropout_in']\n",
    "        \n",
    "        # vector of hidden dimensions\n",
    "        net_layers = []\n",
    "        for layer in range(L):\n",
    "            net_layers.append(H)\n",
    "        \n",
    "        # CL cells\n",
    "        # NOTE: Each graph convnet cell uses *TWO* convolutional operations\n",
    "        net_layers_extended = [net_parameters['features']] + net_layers \n",
    "        \n",
    "        L = len(net_layers)\n",
    "        list_of_gnn_cells = [] # list of NN cells\n",
    "        for layer in range(L//2):\n",
    "            Hin, Hout = net_layers_extended[2*layer], net_layers_extended[2*layer+2]\n",
    "            list_of_gnn_cells.append(OurConvNetcell(Hin,Hout, self.dropout_fc, self.dropout_edge))\n",
    "        \n",
    "        # register the cells for pytorch\n",
    "        self.gnn_cells = nn.ModuleList(list_of_gnn_cells)\n",
    "            \n",
    "        # fc\n",
    "        Hfinal = net_layers_extended[-1]\n",
    "        self.fc = nn.Linear(Hfinal,nb_clusters_target) \n",
    "        \n",
    "        # init\n",
    "        self.init_weights_Graph_OurConvNet(Voc,D,Hfinal,nb_clusters_target,1)\n",
    "        \n",
    "        # print\n",
    "#         print('\\nnb of hidden layers=',L)\n",
    "#         print('dim of layers (w/ embed dim)=',net_layers_extended)      \n",
    "#         print('\\n')\n",
    "        \n",
    "        # class variables\n",
    "        self.D = D\n",
    "        self.L = L\n",
    "        self.net_layers_extended = net_layers_extended      \n",
    "        \n",
    "        \n",
    "    def init_weights_Graph_OurConvNet(self, Fin_enc, Fout_enc, Fin_fc, Fout_fc, gain):\n",
    "        scale = gain* np.sqrt(2.0/ (Fin_fc+Fout_fc))\n",
    "        self.fc.weight.data.uniform_(-scale, scale)  \n",
    "        self.fc.bias.data.fill_(0)  \n",
    "        \n",
    "    def forward(self, x, E_start, E_end, E_identity, E_dropin):\n",
    "        if self.training:\n",
    "            # Edge Start+End Dropout for all layers\n",
    "            num_edges = E_start.shape[0]\n",
    "            dropout_idx = np.array([i for i in range(num_edges) if i not in E_identity])\n",
    "            np.random.shuffle(dropout_idx)\n",
    "            E_start = E_start.clone()\n",
    "            E_start[dropout_idx[:int(num_edges*self.dropout_edge)]] = 0\n",
    "            E_end = E_end.clone()\n",
    "            E_end[dropout_idx[:int(num_edges*self.dropout_edge)]] = 0\n",
    "            \n",
    "            # Dropin\n",
    "            D_start, D_end = dropin(E_dropin, self.drop_in)\n",
    "            E_start = torch.cat((E_start, D_start), 0)\n",
    "            E_end = torch.cat((E_end, D_end), 0)\n",
    "            \n",
    "        # convnet cells  \n",
    "        for layer in range(self.L//2):\n",
    "            gnn_layer = self.gnn_cells[layer]            \n",
    "            x = gnn_layer(x,E_start,E_end) # V x H\n",
    "            \n",
    "        x = F.dropout(x, self.dropout_fc, training=self.training) #FC Dropout\n",
    "        x = self.fc(x) # FC\n",
    "        return x\n",
    "         \n",
    "    def loss(self, y, y_target, weight):\n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "        return loss\n",
    "       \n",
    "    def update(self, lr, l2):\n",
    "        update = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "        return update\n",
    "    \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return optimizer\n",
    "    \n",
    "    def nb_param(self):\n",
    "        return self.nb_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_avg_accuracy(nb_classes, labels, pred_y):\n",
    "    S = labels.data.cpu().numpy()\n",
    "    C = np.argmax(torch.nn.Softmax(dim=1)(pred_y).data.cpu().numpy() , axis=1)\n",
    "    return np.sum(S==C)/S.shape[0]\n",
    "\n",
    "def update_lr(net, optimizer, average_loss, average_loss_old, lr, decay_rate, early_stopping, verbose):\n",
    "    # Update LR if > early_stopping and avg val loss is higher\n",
    "    if average_loss > average_loss_old and lr > early_stopping:\n",
    "        lr /= decay_rate\n",
    "        if verbose:\n",
    "            print('Updating LR to %.7f' % lr)\n",
    "    return net.update_learning_rate(optimizer, lr), lr\n",
    "\n",
    "def print_results(iteration, batch_iters, avg_train_acc, running_train_loss, val_accuracy, lr, t_start):\n",
    "    print('\\niteration= %d, train loss(%diter)= %.3f, lr= %.7f, time(%diter)= %.2f' % \n",
    "          (iteration, batch_iters, running_train_loss/batch_iters, lr, \n",
    "           batch_iters, time.time() - t_start))\n",
    "    print('val accuracy= %.3f' % (100* val_accuracy))\n",
    "    print('train accuracy= %.3f' % (100* avg_train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, lr, l2, batch_iters, early_stopping, verbose=False):\n",
    "    ### optimization parameters\n",
    "    nb_classes = 7 \n",
    "    max_iters = 1000\n",
    "    decay_rate = 1.25\n",
    "    SAVE_PATH = 'model_state'\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = net.update(lr, l2) \n",
    "    t_start = time.time()\n",
    "    t_start_total = time.time()\n",
    "    average_loss_old = torch.tensor(1e4).cuda()\n",
    "    best = running_train_acc = running_train_loss = running_val_loss = 0.0\n",
    "    tab_results = []\n",
    "    \n",
    "    features_x, train_y, E_start, E_end, E_identity, E_dropin = get_cora_dataset()\n",
    "\n",
    "    for iteration in range(1, max_iters):  # loop over the dataset multiple times\n",
    "        # forward, loss\n",
    "        net.train()\n",
    "        pred_y = net.forward(features_x, E_start, E_end, E_identity, E_dropin)\n",
    "        loss = net.loss(pred_y[idx_train], train_y[idx_train], None) \n",
    "        train_acc = calculate_avg_accuracy(nb_classes, train_y[idx_train], pred_y[idx_train]) # training acc\n",
    "        running_train_acc += train_acc    \n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "        # backward, update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # validation eval\n",
    "        net.eval()\n",
    "        y_eval = net.forward(features_x, E_start, E_end, E_identity, E_dropin)\n",
    "        val_loss = net.loss(y_eval[idx_val], train_y[idx_val], None) \n",
    "        running_val_loss += val_loss.item()\n",
    "\n",
    "        # learning rate, print results\n",
    "        if not iteration%batch_iters:\n",
    "            val_accuracy = calculate_avg_accuracy(nb_classes, train_y[idx_val], y_eval[idx_val])\n",
    "            average_val_loss = running_val_loss/ batch_iters\n",
    "            avg_train_acc = running_train_acc/ batch_iters\n",
    "\n",
    "            # update learning rate \n",
    "            if val_accuracy < avg_train_acc:\n",
    "                optimizer, lr = update_lr(net, optimizer, average_val_loss, average_loss_old, \n",
    "                                          lr, decay_rate, early_stopping, verbose)\n",
    "\n",
    "            # save intermediate results\n",
    "            if val_accuracy > best:\n",
    "                torch.save(net.state_dict(), SAVE_PATH)\n",
    "                best = val_accuracy\n",
    "            tab_results.append([iteration,average_val_loss,100* val_accuracy, time.time()-t_start_total])\n",
    "\n",
    "            if verbose:\n",
    "                print_results(iteration, batch_iters, avg_train_acc, running_train_loss, val_accuracy, lr, t_start)\n",
    "            if lr < torch.tensor(early_stopping).cuda() and avg_train_acc - val_accuracy > 0.05:\n",
    "                print(\"Early Stopping at %d. Highest Val: %.3f \" % (iteration, max([tab_results[i][2] for i in range(len(tab_results))])))\n",
    "                return max([tab_results[i][2] for i in range(len(tab_results))])\n",
    "                break\n",
    "\n",
    "            # reset counters\n",
    "            t_start = time.time()\n",
    "            running_train_acc = running_train_loss = running_val_loss = 0.0\n",
    "            average_loss_old = average_val_loss\n",
    "    return max([tab_results[i][2] for i in range(len(tab_results))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_x, _, _, _, _, _ = get_cora_dataset()\n",
    "\n",
    "CORA = 1\n",
    "net_parameters = {}\n",
    "net_parameters['features'] = features_x.shape[1]\n",
    "net_parameters['Voc'] = 7+1 \n",
    "net_parameters['nb_clusters_target'] = 7\n",
    "net_parameters['D'] = net_parameters['H'] = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Layers=4, Drop In=0.03000, LR=0.001000\n",
      "Early Stopping at 580. Highest Val: 65.000 \n",
      "Avg Val Accuracy 65.0\n",
      "Setting Layers=4, Drop In=0.04000, LR=0.001000\n",
      "Early Stopping at 620. Highest Val: 64.600 \n",
      "Avg Val Accuracy 64.60000000000001\n",
      "Setting Layers=4, Drop In=0.05000, LR=0.001000\n",
      "Early Stopping at 500. Highest Val: 64.600 \n",
      "Avg Val Accuracy 64.60000000000001\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "l2 = 0.005\n",
    "layers = [4,10]\n",
    "iters = 1\n",
    "batch_iters = 20\n",
    "early_stopping = 5e-5\n",
    "dfc = [0.03,0.04,0.05]\n",
    "    \n",
    "for i in range(len(dfc)):\n",
    "    net_parameters['L'] = 4\n",
    "    net_parameters['Dropout_fc'] = 0.0\n",
    "    net_parameters['Dropout_edge'] = 0.0\n",
    "    net_parameters['Dropout_in'] = dfc[i]\n",
    "    print(\"Setting Layers=%d, Drop In=%.5f, LR=%f\" % (4, dfc[i], lr))\n",
    "\n",
    "    val_total = 0.\n",
    "    for iteration in range(iters):\n",
    "        net = Graph_OurConvNet(net_parameters, 1)\n",
    "        net.cuda()\n",
    "        val_total += train(net, lr, l2, batch_iters, early_stopping, verbose=False)\n",
    "    print(\"Avg Val Accuracy\", val_total)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Layers=4, Drop In=0.00800, LR=0.001000\n",
      "Early Stopping at 540. Highest Val: 69.200 \n",
      "Avg Val Accuracy 69.19999999999999\n",
      "Setting Layers=4, Drop In=0.01000, LR=0.001000\n",
      "Early Stopping at 520. Highest Val: 66.200 \n",
      "Avg Val Accuracy 66.2\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "l2 = 0.005\n",
    "layers = [4,10]\n",
    "iters = 1\n",
    "batch_iters = 20\n",
    "early_stopping = 5e-5\n",
    "dfc = [0.008, 0.01]\n",
    "    \n",
    "for i in range(len(dfc)):\n",
    "    net_parameters['L'] = 4\n",
    "    net_parameters['Dropout_fc'] = 0.0\n",
    "    net_parameters['Dropout_edge'] = 0.0\n",
    "    net_parameters['Dropout_in'] = dfc[i]\n",
    "    print(\"Setting Layers=%d, Drop In=%.5f, LR=%f\" % (4, dfc[i], lr))\n",
    "\n",
    "    val_total = 0.\n",
    "    for iteration in range(iters):\n",
    "        net = Graph_OurConvNet(net_parameters, 1)\n",
    "        net.cuda()\n",
    "        val_total += train(net, lr, l2, batch_iters, early_stopping, verbose=False)\n",
    "    print(\"Avg Val Accuracy\", val_total)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEXCAYAAACDChKsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMSYmFkCMMcbExAKIMcaYmPx/1/+uvFlJW68AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f03b1f9a9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = [0.0,0.002,0.004,0.006,0.008,0.01,0.012,0.014,0.016,0.018,0.02,0.03,0.04,0.05]\n",
    "y = [63.6,69.4,70.6,68.6,69.2,66.2,70.8,69.2,64.6,65.6,66.4,65,64.6,64.6]\n",
    "\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=14)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=17)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=17)    # fontsize of the x and y labels\n",
    "plt.rc('legend', fontsize=14)    # legend fontsize\n",
    "plt.rc('xtick', labelsize=15)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=15)    # fontsize of the tick labels\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.xlabel('Dropin Rate')\n",
    "plt.plot(x, y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
