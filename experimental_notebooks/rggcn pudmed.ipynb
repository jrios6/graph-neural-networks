{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pdb \n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sys\n",
    "import os \n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "CUDA = False\n",
    "\n",
    "if CUDA and torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "# Helper methods for loading CORA Graph\n",
    "from utils import load_data3, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data (GCN)\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, labels = load_data3('pubmed')\n",
    "adj = adj.toarray().astype(float)\n",
    "adj += np.eye(adj.shape[0])\n",
    "idx_train = np.argwhere(train_mask).reshape(-1)\n",
    "idx_val = np.argwhere(val_mask).reshape(-1)\n",
    "idx_test = np.argwhere(test_mask).reshape(-1)\n",
    "labels = torch.LongTensor(np.where(labels)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_edges = []\n",
    "for v1 in idx_train:\n",
    "    for v2 in idx_train:\n",
    "        if v1 != v2 and adj[v1, v2] != 1: # and labels[v1] == labels[v2]:\n",
    "            new_edges.append((v1,v2))\n",
    "new_edges = np.array(new_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Pudmed\n",
    "v = adj.shape[0]\n",
    "Estart = np.zeros((110000, v))\n",
    "Eend = np.zeros((110000, v))\n",
    "Eidentity = [] # idx of identity edges\n",
    "\n",
    "# converting adjacency matrix to edge-to-start, edge-to-end vertex matrix\n",
    "count = 0\n",
    "for i in range(v):\n",
    "    for j in range(v):\n",
    "        if adj[i,j] == 1:\n",
    "            Estart[count,i] = 1\n",
    "            Eend[count,j] = 1\n",
    "            if i == j:\n",
    "                Eidentity.append(count)\n",
    "            count += 1\n",
    "Estart = Eend[:count]\n",
    "Eend = Eend[:count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_x = Variable(features, requires_grad=False)\n",
    "train_y = Variable(labels)\n",
    "E_start = Variable(torch.from_numpy(Estart).float())\n",
    "E_end = Variable(torch.from_numpy(Eend).float())\n",
    "E_identity = Eidentity\n",
    "E_dropin = new_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropin(new_edges, rate, dim=2708, cuda=False):\n",
    "    np.random.shuffle(new_edges)\n",
    "    v = new_edges.shape[0]\n",
    "    E_start = np.zeros((v, dim))\n",
    "    E_end = np.zeros((v, dim))\n",
    "    for i in range(0, int(v*rate), 2):\n",
    "        v1, v2 = new_edges[i]\n",
    "        E_start[i,v1] = E_end[i,v2] = E_start[i+1,v1] = E_end[i+1,v2] = 1\n",
    "    E_start = Variable(torch.from_numpy(E_start[:i+2,:]).float())\n",
    "    E_end = Variable(torch.from_numpy(E_end[:i+2,:]).float())\n",
    "    \n",
    "    if cuda:\n",
    "        return E_start.cuda(), E_end.cuda()\n",
    "    return E_start, E_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OurConvNetcell(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dropout_fc=0, dropout_edge=0):\n",
    "        super(OurConvNetcell, self).__init__()\n",
    "    \n",
    "        # conv1\n",
    "        self.Ui1 = nn.Linear(dim_in, dim_out, bias=False) \n",
    "        self.Vi1 = nn.Linear(dim_in, dim_out, bias=False) \n",
    "        self.Vj1 = nn.Linear(dim_in, dim_out, bias=False)  \n",
    "        self.bu1 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        self.bv1 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        \n",
    "        self.dropout_fc = dropout_fc\n",
    "        self.dropout_edge = dropout_edge\n",
    "        \n",
    "        # conv2\n",
    "        self.Ui2 = nn.Linear(dim_out, dim_out, bias=False) \n",
    "        self.Vi2 = nn.Linear(dim_out, dim_out, bias=False) \n",
    "        self.Vj2 = nn.Linear(dim_out, dim_out, bias=False)  \n",
    "        self.bu2 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        self.bv2 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        \n",
    "        # bn1, bn2\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim_out)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim_out)\n",
    "        \n",
    "        # resnet\n",
    "        self.R = nn.Linear(dim_in, dim_out, bias=False) \n",
    "        \n",
    "        # init\n",
    "        self.init_weights_OurConvNetcell(dim_in, dim_out, 1)\n",
    "        \n",
    "         \n",
    "    def init_weights_OurConvNetcell(self, dim_in, dim_out, gain):   \n",
    "        # conv1\n",
    "        scale = gain* np.sqrt( 2.0/ dim_in )\n",
    "        self.Ui1.weight.data.uniform_(-scale, scale) \n",
    "        self.Vi1.weight.data.uniform_(-scale, scale) \n",
    "        self.Vj1.weight.data.uniform_(-scale, scale) \n",
    "        self.bu1.data.fill_(0)\n",
    "        self.bv1.data.fill_(0)\n",
    "        \n",
    "        # conv2\n",
    "        scale = gain* np.sqrt( 2.0/ dim_out )\n",
    "        self.Ui2.weight.data.uniform_(-scale, scale) \n",
    "        self.Vi2.weight.data.uniform_(-scale, scale) \n",
    "        self.Vj2.weight.data.uniform_(-scale, scale) \n",
    "        self.bu2.data.fill_(0)\n",
    "        self.bv2.data.fill_(0)\n",
    "        \n",
    "        # RN\n",
    "        scale = gain* np.sqrt( 2.0/ dim_in )\n",
    "        self.R.weight.data.uniform_(-scale, scale)  \n",
    "        \n",
    "        \n",
    "    def forward(self, x, E_start, E_end):\n",
    "        x = F.dropout(x, self.dropout_fc, training=self.training)\n",
    "        xin = x\n",
    "        \n",
    "        # edge norm\n",
    "#         norm = torch.sum(E_end.t(), 1).reshape(-1,1)\n",
    "#         norm = torch.max(norm, torch.ones(norm.shape).cuda())\n",
    "\n",
    "        # conv1\n",
    "        Uix = self.Ui1(x)  #  V x H_out\n",
    "        Vix = self.Vi1(x)  #  V x H_out\n",
    "        Vjx = self.Vj1(x)  #  V x H_out\n",
    "        x1 = torch.mm(E_end,Vix) + torch.mm(E_start,Vjx) + self.bv1  # E x H_out\n",
    "        x1 = torch.sigmoid(x1)\n",
    "        x1 = F.dropout(x1, self.dropout_fc, training=self.training)\n",
    "\n",
    "        x2 = torch.mm(E_start, Uix)  #  E x H_out\n",
    "        x = torch.mm(E_end.t(), x1*x2) + self.bu1 #  V x H_out\n",
    "        \n",
    "#         x = torch.div(x, norm)# norm\n",
    "        x = self.bn1(x) # bn1\n",
    "        x = torch.nn.LeakyReLU(0.1)(x) # relu1\n",
    "\n",
    "        # conv2\n",
    "        Uix = self.Ui2(x)  #  V x H_out\n",
    "        Vix = self.Vi2(x)  #  V x H_out\n",
    "        Vjx = self.Vj2(x)  #  V x H_out\n",
    "        x1 = torch.mm(E_end,Vix) + torch.mm(E_start,Vjx) + self.bv2  # E x H_out\n",
    "        x1 = torch.sigmoid(x1)\n",
    "        x1 = F.dropout(x1, self.dropout_fc, training=self.training)\n",
    "        \n",
    "        x2 = torch.mm(E_start, Uix)  #  V x H_out        \n",
    "        x = torch.mm(E_end.t(), x1*x2) + self.bu2 #  V x H_out\n",
    "        \n",
    "#         x = torch.div(x, norm) # normalization\n",
    "        \n",
    "        x = self.bn2(x) # bn2\n",
    "        x = x + self.R(xin) # addition\n",
    "        x = torch.nn.LeakyReLU(0.1)(x) # relu2\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class Graph_OurConvNet(nn.Module):\n",
    "    def __init__(self, net_parameters, cora=False, cuda=False):\n",
    "        super(Graph_OurConvNet, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        Voc = net_parameters['Voc']\n",
    "        D = net_parameters['D']\n",
    "        nb_clusters_target = net_parameters['nb_clusters_target']\n",
    "        H = net_parameters['H']\n",
    "        L = net_parameters['L']\n",
    "        self.cora = cora\n",
    "        self.cuda = cuda\n",
    "        self.dropout_fc = net_parameters['Dropout_fc']\n",
    "        self.dropout_edge = net_parameters['Dropout_edge']\n",
    "        self.drop_in = net_parameters['Dropout_in']\n",
    "        \n",
    "        # vector of hidden dimensions\n",
    "        net_layers = []\n",
    "        for layer in range(L):\n",
    "            net_layers.append(H)\n",
    "        \n",
    "        # CL cells\n",
    "        # NOTE: Each graph convnet cell uses *TWO* convolutional operations\n",
    "        net_layers_extended = [net_parameters['features']] + net_layers \n",
    "        \n",
    "        L = len(net_layers)\n",
    "        list_of_gnn_cells = [] # list of NN cells\n",
    "        for layer in range(L//2):\n",
    "            Hin, Hout = net_layers_extended[2*layer], net_layers_extended[2*layer+2]\n",
    "            list_of_gnn_cells.append(OurConvNetcell(Hin,Hout, self.dropout_fc, self.dropout_edge))\n",
    "        \n",
    "        # register the cells for pytorch\n",
    "        self.gnn_cells = nn.ModuleList(list_of_gnn_cells)\n",
    "            \n",
    "        # fc\n",
    "        Hfinal = net_layers_extended[-1]\n",
    "        self.fc = nn.Linear(Hfinal,nb_clusters_target) \n",
    "        \n",
    "        # init\n",
    "        self.init_weights_Graph_OurConvNet(Voc,D,Hfinal,nb_clusters_target,1)\n",
    "        \n",
    "        # class variables\n",
    "        self.D = D\n",
    "        self.L = L\n",
    "        self.net_layers_extended = net_layers_extended      \n",
    "        \n",
    "        \n",
    "    def init_weights_Graph_OurConvNet(self, Fin_enc, Fout_enc, Fin_fc, Fout_fc, gain):\n",
    "        scale = gain* np.sqrt(2.0/ (Fin_fc+Fout_fc))\n",
    "        self.fc.weight.data.uniform_(-scale, scale)  \n",
    "        self.fc.bias.data.fill_(0)  \n",
    "        \n",
    "    def forward(self, x, E_start, E_end, E_identity, E_dropin):\n",
    "#         if self.training:\n",
    "            # Edge Start+End Dropout for all layers\n",
    "#             num_edges = E_start.shape[0]\n",
    "#             dropout_idx = np.array([i for i in range(num_edges) if i not in E_identity])\n",
    "#             np.random.shuffle(dropout_idx)\n",
    "            \n",
    "#             tmp_start = E_start[dropout_idx[:int(num_edges*self.dropout_edge)]].clone()\n",
    "#             tmp_end = E_end[dropout_idx[:int(num_edges*self.dropout_edge)]].clone()\n",
    "            \n",
    "#             E_start[dropout_idx[:int(num_edges*self.dropout_edge)]] = 0\n",
    "#             E_end[dropout_idx[:int(num_edges*self.dropout_edge)]] = 0\n",
    "        E_start = F.dropout(E_start, self.dropout_edge, training=self.training) #FC Dropout\n",
    "    \n",
    "    \n",
    "        # convnet cells  \n",
    "        for layer in range(self.L//2):\n",
    "            gnn_layer = self.gnn_cells[layer]            \n",
    "            x = gnn_layer(x,E_start,E_end) # V x H\n",
    "            \n",
    "        x = F.dropout(x, self.dropout_fc, training=self.training) #FC Dropout\n",
    "        x = self.fc(x) # FC\n",
    "        \n",
    "#         if self.training:\n",
    "#             E_start[dropout_idx[:int(num_edges*self.dropout_edge)]] = tmp_start\n",
    "#             E_end[dropout_idx[:int(num_edges*self.dropout_edge)]] = tmp_end\n",
    "        \n",
    "        return x\n",
    "         \n",
    "    def loss(self, y, y_target, weight):\n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "        return loss\n",
    "       \n",
    "    def update(self, lr, l2):\n",
    "        update = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "        return update\n",
    "    \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return optimizer\n",
    "    \n",
    "    def nb_param(self):\n",
    "        return self.nb_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_avg_accuracy(nb_classes, labels, pred_y):\n",
    "    S = labels.data.cpu().numpy()\n",
    "    C = np.argmax(torch.nn.Softmax(dim=1)(pred_y).data.cpu().numpy() , axis=1)\n",
    "    return np.sum(S==C)/S.shape[0]\n",
    "\n",
    "def update_lr(net, optimizer, average_loss, average_loss_old, lr, decay_rate, early_stopping, verbose):\n",
    "    # Update LR if > early_stopping and avg val loss is higher\n",
    "    if average_loss > average_loss_old and lr > early_stopping:\n",
    "        lr /= decay_rate\n",
    "        if verbose:\n",
    "            print('Updating LR to %.7f' % lr)\n",
    "    return net.update_learning_rate(optimizer, lr), lr\n",
    "\n",
    "def print_results(iteration, batch_iters, avg_train_acc, running_train_loss, val_accuracy, lr, t_start):\n",
    "    print('\\niteration= %d, train loss(%diter)= %.3f, lr= %.7f, time(%diter)= %.2f' % \n",
    "          (iteration, batch_iters, running_train_loss/batch_iters, lr, \n",
    "           batch_iters, time.time() - t_start))\n",
    "    print('val accuracy= %.3f' % (100* val_accuracy))\n",
    "    print('train accuracy= %.3f' % (100* avg_train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features_x, train_y, E_start, E_end, E_identity, E_dropin = get_cora_dataset(CUDA)\n",
    "\n",
    "def train(net, lr, l2, batch_iters, early_stopping, verbose=False):\n",
    "    ### optimization parameters\n",
    "    nb_classes = 3\n",
    "    max_iters = 500\n",
    "    decay_rate = 1.25\n",
    "    SAVE_PATH = 'model_state'\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = net.update(lr, l2) \n",
    "    t_start = time.time()\n",
    "    t_start_total = time.time()\n",
    "    average_loss_old = torch.tensor(1e4).cuda() if net.cuda else torch.tensor(1e4)\n",
    "    best = running_train_acc = running_train_loss = running_val_loss = 0.0\n",
    "    tab_results = []\n",
    "\n",
    "    for iteration in range(1, max_iters):  # loop over the dataset multiple times\n",
    "        # forward, loss\n",
    "        net.train()\n",
    "        pred_y = net.forward(features_x, E_start, E_end, E_identity, E_dropin)\n",
    "        loss = net.loss(pred_y[idx_train], train_y[idx_train], None) \n",
    "        train_acc = calculate_avg_accuracy(nb_classes, train_y[idx_train], pred_y[idx_train]) # training acc\n",
    "        running_train_acc += train_acc    \n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "        # backward, update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # validation eval\n",
    "        net.eval()\n",
    "        y_eval = net.forward(features_x, E_start, E_end, E_identity, E_dropin)\n",
    "        val_loss = net.loss(y_eval[idx_val], train_y[idx_val], None) \n",
    "        running_val_loss += val_loss.item()\n",
    "\n",
    "        # learning rate, print results\n",
    "        if not iteration%batch_iters:\n",
    "            val_accuracy = calculate_avg_accuracy(nb_classes, train_y[idx_val], y_eval[idx_val])\n",
    "            average_val_loss = running_val_loss/ batch_iters\n",
    "            avg_train_acc = running_train_acc/ batch_iters\n",
    "\n",
    "            # update learning rate \n",
    "            if val_accuracy < avg_train_acc:\n",
    "                optimizer, lr = update_lr(net, optimizer, average_val_loss, average_loss_old, \n",
    "                                          lr, decay_rate, early_stopping, verbose)\n",
    "\n",
    "            # save intermediate results\n",
    "            if val_accuracy > best:\n",
    "                torch.save(net.state_dict(), SAVE_PATH)\n",
    "                best = val_accuracy\n",
    "            tab_results.append([iteration,average_val_loss,100* val_accuracy, time.time()-t_start_total])\n",
    "\n",
    "            if verbose:\n",
    "                print_results(iteration, batch_iters, avg_train_acc, running_train_loss, val_accuracy, lr, t_start)\n",
    "            if lr < torch.tensor(early_stopping).cuda() and avg_train_acc - val_accuracy > 0.05:\n",
    "                print(\"Early Stopping at %d. Highest Val: %.3f \" % (iteration, max([tab_results[i][2] for i in range(len(tab_results))])))\n",
    "                return max([tab_results[i][2] for i in range(len(tab_results))])\n",
    "                break\n",
    "\n",
    "            # reset counters\n",
    "            t_start = time.time()\n",
    "            running_train_acc = running_train_loss = running_val_loss = 0.0\n",
    "            average_loss_old = average_val_loss\n",
    "    return max([tab_results[i][2] for i in range(len(tab_results))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORA = 1\n",
    "net_parameters = {}\n",
    "net_parameters['D'] = net_parameters['H'] = 50\n",
    "net_parameters['features'] = features.shape[1]\n",
    "net_parameters['Voc'] = 1\n",
    "net_parameters['nb_clusters_target'] = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_param= 125953  L= 4\n",
      "\n",
      "iteration= 1, train loss(1iter)= 1.112, lr= 0.0010000, time(1iter)= 70.95\n",
      "val accuracy= 38.400\n",
      "train accuracy= 28.333\n",
      "\n",
      "iteration= 2, train loss(1iter)= 1.108, lr= 0.0010000, time(1iter)= 66.99\n",
      "val accuracy= 39.000\n",
      "train accuracy= 38.333\n",
      "\n",
      "iteration= 3, train loss(1iter)= 1.084, lr= 0.0010000, time(1iter)= 65.86\n",
      "val accuracy= 38.400\n",
      "train accuracy= 31.667\n",
      "\n",
      "iteration= 4, train loss(1iter)= 1.097, lr= 0.0010000, time(1iter)= 64.22\n",
      "val accuracy= 36.400\n",
      "train accuracy= 31.667\n",
      "\n",
      "iteration= 5, train loss(1iter)= 1.107, lr= 0.0010000, time(1iter)= 65.88\n",
      "val accuracy= 38.200\n",
      "train accuracy= 33.333\n",
      "\n",
      "iteration= 6, train loss(1iter)= 1.091, lr= 0.0010000, time(1iter)= 65.95\n",
      "val accuracy= 38.000\n",
      "train accuracy= 40.000\n",
      "\n",
      "iteration= 7, train loss(1iter)= 1.095, lr= 0.0010000, time(1iter)= 65.57\n",
      "val accuracy= 38.800\n",
      "train accuracy= 33.333\n",
      "\n",
      "iteration= 8, train loss(1iter)= 1.115, lr= 0.0010000, time(1iter)= 64.75\n",
      "val accuracy= 39.400\n",
      "train accuracy= 33.333\n",
      "\n",
      "iteration= 9, train loss(1iter)= 1.092, lr= 0.0010000, time(1iter)= 65.20\n",
      "val accuracy= 40.200\n",
      "train accuracy= 31.667\n",
      "\n",
      "iteration= 10, train loss(1iter)= 1.146, lr= 0.0010000, time(1iter)= 66.13\n",
      "val accuracy= 40.200\n",
      "train accuracy= 31.667\n",
      "\n",
      "iteration= 11, train loss(1iter)= 1.096, lr= 0.0010000, time(1iter)= 68.03\n",
      "val accuracy= 40.800\n",
      "train accuracy= 40.000\n",
      "\n",
      "iteration= 12, train loss(1iter)= 1.102, lr= 0.0010000, time(1iter)= 67.88\n",
      "val accuracy= 41.200\n",
      "train accuracy= 30.000\n",
      "\n",
      "iteration= 13, train loss(1iter)= 1.093, lr= 0.0010000, time(1iter)= 68.52\n",
      "val accuracy= 41.200\n",
      "train accuracy= 38.333\n",
      "\n",
      "iteration= 14, train loss(1iter)= 1.092, lr= 0.0010000, time(1iter)= 66.73\n",
      "val accuracy= 41.200\n",
      "train accuracy= 25.000\n",
      "\n",
      "iteration= 15, train loss(1iter)= 1.092, lr= 0.0010000, time(1iter)= 67.02\n",
      "val accuracy= 41.200\n",
      "train accuracy= 36.667\n",
      "\n",
      "iteration= 16, train loss(1iter)= 1.061, lr= 0.0010000, time(1iter)= 66.42\n",
      "val accuracy= 41.000\n",
      "train accuracy= 40.000\n",
      "\n",
      "iteration= 17, train loss(1iter)= 1.080, lr= 0.0010000, time(1iter)= 67.99\n",
      "val accuracy= 41.200\n",
      "train accuracy= 41.667\n",
      "\n",
      "iteration= 18, train loss(1iter)= 1.091, lr= 0.0010000, time(1iter)= 64.82\n",
      "val accuracy= 40.800\n",
      "train accuracy= 31.667\n",
      "\n",
      "iteration= 19, train loss(1iter)= 1.111, lr= 0.0010000, time(1iter)= 65.65\n",
      "val accuracy= 40.800\n",
      "train accuracy= 40.000\n",
      "\n",
      "iteration= 20, train loss(1iter)= 1.096, lr= 0.0010000, time(1iter)= 65.11\n",
      "val accuracy= 40.600\n",
      "train accuracy= 35.000\n",
      "Updating LR to 0.0008000\n",
      "\n",
      "iteration= 21, train loss(1iter)= 1.071, lr= 0.0008000, time(1iter)= 64.06\n",
      "val accuracy= 40.400\n",
      "train accuracy= 46.667\n",
      "\n",
      "iteration= 22, train loss(1iter)= 1.075, lr= 0.0008000, time(1iter)= 65.21\n",
      "val accuracy= 40.600\n",
      "train accuracy= 38.333\n",
      "Updating LR to 0.0006400\n",
      "\n",
      "iteration= 23, train loss(1iter)= 1.080, lr= 0.0006400, time(1iter)= 68.75\n",
      "val accuracy= 40.600\n",
      "train accuracy= 45.000\n",
      "\n",
      "iteration= 24, train loss(1iter)= 1.050, lr= 0.0006400, time(1iter)= 66.80\n",
      "val accuracy= 40.600\n",
      "train accuracy= 50.000\n",
      "\n",
      "iteration= 25, train loss(1iter)= 1.059, lr= 0.0006400, time(1iter)= 67.28\n",
      "val accuracy= 40.600\n",
      "train accuracy= 53.333\n",
      "\n",
      "iteration= 26, train loss(1iter)= 1.056, lr= 0.0006400, time(1iter)= 67.76\n",
      "val accuracy= 40.800\n",
      "train accuracy= 41.667\n",
      "\n",
      "iteration= 27, train loss(1iter)= 1.089, lr= 0.0006400, time(1iter)= 65.10\n",
      "val accuracy= 40.600\n",
      "train accuracy= 36.667\n",
      "\n",
      "iteration= 28, train loss(1iter)= 1.068, lr= 0.0006400, time(1iter)= 67.69\n",
      "val accuracy= 40.600\n",
      "train accuracy= 43.333\n",
      "\n",
      "iteration= 29, train loss(1iter)= 1.072, lr= 0.0006400, time(1iter)= 67.57\n",
      "val accuracy= 40.600\n",
      "train accuracy= 45.000\n",
      "\n",
      "iteration= 30, train loss(1iter)= 1.071, lr= 0.0006400, time(1iter)= 66.26\n",
      "val accuracy= 40.600\n",
      "train accuracy= 43.333\n",
      "\n",
      "iteration= 31, train loss(1iter)= 1.077, lr= 0.0006400, time(1iter)= 65.95\n",
      "val accuracy= 40.600\n",
      "train accuracy= 41.667\n",
      "Updating LR to 0.0005120\n",
      "\n",
      "iteration= 32, train loss(1iter)= 1.078, lr= 0.0005120, time(1iter)= 66.73\n",
      "val accuracy= 40.600\n",
      "train accuracy= 50.000\n",
      "\n",
      "iteration= 33, train loss(1iter)= 1.049, lr= 0.0005120, time(1iter)= 65.35\n",
      "val accuracy= 40.800\n",
      "train accuracy= 48.333\n",
      "\n",
      "iteration= 34, train loss(1iter)= 1.055, lr= 0.0005120, time(1iter)= 66.75\n",
      "val accuracy= 40.600\n",
      "train accuracy= 45.000\n",
      "\n",
      "iteration= 35, train loss(1iter)= 1.066, lr= 0.0005120, time(1iter)= 65.50\n",
      "val accuracy= 40.600\n",
      "train accuracy= 46.667\n",
      "\n",
      "iteration= 36, train loss(1iter)= 1.092, lr= 0.0005120, time(1iter)= 67.29\n",
      "val accuracy= 40.400\n",
      "train accuracy= 25.000\n",
      "\n",
      "iteration= 37, train loss(1iter)= 1.063, lr= 0.0005120, time(1iter)= 65.63\n",
      "val accuracy= 40.400\n",
      "train accuracy= 45.000\n",
      "\n",
      "iteration= 38, train loss(1iter)= 1.095, lr= 0.0005120, time(1iter)= 66.98\n",
      "val accuracy= 40.200\n",
      "train accuracy= 33.333\n",
      "\n",
      "iteration= 39, train loss(1iter)= 1.048, lr= 0.0005120, time(1iter)= 67.21\n",
      "val accuracy= 40.200\n",
      "train accuracy= 40.000\n",
      "\n",
      "iteration= 40, train loss(1iter)= 1.050, lr= 0.0005120, time(1iter)= 67.09\n",
      "val accuracy= 40.200\n",
      "train accuracy= 45.000\n",
      "\n",
      "iteration= 41, train loss(1iter)= 1.053, lr= 0.0005120, time(1iter)= 66.53\n",
      "val accuracy= 40.400\n",
      "train accuracy= 43.333\n",
      "\n",
      "iteration= 42, train loss(1iter)= 1.052, lr= 0.0005120, time(1iter)= 64.30\n",
      "val accuracy= 40.000\n",
      "train accuracy= 41.667\n",
      "\n",
      "iteration= 43, train loss(1iter)= 1.048, lr= 0.0005120, time(1iter)= 65.10\n",
      "val accuracy= 39.600\n",
      "train accuracy= 36.667\n",
      "\n",
      "iteration= 44, train loss(1iter)= 1.059, lr= 0.0005120, time(1iter)= 65.97\n",
      "val accuracy= 39.600\n",
      "train accuracy= 38.333\n",
      "\n",
      "iteration= 45, train loss(1iter)= 1.080, lr= 0.0005120, time(1iter)= 66.82\n",
      "val accuracy= 39.400\n",
      "train accuracy= 41.667\n",
      "\n",
      "iteration= 46, train loss(1iter)= 1.065, lr= 0.0005120, time(1iter)= 66.24\n",
      "val accuracy= 39.000\n",
      "train accuracy= 48.333\n",
      "\n",
      "iteration= 47, train loss(1iter)= 1.035, lr= 0.0005120, time(1iter)= 66.48\n",
      "val accuracy= 39.200\n",
      "train accuracy= 51.667\n",
      "\n",
      "iteration= 48, train loss(1iter)= 1.047, lr= 0.0005120, time(1iter)= 64.62\n",
      "val accuracy= 39.400\n",
      "train accuracy= 40.000\n",
      "\n",
      "iteration= 49, train loss(1iter)= 1.078, lr= 0.0005120, time(1iter)= 65.80\n",
      "val accuracy= 39.600\n",
      "train accuracy= 35.000\n",
      "\n",
      "iteration= 50, train loss(1iter)= 1.052, lr= 0.0005120, time(1iter)= 64.56\n",
      "val accuracy= 39.600\n",
      "train accuracy= 46.667\n",
      "\n",
      "iteration= 51, train loss(1iter)= 1.060, lr= 0.0005120, time(1iter)= 65.92\n",
      "val accuracy= 39.600\n",
      "train accuracy= 41.667\n",
      "\n",
      "iteration= 52, train loss(1iter)= 1.049, lr= 0.0005120, time(1iter)= 66.02\n",
      "val accuracy= 39.600\n",
      "train accuracy= 43.333\n",
      "\n",
      "iteration= 53, train loss(1iter)= 1.067, lr= 0.0005120, time(1iter)= 64.77\n",
      "val accuracy= 39.600\n",
      "train accuracy= 46.667\n",
      "\n",
      "iteration= 54, train loss(1iter)= 1.049, lr= 0.0005120, time(1iter)= 65.86\n",
      "val accuracy= 39.200\n",
      "train accuracy= 41.667\n",
      "\n",
      "iteration= 55, train loss(1iter)= 1.057, lr= 0.0005120, time(1iter)= 66.98\n",
      "val accuracy= 39.200\n",
      "train accuracy= 41.667\n",
      "\n",
      "iteration= 56, train loss(1iter)= 1.027, lr= 0.0005120, time(1iter)= 64.79\n",
      "val accuracy= 39.600\n",
      "train accuracy= 41.667\n",
      "\n",
      "iteration= 57, train loss(1iter)= 1.037, lr= 0.0005120, time(1iter)= 66.65\n",
      "val accuracy= 39.600\n",
      "train accuracy= 51.667\n",
      "\n",
      "iteration= 58, train loss(1iter)= 1.052, lr= 0.0005120, time(1iter)= 64.99\n",
      "val accuracy= 39.600\n",
      "train accuracy= 40.000\n",
      "\n",
      "iteration= 59, train loss(1iter)= 1.049, lr= 0.0005120, time(1iter)= 68.38\n",
      "val accuracy= 40.200\n",
      "train accuracy= 53.333\n",
      "\n",
      "iteration= 60, train loss(1iter)= 1.044, lr= 0.0005120, time(1iter)= 66.72\n",
      "val accuracy= 40.200\n",
      "train accuracy= 50.000\n",
      "\n",
      "iteration= 61, train loss(1iter)= 1.053, lr= 0.0005120, time(1iter)= 66.11\n",
      "val accuracy= 40.200\n",
      "train accuracy= 48.333\n",
      "\n",
      "iteration= 62, train loss(1iter)= 1.069, lr= 0.0005120, time(1iter)= 66.14\n",
      "val accuracy= 40.200\n",
      "train accuracy= 43.333\n",
      "\n",
      "iteration= 63, train loss(1iter)= 1.025, lr= 0.0005120, time(1iter)= 68.88\n",
      "val accuracy= 40.000\n",
      "train accuracy= 46.667\n",
      "\n",
      "iteration= 64, train loss(1iter)= 1.038, lr= 0.0005120, time(1iter)= 66.78\n",
      "val accuracy= 39.800\n",
      "train accuracy= 38.333\n",
      "\n",
      "iteration= 65, train loss(1iter)= 1.036, lr= 0.0005120, time(1iter)= 65.77\n",
      "val accuracy= 39.800\n",
      "train accuracy= 48.333\n",
      "\n",
      "iteration= 66, train loss(1iter)= 1.047, lr= 0.0005120, time(1iter)= 66.22\n",
      "val accuracy= 39.800\n",
      "train accuracy= 51.667\n",
      "\n",
      "iteration= 67, train loss(1iter)= 1.019, lr= 0.0005120, time(1iter)= 65.03\n",
      "val accuracy= 39.800\n",
      "train accuracy= 43.333\n",
      "\n",
      "iteration= 68, train loss(1iter)= 1.068, lr= 0.0005120, time(1iter)= 66.99\n",
      "val accuracy= 40.000\n",
      "train accuracy= 48.333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration= 69, train loss(1iter)= 1.028, lr= 0.0005120, time(1iter)= 67.64\n",
      "val accuracy= 40.400\n",
      "train accuracy= 53.333\n",
      "\n",
      "iteration= 70, train loss(1iter)= 1.031, lr= 0.0005120, time(1iter)= 64.33\n",
      "val accuracy= 40.200\n",
      "train accuracy= 55.000\n",
      "\n",
      "iteration= 71, train loss(1iter)= 1.034, lr= 0.0005120, time(1iter)= 67.66\n",
      "val accuracy= 40.200\n",
      "train accuracy= 55.000\n",
      "\n",
      "iteration= 72, train loss(1iter)= 1.019, lr= 0.0005120, time(1iter)= 68.50\n",
      "val accuracy= 40.000\n",
      "train accuracy= 50.000\n",
      "\n",
      "iteration= 73, train loss(1iter)= 1.048, lr= 0.0005120, time(1iter)= 65.95\n",
      "val accuracy= 39.800\n",
      "train accuracy= 50.000\n",
      "\n",
      "iteration= 74, train loss(1iter)= 1.026, lr= 0.0005120, time(1iter)= 63.53\n",
      "val accuracy= 39.800\n",
      "train accuracy= 53.333\n",
      "\n",
      "iteration= 75, train loss(1iter)= 1.052, lr= 0.0005120, time(1iter)= 66.43\n",
      "val accuracy= 40.000\n",
      "train accuracy= 45.000\n",
      "\n",
      "iteration= 76, train loss(1iter)= 1.019, lr= 0.0005120, time(1iter)= 65.51\n",
      "val accuracy= 40.200\n",
      "train accuracy= 51.667\n",
      "\n",
      "iteration= 77, train loss(1iter)= 1.009, lr= 0.0005120, time(1iter)= 66.80\n",
      "val accuracy= 40.400\n",
      "train accuracy= 56.667\n",
      "\n",
      "iteration= 78, train loss(1iter)= 1.012, lr= 0.0005120, time(1iter)= 67.38\n",
      "val accuracy= 40.600\n",
      "train accuracy= 48.333\n",
      "\n",
      "iteration= 79, train loss(1iter)= 1.016, lr= 0.0005120, time(1iter)= 66.95\n",
      "val accuracy= 40.600\n",
      "train accuracy= 51.667\n",
      "\n",
      "iteration= 80, train loss(1iter)= 1.013, lr= 0.0005120, time(1iter)= 67.74\n",
      "val accuracy= 40.600\n",
      "train accuracy= 51.667\n",
      "\n",
      "iteration= 81, train loss(1iter)= 1.001, lr= 0.0005120, time(1iter)= 67.09\n",
      "val accuracy= 40.800\n",
      "train accuracy= 48.333\n",
      "\n",
      "iteration= 82, train loss(1iter)= 1.031, lr= 0.0005120, time(1iter)= 65.55\n",
      "val accuracy= 40.800\n",
      "train accuracy= 46.667\n",
      "\n",
      "iteration= 83, train loss(1iter)= 1.004, lr= 0.0005120, time(1iter)= 65.58\n",
      "val accuracy= 40.800\n",
      "train accuracy= 56.667\n",
      "\n",
      "iteration= 84, train loss(1iter)= 1.024, lr= 0.0005120, time(1iter)= 63.97\n",
      "val accuracy= 41.000\n",
      "train accuracy= 46.667\n",
      "\n",
      "iteration= 85, train loss(1iter)= 1.069, lr= 0.0005120, time(1iter)= 64.51\n",
      "val accuracy= 41.200\n",
      "train accuracy= 48.333\n",
      "\n",
      "iteration= 86, train loss(1iter)= 1.001, lr= 0.0005120, time(1iter)= 66.91\n",
      "val accuracy= 41.200\n",
      "train accuracy= 51.667\n",
      "\n",
      "iteration= 87, train loss(1iter)= 1.051, lr= 0.0005120, time(1iter)= 65.84\n",
      "val accuracy= 41.400\n",
      "train accuracy= 40.000\n",
      "\n",
      "iteration= 88, train loss(1iter)= 1.026, lr= 0.0005120, time(1iter)= 68.29\n",
      "val accuracy= 41.200\n",
      "train accuracy= 58.333\n",
      "\n",
      "iteration= 89, train loss(1iter)= 1.008, lr= 0.0005120, time(1iter)= 65.43\n",
      "val accuracy= 41.400\n",
      "train accuracy= 58.333\n",
      "\n",
      "iteration= 90, train loss(1iter)= 0.995, lr= 0.0005120, time(1iter)= 63.79\n",
      "val accuracy= 41.800\n",
      "train accuracy= 61.667\n",
      "\n",
      "iteration= 91, train loss(1iter)= 1.002, lr= 0.0005120, time(1iter)= 64.74\n",
      "val accuracy= 42.200\n",
      "train accuracy= 56.667\n",
      "\n",
      "iteration= 92, train loss(1iter)= 0.990, lr= 0.0005120, time(1iter)= 64.44\n",
      "val accuracy= 41.800\n",
      "train accuracy= 55.000\n",
      "\n",
      "iteration= 93, train loss(1iter)= 0.993, lr= 0.0005120, time(1iter)= 64.68\n",
      "val accuracy= 42.200\n",
      "train accuracy= 51.667\n",
      "\n",
      "iteration= 94, train loss(1iter)= 1.017, lr= 0.0005120, time(1iter)= 65.08\n",
      "val accuracy= 42.600\n",
      "train accuracy= 50.000\n",
      "\n",
      "iteration= 95, train loss(1iter)= 0.981, lr= 0.0005120, time(1iter)= 65.66\n",
      "val accuracy= 43.200\n",
      "train accuracy= 58.333\n",
      "\n",
      "iteration= 96, train loss(1iter)= 0.993, lr= 0.0005120, time(1iter)= 66.69\n",
      "val accuracy= 43.400\n",
      "train accuracy= 58.333\n",
      "\n",
      "iteration= 97, train loss(1iter)= 0.951, lr= 0.0005120, time(1iter)= 66.13\n",
      "val accuracy= 43.400\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 98, train loss(1iter)= 0.973, lr= 0.0005120, time(1iter)= 66.18\n",
      "val accuracy= 43.200\n",
      "train accuracy= 53.333\n",
      "\n",
      "iteration= 99, train loss(1iter)= 1.005, lr= 0.0005120, time(1iter)= 64.86\n",
      "val accuracy= 43.600\n",
      "train accuracy= 53.333\n",
      "\n",
      "iteration= 100, train loss(1iter)= 0.967, lr= 0.0005120, time(1iter)= 66.28\n",
      "val accuracy= 43.600\n",
      "train accuracy= 58.333\n",
      "\n",
      "iteration= 101, train loss(1iter)= 0.989, lr= 0.0005120, time(1iter)= 65.93\n",
      "val accuracy= 43.600\n",
      "train accuracy= 63.333\n",
      "\n",
      "iteration= 102, train loss(1iter)= 1.006, lr= 0.0005120, time(1iter)= 66.29\n",
      "val accuracy= 43.600\n",
      "train accuracy= 58.333\n",
      "\n",
      "iteration= 103, train loss(1iter)= 0.972, lr= 0.0005120, time(1iter)= 67.03\n",
      "val accuracy= 43.800\n",
      "train accuracy= 53.333\n",
      "\n",
      "iteration= 104, train loss(1iter)= 0.947, lr= 0.0005120, time(1iter)= 66.71\n",
      "val accuracy= 44.000\n",
      "train accuracy= 61.667\n",
      "\n",
      "iteration= 105, train loss(1iter)= 0.932, lr= 0.0005120, time(1iter)= 66.54\n",
      "val accuracy= 44.400\n",
      "train accuracy= 56.667\n",
      "\n",
      "iteration= 106, train loss(1iter)= 0.986, lr= 0.0005120, time(1iter)= 66.77\n",
      "val accuracy= 44.400\n",
      "train accuracy= 58.333\n",
      "\n",
      "iteration= 107, train loss(1iter)= 0.955, lr= 0.0005120, time(1iter)= 65.75\n",
      "val accuracy= 44.400\n",
      "train accuracy= 60.000\n",
      "\n",
      "iteration= 108, train loss(1iter)= 0.969, lr= 0.0005120, time(1iter)= 66.19\n",
      "val accuracy= 44.200\n",
      "train accuracy= 60.000\n",
      "\n",
      "iteration= 109, train loss(1iter)= 0.984, lr= 0.0005120, time(1iter)= 65.82\n",
      "val accuracy= 44.400\n",
      "train accuracy= 56.667\n",
      "\n",
      "iteration= 110, train loss(1iter)= 0.969, lr= 0.0005120, time(1iter)= 63.91\n",
      "val accuracy= 44.400\n",
      "train accuracy= 65.000\n",
      "\n",
      "iteration= 111, train loss(1iter)= 0.997, lr= 0.0005120, time(1iter)= 63.89\n",
      "val accuracy= 44.600\n",
      "train accuracy= 51.667\n",
      "\n",
      "iteration= 112, train loss(1iter)= 0.981, lr= 0.0005120, time(1iter)= 67.45\n",
      "val accuracy= 45.200\n",
      "train accuracy= 55.000\n",
      "\n",
      "iteration= 113, train loss(1iter)= 0.934, lr= 0.0005120, time(1iter)= 65.17\n",
      "val accuracy= 45.400\n",
      "train accuracy= 63.333\n",
      "\n",
      "iteration= 114, train loss(1iter)= 0.948, lr= 0.0005120, time(1iter)= 64.84\n",
      "val accuracy= 45.800\n",
      "train accuracy= 61.667\n",
      "\n",
      "iteration= 115, train loss(1iter)= 0.945, lr= 0.0005120, time(1iter)= 68.00\n",
      "val accuracy= 46.000\n",
      "train accuracy= 60.000\n",
      "\n",
      "iteration= 116, train loss(1iter)= 0.971, lr= 0.0005120, time(1iter)= 67.10\n",
      "val accuracy= 46.000\n",
      "train accuracy= 61.667\n",
      "\n",
      "iteration= 117, train loss(1iter)= 0.932, lr= 0.0005120, time(1iter)= 65.51\n",
      "val accuracy= 46.400\n",
      "train accuracy= 68.333\n",
      "\n",
      "iteration= 118, train loss(1iter)= 0.974, lr= 0.0005120, time(1iter)= 68.00\n",
      "val accuracy= 46.800\n",
      "train accuracy= 55.000\n",
      "\n",
      "iteration= 119, train loss(1iter)= 0.960, lr= 0.0005120, time(1iter)= 64.63\n",
      "val accuracy= 47.000\n",
      "train accuracy= 60.000\n",
      "\n",
      "iteration= 120, train loss(1iter)= 0.934, lr= 0.0005120, time(1iter)= 65.95\n",
      "val accuracy= 47.200\n",
      "train accuracy= 63.333\n",
      "\n",
      "iteration= 121, train loss(1iter)= 0.986, lr= 0.0005120, time(1iter)= 66.82\n",
      "val accuracy= 47.400\n",
      "train accuracy= 56.667\n",
      "\n",
      "iteration= 122, train loss(1iter)= 0.971, lr= 0.0005120, time(1iter)= 64.72\n",
      "val accuracy= 47.400\n",
      "train accuracy= 51.667\n",
      "\n",
      "iteration= 123, train loss(1iter)= 0.986, lr= 0.0005120, time(1iter)= 65.61\n",
      "val accuracy= 47.200\n",
      "train accuracy= 53.333\n",
      "\n",
      "iteration= 124, train loss(1iter)= 0.926, lr= 0.0005120, time(1iter)= 63.88\n",
      "val accuracy= 47.600\n",
      "train accuracy= 48.333\n",
      "\n",
      "iteration= 125, train loss(1iter)= 0.953, lr= 0.0005120, time(1iter)= 64.72\n",
      "val accuracy= 48.000\n",
      "train accuracy= 55.000\n",
      "\n",
      "iteration= 126, train loss(1iter)= 0.932, lr= 0.0005120, time(1iter)= 64.75\n",
      "val accuracy= 48.400\n",
      "train accuracy= 65.000\n",
      "\n",
      "iteration= 127, train loss(1iter)= 0.915, lr= 0.0005120, time(1iter)= 65.30\n",
      "val accuracy= 48.400\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 128, train loss(1iter)= 0.934, lr= 0.0005120, time(1iter)= 62.95\n",
      "val accuracy= 48.800\n",
      "train accuracy= 58.333\n",
      "\n",
      "iteration= 129, train loss(1iter)= 0.906, lr= 0.0005120, time(1iter)= 62.93\n",
      "val accuracy= 48.600\n",
      "train accuracy= 65.000\n",
      "\n",
      "iteration= 130, train loss(1iter)= 0.901, lr= 0.0005120, time(1iter)= 65.30\n",
      "val accuracy= 49.200\n",
      "train accuracy= 63.333\n",
      "\n",
      "iteration= 131, train loss(1iter)= 0.910, lr= 0.0005120, time(1iter)= 66.12\n",
      "val accuracy= 49.000\n",
      "train accuracy= 68.333\n",
      "\n",
      "iteration= 132, train loss(1iter)= 0.924, lr= 0.0005120, time(1iter)= 64.81\n",
      "val accuracy= 49.400\n",
      "train accuracy= 73.333\n",
      "\n",
      "iteration= 133, train loss(1iter)= 0.922, lr= 0.0005120, time(1iter)= 67.95\n",
      "val accuracy= 49.400\n",
      "train accuracy= 68.333\n",
      "\n",
      "iteration= 134, train loss(1iter)= 0.870, lr= 0.0005120, time(1iter)= 65.64\n",
      "val accuracy= 49.400\n",
      "train accuracy= 65.000\n",
      "\n",
      "iteration= 135, train loss(1iter)= 0.910, lr= 0.0005120, time(1iter)= 66.44\n",
      "val accuracy= 50.000\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 136, train loss(1iter)= 0.853, lr= 0.0005120, time(1iter)= 84.70\n",
      "val accuracy= 50.200\n",
      "train accuracy= 63.333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration= 137, train loss(1iter)= 0.889, lr= 0.0005120, time(1iter)= 84.76\n",
      "val accuracy= 50.200\n",
      "train accuracy= 71.667\n",
      "\n",
      "iteration= 138, train loss(1iter)= 0.889, lr= 0.0005120, time(1iter)= 83.17\n",
      "val accuracy= 50.400\n",
      "train accuracy= 61.667\n",
      "\n",
      "iteration= 139, train loss(1iter)= 0.864, lr= 0.0005120, time(1iter)= 83.30\n",
      "val accuracy= 51.000\n",
      "train accuracy= 68.333\n",
      "\n",
      "iteration= 140, train loss(1iter)= 0.851, lr= 0.0005120, time(1iter)= 83.98\n",
      "val accuracy= 51.400\n",
      "train accuracy= 70.000\n",
      "\n",
      "iteration= 141, train loss(1iter)= 0.872, lr= 0.0005120, time(1iter)= 86.89\n",
      "val accuracy= 51.800\n",
      "train accuracy= 60.000\n",
      "\n",
      "iteration= 142, train loss(1iter)= 0.925, lr= 0.0005120, time(1iter)= 84.09\n",
      "val accuracy= 52.000\n",
      "train accuracy= 61.667\n",
      "\n",
      "iteration= 143, train loss(1iter)= 0.878, lr= 0.0005120, time(1iter)= 83.69\n",
      "val accuracy= 52.000\n",
      "train accuracy= 70.000\n",
      "\n",
      "iteration= 144, train loss(1iter)= 0.877, lr= 0.0005120, time(1iter)= 83.92\n",
      "val accuracy= 52.200\n",
      "train accuracy= 70.000\n",
      "\n",
      "iteration= 145, train loss(1iter)= 0.912, lr= 0.0005120, time(1iter)= 83.70\n",
      "val accuracy= 52.000\n",
      "train accuracy= 63.333\n",
      "\n",
      "iteration= 146, train loss(1iter)= 0.864, lr= 0.0005120, time(1iter)= 83.18\n",
      "val accuracy= 52.600\n",
      "train accuracy= 63.333\n",
      "\n",
      "iteration= 147, train loss(1iter)= 0.838, lr= 0.0005120, time(1iter)= 84.42\n",
      "val accuracy= 52.400\n",
      "train accuracy= 70.000\n",
      "\n",
      "iteration= 148, train loss(1iter)= 0.883, lr= 0.0005120, time(1iter)= 84.29\n",
      "val accuracy= 52.400\n",
      "train accuracy= 81.667\n",
      "\n",
      "iteration= 149, train loss(1iter)= 0.912, lr= 0.0005120, time(1iter)= 82.88\n",
      "val accuracy= 52.600\n",
      "train accuracy= 58.333\n",
      "\n",
      "iteration= 150, train loss(1iter)= 0.900, lr= 0.0005120, time(1iter)= 71.53\n",
      "val accuracy= 52.200\n",
      "train accuracy= 63.333\n",
      "\n",
      "iteration= 151, train loss(1iter)= 0.875, lr= 0.0005120, time(1iter)= 68.28\n",
      "val accuracy= 52.800\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 152, train loss(1iter)= 0.871, lr= 0.0005120, time(1iter)= 63.11\n",
      "val accuracy= 52.800\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 153, train loss(1iter)= 0.879, lr= 0.0005120, time(1iter)= 63.21\n",
      "val accuracy= 53.200\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 154, train loss(1iter)= 0.863, lr= 0.0005120, time(1iter)= 64.59\n",
      "val accuracy= 52.800\n",
      "train accuracy= 60.000\n",
      "\n",
      "iteration= 155, train loss(1iter)= 0.876, lr= 0.0005120, time(1iter)= 63.47\n",
      "val accuracy= 52.600\n",
      "train accuracy= 76.667\n",
      "\n",
      "iteration= 156, train loss(1iter)= 0.854, lr= 0.0005120, time(1iter)= 66.19\n",
      "val accuracy= 52.400\n",
      "train accuracy= 63.333\n",
      "\n",
      "iteration= 157, train loss(1iter)= 0.825, lr= 0.0005120, time(1iter)= 66.50\n",
      "val accuracy= 53.200\n",
      "train accuracy= 65.000\n",
      "\n",
      "iteration= 158, train loss(1iter)= 0.887, lr= 0.0005120, time(1iter)= 66.20\n",
      "val accuracy= 53.400\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 159, train loss(1iter)= 0.898, lr= 0.0005120, time(1iter)= 64.60\n",
      "val accuracy= 52.800\n",
      "train accuracy= 65.000\n",
      "\n",
      "iteration= 160, train loss(1iter)= 0.833, lr= 0.0005120, time(1iter)= 64.52\n",
      "val accuracy= 53.200\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 161, train loss(1iter)= 0.792, lr= 0.0005120, time(1iter)= 64.78\n",
      "val accuracy= 53.600\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 162, train loss(1iter)= 0.795, lr= 0.0005120, time(1iter)= 67.07\n",
      "val accuracy= 53.600\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 163, train loss(1iter)= 0.845, lr= 0.0005120, time(1iter)= 62.83\n",
      "val accuracy= 53.800\n",
      "train accuracy= 70.000\n",
      "\n",
      "iteration= 164, train loss(1iter)= 0.808, lr= 0.0005120, time(1iter)= 64.25\n",
      "val accuracy= 53.600\n",
      "train accuracy= 71.667\n",
      "\n",
      "iteration= 165, train loss(1iter)= 0.807, lr= 0.0005120, time(1iter)= 65.49\n",
      "val accuracy= 53.200\n",
      "train accuracy= 61.667\n",
      "\n",
      "iteration= 166, train loss(1iter)= 0.770, lr= 0.0005120, time(1iter)= 64.28\n",
      "val accuracy= 53.000\n",
      "train accuracy= 83.333\n",
      "\n",
      "iteration= 167, train loss(1iter)= 0.777, lr= 0.0005120, time(1iter)= 64.35\n",
      "val accuracy= 53.200\n",
      "train accuracy= 71.667\n",
      "\n",
      "iteration= 168, train loss(1iter)= 0.824, lr= 0.0005120, time(1iter)= 65.65\n",
      "val accuracy= 53.800\n",
      "train accuracy= 61.667\n",
      "\n",
      "iteration= 169, train loss(1iter)= 0.819, lr= 0.0005120, time(1iter)= 67.89\n",
      "val accuracy= 54.400\n",
      "train accuracy= 71.667\n",
      "\n",
      "iteration= 170, train loss(1iter)= 0.794, lr= 0.0005120, time(1iter)= 65.59\n",
      "val accuracy= 54.400\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 171, train loss(1iter)= 0.863, lr= 0.0005120, time(1iter)= 67.61\n",
      "val accuracy= 54.600\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 172, train loss(1iter)= 0.779, lr= 0.0005120, time(1iter)= 66.96\n",
      "val accuracy= 55.400\n",
      "train accuracy= 73.333\n",
      "\n",
      "iteration= 173, train loss(1iter)= 0.788, lr= 0.0005120, time(1iter)= 65.05\n",
      "val accuracy= 55.600\n",
      "train accuracy= 75.000\n",
      "\n",
      "iteration= 174, train loss(1iter)= 0.860, lr= 0.0005120, time(1iter)= 66.59\n",
      "val accuracy= 55.600\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 175, train loss(1iter)= 0.757, lr= 0.0005120, time(1iter)= 86.35\n",
      "val accuracy= 56.200\n",
      "train accuracy= 73.333\n",
      "\n",
      "iteration= 176, train loss(1iter)= 0.823, lr= 0.0005120, time(1iter)= 85.37\n",
      "val accuracy= 57.000\n",
      "train accuracy= 70.000\n",
      "\n",
      "iteration= 177, train loss(1iter)= 0.768, lr= 0.0005120, time(1iter)= 86.39\n",
      "val accuracy= 57.200\n",
      "train accuracy= 68.333\n",
      "\n",
      "iteration= 178, train loss(1iter)= 0.744, lr= 0.0005120, time(1iter)= 95.35\n",
      "val accuracy= 57.600\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 179, train loss(1iter)= 0.771, lr= 0.0005120, time(1iter)= 94.46\n",
      "val accuracy= 58.000\n",
      "train accuracy= 80.000\n",
      "\n",
      "iteration= 180, train loss(1iter)= 0.751, lr= 0.0005120, time(1iter)= 85.11\n",
      "val accuracy= 58.400\n",
      "train accuracy= 70.000\n",
      "\n",
      "iteration= 181, train loss(1iter)= 0.782, lr= 0.0005120, time(1iter)= 86.08\n",
      "val accuracy= 59.200\n",
      "train accuracy= 71.667\n",
      "\n",
      "iteration= 182, train loss(1iter)= 0.743, lr= 0.0005120, time(1iter)= 83.64\n",
      "val accuracy= 59.600\n",
      "train accuracy= 71.667\n",
      "\n",
      "iteration= 183, train loss(1iter)= 0.666, lr= 0.0005120, time(1iter)= 84.69\n",
      "val accuracy= 60.200\n",
      "train accuracy= 71.667\n",
      "\n",
      "iteration= 184, train loss(1iter)= 0.737, lr= 0.0005120, time(1iter)= 83.42\n",
      "val accuracy= 60.800\n",
      "train accuracy= 71.667\n",
      "\n",
      "iteration= 185, train loss(1iter)= 0.758, lr= 0.0005120, time(1iter)= 84.74\n",
      "val accuracy= 60.400\n",
      "train accuracy= 73.333\n",
      "\n",
      "iteration= 186, train loss(1iter)= 0.794, lr= 0.0005120, time(1iter)= 84.40\n",
      "val accuracy= 60.600\n",
      "train accuracy= 65.000\n",
      "\n",
      "iteration= 187, train loss(1iter)= 0.728, lr= 0.0005120, time(1iter)= 85.66\n",
      "val accuracy= 60.800\n",
      "train accuracy= 85.000\n",
      "\n",
      "iteration= 188, train loss(1iter)= 0.731, lr= 0.0005120, time(1iter)= 85.66\n",
      "val accuracy= 60.800\n",
      "train accuracy= 71.667\n",
      "\n",
      "iteration= 189, train loss(1iter)= 0.747, lr= 0.0005120, time(1iter)= 88.39\n",
      "val accuracy= 60.800\n",
      "train accuracy= 76.667\n",
      "\n",
      "iteration= 190, train loss(1iter)= 0.766, lr= 0.0005120, time(1iter)= 79.02\n",
      "val accuracy= 61.200\n",
      "train accuracy= 70.000\n",
      "\n",
      "iteration= 191, train loss(1iter)= 0.680, lr= 0.0005120, time(1iter)= 72.68\n",
      "val accuracy= 61.200\n",
      "train accuracy= 80.000\n",
      "\n",
      "iteration= 192, train loss(1iter)= 0.671, lr= 0.0005120, time(1iter)= 70.18\n",
      "val accuracy= 60.800\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 193, train loss(1iter)= 0.675, lr= 0.0005120, time(1iter)= 67.29\n",
      "val accuracy= 60.800\n",
      "train accuracy= 71.667\n",
      "\n",
      "iteration= 194, train loss(1iter)= 0.750, lr= 0.0005120, time(1iter)= 79.61\n",
      "val accuracy= 60.800\n",
      "train accuracy= 81.667\n",
      "\n",
      "iteration= 195, train loss(1iter)= 0.713, lr= 0.0005120, time(1iter)= 85.71\n",
      "val accuracy= 60.400\n",
      "train accuracy= 73.333\n",
      "\n",
      "iteration= 196, train loss(1iter)= 0.640, lr= 0.0005120, time(1iter)= 83.93\n",
      "val accuracy= 60.800\n",
      "train accuracy= 83.333\n",
      "\n",
      "iteration= 197, train loss(1iter)= 0.661, lr= 0.0005120, time(1iter)= 86.34\n",
      "val accuracy= 60.400\n",
      "train accuracy= 80.000\n",
      "\n",
      "iteration= 198, train loss(1iter)= 0.674, lr= 0.0005120, time(1iter)= 84.99\n",
      "val accuracy= 60.600\n",
      "train accuracy= 73.333\n",
      "\n",
      "iteration= 199, train loss(1iter)= 0.690, lr= 0.0005120, time(1iter)= 87.34\n",
      "val accuracy= 61.000\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 200, train loss(1iter)= 0.743, lr= 0.0005120, time(1iter)= 83.92\n",
      "val accuracy= 60.800\n",
      "train accuracy= 60.000\n",
      "\n",
      "iteration= 201, train loss(1iter)= 0.642, lr= 0.0005120, time(1iter)= 86.16\n",
      "val accuracy= 60.600\n",
      "train accuracy= 80.000\n",
      "\n",
      "iteration= 202, train loss(1iter)= 0.649, lr= 0.0005120, time(1iter)= 84.85\n",
      "val accuracy= 60.400\n",
      "train accuracy= 76.667\n",
      "\n",
      "iteration= 203, train loss(1iter)= 0.655, lr= 0.0005120, time(1iter)= 86.42\n",
      "val accuracy= 60.600\n",
      "train accuracy= 76.667\n",
      "\n",
      "iteration= 204, train loss(1iter)= 0.711, lr= 0.0005120, time(1iter)= 83.96\n",
      "val accuracy= 61.800\n",
      "train accuracy= 75.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration= 205, train loss(1iter)= 0.684, lr= 0.0005120, time(1iter)= 84.97\n",
      "val accuracy= 62.000\n",
      "train accuracy= 68.333\n",
      "\n",
      "iteration= 206, train loss(1iter)= 0.681, lr= 0.0005120, time(1iter)= 87.07\n",
      "val accuracy= 62.400\n",
      "train accuracy= 80.000\n",
      "\n",
      "iteration= 207, train loss(1iter)= 0.628, lr= 0.0005120, time(1iter)= 87.61\n",
      "val accuracy= 63.400\n",
      "train accuracy= 81.667\n",
      "\n",
      "iteration= 208, train loss(1iter)= 0.667, lr= 0.0005120, time(1iter)= 86.62\n",
      "val accuracy= 63.200\n",
      "train accuracy= 75.000\n",
      "\n",
      "iteration= 209, train loss(1iter)= 0.591, lr= 0.0005120, time(1iter)= 84.39\n",
      "val accuracy= 63.200\n",
      "train accuracy= 80.000\n",
      "\n",
      "iteration= 210, train loss(1iter)= 0.718, lr= 0.0005120, time(1iter)= 87.78\n",
      "val accuracy= 62.400\n",
      "train accuracy= 75.000\n",
      "\n",
      "iteration= 211, train loss(1iter)= 0.663, lr= 0.0005120, time(1iter)= 87.47\n",
      "val accuracy= 62.600\n",
      "train accuracy= 71.667\n",
      "\n",
      "iteration= 212, train loss(1iter)= 0.707, lr= 0.0005120, time(1iter)= 84.69\n",
      "val accuracy= 62.800\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 213, train loss(1iter)= 0.649, lr= 0.0005120, time(1iter)= 82.84\n",
      "val accuracy= 63.200\n",
      "train accuracy= 80.000\n",
      "Updating LR to 0.0004096\n",
      "\n",
      "iteration= 214, train loss(1iter)= 0.669, lr= 0.0004096, time(1iter)= 83.34\n",
      "val accuracy= 63.200\n",
      "train accuracy= 78.333\n",
      "Updating LR to 0.0003277\n",
      "\n",
      "iteration= 215, train loss(1iter)= 0.627, lr= 0.0003277, time(1iter)= 85.75\n",
      "val accuracy= 63.000\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 216, train loss(1iter)= 0.644, lr= 0.0003277, time(1iter)= 83.80\n",
      "val accuracy= 63.000\n",
      "train accuracy= 71.667\n",
      "Updating LR to 0.0002621\n",
      "\n",
      "iteration= 217, train loss(1iter)= 0.636, lr= 0.0002621, time(1iter)= 81.14\n",
      "val accuracy= 62.800\n",
      "train accuracy= 76.667\n",
      "\n",
      "iteration= 218, train loss(1iter)= 0.559, lr= 0.0002621, time(1iter)= 82.21\n",
      "val accuracy= 62.800\n",
      "train accuracy= 83.333\n",
      "Updating LR to 0.0002097\n",
      "\n",
      "iteration= 219, train loss(1iter)= 0.617, lr= 0.0002097, time(1iter)= 78.53\n",
      "val accuracy= 61.800\n",
      "train accuracy= 73.333\n",
      "\n",
      "iteration= 220, train loss(1iter)= 0.519, lr= 0.0002097, time(1iter)= 80.59\n",
      "val accuracy= 62.200\n",
      "train accuracy= 91.667\n",
      "\n",
      "iteration= 221, train loss(1iter)= 0.637, lr= 0.0002097, time(1iter)= 86.52\n",
      "val accuracy= 62.000\n",
      "train accuracy= 76.667\n",
      "\n",
      "iteration= 222, train loss(1iter)= 0.640, lr= 0.0002097, time(1iter)= 87.26\n",
      "val accuracy= 62.000\n",
      "train accuracy= 86.667\n",
      "\n",
      "iteration= 223, train loss(1iter)= 0.677, lr= 0.0002097, time(1iter)= 85.95\n",
      "val accuracy= 62.000\n",
      "train accuracy= 66.667\n",
      "\n",
      "iteration= 224, train loss(1iter)= 0.596, lr= 0.0002097, time(1iter)= 86.73\n",
      "val accuracy= 62.000\n",
      "train accuracy= 81.667\n",
      "\n",
      "iteration= 225, train loss(1iter)= 0.636, lr= 0.0002097, time(1iter)= 86.43\n",
      "val accuracy= 62.200\n",
      "train accuracy= 76.667\n",
      "\n",
      "iteration= 226, train loss(1iter)= 0.509, lr= 0.0002097, time(1iter)= 85.41\n",
      "val accuracy= 62.400\n",
      "train accuracy= 81.667\n",
      "\n",
      "iteration= 227, train loss(1iter)= 0.598, lr= 0.0002097, time(1iter)= 85.63\n",
      "val accuracy= 62.800\n",
      "train accuracy= 76.667\n",
      "\n",
      "iteration= 228, train loss(1iter)= 0.609, lr= 0.0002097, time(1iter)= 85.99\n",
      "val accuracy= 62.600\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 229, train loss(1iter)= 0.539, lr= 0.0002097, time(1iter)= 85.68\n",
      "val accuracy= 63.200\n",
      "train accuracy= 90.000\n",
      "\n",
      "iteration= 230, train loss(1iter)= 0.648, lr= 0.0002097, time(1iter)= 79.85\n",
      "val accuracy= 64.000\n",
      "train accuracy= 68.333\n",
      "\n",
      "iteration= 231, train loss(1iter)= 0.586, lr= 0.0002097, time(1iter)= 85.58\n",
      "val accuracy= 63.600\n",
      "train accuracy= 86.667\n",
      "\n",
      "iteration= 232, train loss(1iter)= 0.567, lr= 0.0002097, time(1iter)= 82.78\n",
      "val accuracy= 64.000\n",
      "train accuracy= 76.667\n",
      "\n",
      "iteration= 233, train loss(1iter)= 0.539, lr= 0.0002097, time(1iter)= 81.96\n",
      "val accuracy= 63.800\n",
      "train accuracy= 85.000\n",
      "\n",
      "iteration= 234, train loss(1iter)= 0.584, lr= 0.0002097, time(1iter)= 86.15\n",
      "val accuracy= 63.400\n",
      "train accuracy= 81.667\n",
      "\n",
      "iteration= 235, train loss(1iter)= 0.604, lr= 0.0002097, time(1iter)= 88.62\n",
      "val accuracy= 63.200\n",
      "train accuracy= 83.333\n",
      "\n",
      "iteration= 236, train loss(1iter)= 0.546, lr= 0.0002097, time(1iter)= 90.20\n",
      "val accuracy= 64.000\n",
      "train accuracy= 85.000\n",
      "\n",
      "iteration= 237, train loss(1iter)= 0.571, lr= 0.0002097, time(1iter)= 90.80\n",
      "val accuracy= 63.400\n",
      "train accuracy= 83.333\n",
      "\n",
      "iteration= 238, train loss(1iter)= 0.580, lr= 0.0002097, time(1iter)= 91.42\n",
      "val accuracy= 62.800\n",
      "train accuracy= 81.667\n",
      "\n",
      "iteration= 239, train loss(1iter)= 0.607, lr= 0.0002097, time(1iter)= 91.41\n",
      "val accuracy= 63.200\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 240, train loss(1iter)= 0.599, lr= 0.0002097, time(1iter)= 81.09\n",
      "val accuracy= 63.000\n",
      "train accuracy= 85.000\n",
      "\n",
      "iteration= 241, train loss(1iter)= 0.578, lr= 0.0002097, time(1iter)= 68.06\n",
      "val accuracy= 63.200\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 242, train loss(1iter)= 0.596, lr= 0.0002097, time(1iter)= 68.71\n",
      "val accuracy= 63.000\n",
      "train accuracy= 73.333\n",
      "\n",
      "iteration= 243, train loss(1iter)= 0.527, lr= 0.0002097, time(1iter)= 66.32\n",
      "val accuracy= 63.600\n",
      "train accuracy= 83.333\n",
      "\n",
      "iteration= 244, train loss(1iter)= 0.568, lr= 0.0002097, time(1iter)= 67.53\n",
      "val accuracy= 63.800\n",
      "train accuracy= 88.333\n",
      "\n",
      "iteration= 245, train loss(1iter)= 0.542, lr= 0.0002097, time(1iter)= 67.17\n",
      "val accuracy= 63.600\n",
      "train accuracy= 81.667\n",
      "\n",
      "iteration= 246, train loss(1iter)= 0.536, lr= 0.0002097, time(1iter)= 65.42\n",
      "val accuracy= 63.600\n",
      "train accuracy= 85.000\n",
      "\n",
      "iteration= 247, train loss(1iter)= 0.494, lr= 0.0002097, time(1iter)= 65.64\n",
      "val accuracy= 63.800\n",
      "train accuracy= 85.000\n",
      "\n",
      "iteration= 248, train loss(1iter)= 0.592, lr= 0.0002097, time(1iter)= 65.14\n",
      "val accuracy= 63.600\n",
      "train accuracy= 83.333\n",
      "\n",
      "iteration= 249, train loss(1iter)= 0.523, lr= 0.0002097, time(1iter)= 66.68\n",
      "val accuracy= 63.400\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 250, train loss(1iter)= 0.562, lr= 0.0002097, time(1iter)= 65.56\n",
      "val accuracy= 63.600\n",
      "train accuracy= 85.000\n",
      "\n",
      "iteration= 251, train loss(1iter)= 0.551, lr= 0.0002097, time(1iter)= 64.53\n",
      "val accuracy= 64.000\n",
      "train accuracy= 85.000\n",
      "\n",
      "iteration= 252, train loss(1iter)= 0.556, lr= 0.0002097, time(1iter)= 65.52\n",
      "val accuracy= 64.000\n",
      "train accuracy= 85.000\n",
      "\n",
      "iteration= 253, train loss(1iter)= 0.552, lr= 0.0002097, time(1iter)= 67.34\n",
      "val accuracy= 63.600\n",
      "train accuracy= 83.333\n",
      "\n",
      "iteration= 254, train loss(1iter)= 0.556, lr= 0.0002097, time(1iter)= 67.08\n",
      "val accuracy= 63.600\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 255, train loss(1iter)= 0.579, lr= 0.0002097, time(1iter)= 66.79\n",
      "val accuracy= 63.600\n",
      "train accuracy= 80.000\n",
      "\n",
      "iteration= 256, train loss(1iter)= 0.605, lr= 0.0002097, time(1iter)= 66.57\n",
      "val accuracy= 63.400\n",
      "train accuracy= 73.333\n",
      "\n",
      "iteration= 257, train loss(1iter)= 0.515, lr= 0.0002097, time(1iter)= 65.85\n",
      "val accuracy= 64.200\n",
      "train accuracy= 80.000\n",
      "\n",
      "iteration= 258, train loss(1iter)= 0.566, lr= 0.0002097, time(1iter)= 65.96\n",
      "val accuracy= 63.800\n",
      "train accuracy= 76.667\n",
      "\n",
      "iteration= 259, train loss(1iter)= 0.614, lr= 0.0002097, time(1iter)= 69.50\n",
      "val accuracy= 63.800\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 260, train loss(1iter)= 0.496, lr= 0.0002097, time(1iter)= 63.67\n",
      "val accuracy= 63.800\n",
      "train accuracy= 85.000\n",
      "\n",
      "iteration= 261, train loss(1iter)= 0.551, lr= 0.0002097, time(1iter)= 64.64\n",
      "val accuracy= 63.600\n",
      "train accuracy= 88.333\n",
      "\n",
      "iteration= 262, train loss(1iter)= 0.550, lr= 0.0002097, time(1iter)= 66.19\n",
      "val accuracy= 63.400\n",
      "train accuracy= 80.000\n",
      "\n",
      "iteration= 263, train loss(1iter)= 0.541, lr= 0.0002097, time(1iter)= 64.43\n",
      "val accuracy= 63.400\n",
      "train accuracy= 83.333\n",
      "\n",
      "iteration= 264, train loss(1iter)= 0.518, lr= 0.0002097, time(1iter)= 64.56\n",
      "val accuracy= 63.200\n",
      "train accuracy= 88.333\n",
      "\n",
      "iteration= 265, train loss(1iter)= 0.595, lr= 0.0002097, time(1iter)= 64.46\n",
      "val accuracy= 63.000\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 266, train loss(1iter)= 0.491, lr= 0.0002097, time(1iter)= 65.73\n",
      "val accuracy= 63.400\n",
      "train accuracy= 88.333\n",
      "\n",
      "iteration= 267, train loss(1iter)= 0.520, lr= 0.0002097, time(1iter)= 63.98\n",
      "val accuracy= 63.000\n",
      "train accuracy= 88.333\n",
      "\n",
      "iteration= 268, train loss(1iter)= 0.531, lr= 0.0002097, time(1iter)= 65.06\n",
      "val accuracy= 62.400\n",
      "train accuracy= 83.333\n",
      "\n",
      "iteration= 269, train loss(1iter)= 0.546, lr= 0.0002097, time(1iter)= 65.24\n",
      "val accuracy= 62.400\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 270, train loss(1iter)= 0.538, lr= 0.0002097, time(1iter)= 66.75\n",
      "val accuracy= 62.400\n",
      "train accuracy= 85.000\n",
      "\n",
      "iteration= 271, train loss(1iter)= 0.528, lr= 0.0002097, time(1iter)= 66.02\n",
      "val accuracy= 62.600\n",
      "train accuracy= 81.667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating LR to 0.0001678\n",
      "\n",
      "iteration= 272, train loss(1iter)= 0.492, lr= 0.0001678, time(1iter)= 66.05\n",
      "val accuracy= 62.600\n",
      "train accuracy= 83.333\n",
      "\n",
      "iteration= 273, train loss(1iter)= 0.497, lr= 0.0001678, time(1iter)= 66.28\n",
      "val accuracy= 63.000\n",
      "train accuracy= 83.333\n",
      "Updating LR to 0.0001342\n",
      "\n",
      "iteration= 274, train loss(1iter)= 0.446, lr= 0.0001342, time(1iter)= 63.87\n",
      "val accuracy= 63.400\n",
      "train accuracy= 88.333\n",
      "Updating LR to 0.0001074\n",
      "\n",
      "iteration= 275, train loss(1iter)= 0.554, lr= 0.0001074, time(1iter)= 67.03\n",
      "val accuracy= 63.600\n",
      "train accuracy= 78.333\n",
      "\n",
      "iteration= 276, train loss(1iter)= 0.537, lr= 0.0001074, time(1iter)= 67.45\n",
      "val accuracy= 63.200\n",
      "train accuracy= 88.333\n",
      "\n",
      "iteration= 277, train loss(1iter)= 0.577, lr= 0.0001074, time(1iter)= 65.10\n",
      "val accuracy= 63.200\n",
      "train accuracy= 76.667\n",
      "\n",
      "iteration= 278, train loss(1iter)= 0.527, lr= 0.0001074, time(1iter)= 64.98\n",
      "val accuracy= 62.800\n",
      "train accuracy= 85.000\n",
      "\n",
      "iteration= 279, train loss(1iter)= 0.482, lr= 0.0001074, time(1iter)= 64.99\n",
      "val accuracy= 63.200\n",
      "train accuracy= 88.333\n",
      "\n",
      "iteration= 280, train loss(1iter)= 0.516, lr= 0.0001074, time(1iter)= 64.82\n",
      "val accuracy= 63.200\n",
      "train accuracy= 86.667\n",
      "\n",
      "iteration= 281, train loss(1iter)= 0.534, lr= 0.0001074, time(1iter)= 66.35\n",
      "val accuracy= 62.800\n",
      "train accuracy= 81.667\n",
      "\n",
      "iteration= 282, train loss(1iter)= 0.560, lr= 0.0001074, time(1iter)= 66.29\n",
      "val accuracy= 62.800\n",
      "train accuracy= 83.333\n",
      "Updating LR to 0.0000859\n",
      "\n",
      "iteration= 283, train loss(1iter)= 0.568, lr= 0.0000859, time(1iter)= 64.92\n",
      "val accuracy= 63.000\n",
      "train accuracy= 78.333\n",
      "Updating LR to 0.0000687\n",
      "\n",
      "iteration= 284, train loss(1iter)= 0.514, lr= 0.0000687, time(1iter)= 66.34\n",
      "val accuracy= 63.200\n",
      "train accuracy= 86.667\n",
      "Updating LR to 0.0000550\n",
      "\n",
      "iteration= 285, train loss(1iter)= 0.535, lr= 0.0000550, time(1iter)= 65.09\n",
      "val accuracy= 63.000\n",
      "train accuracy= 80.000\n",
      "Updating LR to 0.0000440\n",
      "\n",
      "iteration= 286, train loss(1iter)= 0.567, lr= 0.0000440, time(1iter)= 65.55\n",
      "val accuracy= 63.400\n",
      "train accuracy= 81.667\n",
      "Early Stopping at 286. Highest Val: 64.200 \n"
     ]
    }
   ],
   "source": [
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    net_parameters['L'] = 4\n",
    "    net_parameters['Dropout_fc'] = 0.5\n",
    "    net_parameters['Dropout_edge'] = 0.5\n",
    "    net_parameters['Dropout_in'] = 0.0\n",
    "    net = Graph_OurConvNet(net_parameters, CORA)\n",
    "\n",
    "    # number of network parameters\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += np.prod(list(param.data.size()))\n",
    "    print('nb_param=',nb_param,' L=',net_parameters['L'])\n",
    "    \n",
    "    lr = 0.001\n",
    "    l2 = 0.005\n",
    "    batch_iters = 1\n",
    "    early_stopping = 5e-5\n",
    "    train(net, lr, l2, batch_iters, early_stopping, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss(100 pre-saved data)= 0.926, accuracy(100 pre-saved data)= 61.500\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = 'model_state'\n",
    "nb_classes = 3\n",
    "net.load_state_dict(torch.load(SAVE_PATH))\n",
    "net.eval()\n",
    "# features_x, train_y, E_start, E_end, E_identity, E_dropin = get_cora_dataset(CUDA)\n",
    "y_eval = net.forward(features_x, E_start, E_end, E_identity, E_dropin)\n",
    "\n",
    "loss = net.loss(y_eval[idx_test], labels[idx_test], None) \n",
    "accuracy = calculate_avg_accuracy(nb_classes, labels[idx_test], y_eval[idx_test])\n",
    "print('\\nloss(100 pre-saved data)= %.3f, accuracy(100 pre-saved data)= %.3f' % (loss.item(), 100* accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'restart': True, 'status': 'ok'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
