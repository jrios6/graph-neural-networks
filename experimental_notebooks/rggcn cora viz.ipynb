{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pdb \n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sys\n",
    "import os \n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    torch.cuda.manual_seed(seed)\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "# Helper methods for loading CORA Graph\n",
    "from utils import load_data2,load_data3,accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Cora Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data (GCN)\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, labels = load_data3('cora')\n",
    "adj = adj.toarray().astype(float)\n",
    "adj += np.eye(adj.shape[0])\n",
    "idx_train = np.argwhere(train_mask).reshape(-1)\n",
    "idx_val = np.argwhere(val_mask).reshape(-1)\n",
    "idx_test = np.argwhere(test_mask).reshape(-1)\n",
    "labels = torch.LongTensor(np.where(labels)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_edges = []\n",
    "for v1 in idx_train:\n",
    "    for v2 in idx_train:\n",
    "        if v1 != v2 and adj[v1, v2] != 1: # and labels[v1] == labels[v2]:\n",
    "            new_edges.append((v1,v2))\n",
    "new_edges = np.array(new_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropin(new_edges, rate, dim=2708):\n",
    "    np.random.shuffle(new_edges)\n",
    "    v = new_edges.shape[0]\n",
    "    E_start = np.zeros((v, dim))\n",
    "    E_end = np.zeros((v, dim))\n",
    "    for i in range(0, int(v*rate), 2):\n",
    "        v1, v2 = new_edges[i]\n",
    "        E_start[i,v1] = E_end[i,v2] = E_start[i+1,v1] = E_end[i+1,v2] = 1\n",
    "    E_start = Variable(torch.from_numpy(E_start[:i+2,:]).float())\n",
    "    E_end = Variable(torch.from_numpy(E_end[:i+2,:]).float())\n",
    "    return E_start.cuda(), E_end.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data (pyGCN)\n",
    "# adj, features, labels, idx_train, idx_val, idx_test = load_data2()\n",
    "\n",
    "# adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "# adj = adj.toarray()\n",
    "# adj += np.eye(adj.shape[0])\n",
    "\n",
    "cora_Estart = np.zeros((20000, 2708))\n",
    "cora_Eend = np.zeros((20000, 2708))\n",
    "cora_Eidentity = [] # idx of identity edges\n",
    "\n",
    "# converting adjacency matrix to edge-to-start, edge-to-end vertex matrix\n",
    "count = 0\n",
    "for i in range(adj.shape[0]):\n",
    "    for j in range(adj.shape[1]):\n",
    "        if adj[i,j] == 1:\n",
    "            cora_Estart[count,i] = 1\n",
    "            cora_Eend[count,j] = 1\n",
    "            if i == j:\n",
    "                cora_Eidentity.append(count)\n",
    "            count += 1\n",
    "cora_Estart = cora_Estart[:count]\n",
    "cora_Eend = cora_Eend[:count]\n",
    "\n",
    "def get_cora_dataset():\n",
    "    x = Variable(features, requires_grad=False)\n",
    "    y = Variable(labels)\n",
    "    E_start = Variable(torch.from_numpy(cora_Estart).float())\n",
    "    E_end = Variable(torch.from_numpy(cora_Eend).float())\n",
    "    \n",
    "    return x.cuda(), y.cuda(), E_start.cuda(), E_end.cuda(), cora_Eidentity, new_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OurConvNetcell(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dropout_fc=0, dropout_edge=0):\n",
    "        super(OurConvNetcell, self).__init__()\n",
    "    \n",
    "        # conv1\n",
    "        self.Ui1 = nn.Linear(dim_in, dim_out, bias=False) \n",
    "        self.Vi1 = nn.Linear(dim_in, dim_out, bias=False) \n",
    "        self.Vj1 = nn.Linear(dim_in, dim_out, bias=False)  \n",
    "        self.bu1 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        self.bv1 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        \n",
    "        self.dropout_fc = dropout_fc\n",
    "        self.dropout_edge = dropout_edge\n",
    "        \n",
    "        # conv2\n",
    "        self.Ui2 = nn.Linear(dim_out, dim_out, bias=False) \n",
    "        self.Vi2 = nn.Linear(dim_out, dim_out, bias=False) \n",
    "        self.Vj2 = nn.Linear(dim_out, dim_out, bias=False)  \n",
    "        self.bu2 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        self.bv2 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        \n",
    "        # bn1, bn2\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim_out)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim_out)\n",
    "        \n",
    "        # resnet\n",
    "        self.R = nn.Linear(dim_in, dim_out, bias=False) \n",
    "        \n",
    "        # init\n",
    "        self.init_weights_OurConvNetcell(dim_in, dim_out, 1)\n",
    "        \n",
    "         \n",
    "    def init_weights_OurConvNetcell(self, dim_in, dim_out, gain):   \n",
    "        # conv1\n",
    "        scale = gain* np.sqrt( 2.0/ dim_in )\n",
    "        self.Ui1.weight.data.uniform_(-scale, scale) \n",
    "        self.Vi1.weight.data.uniform_(-scale, scale) \n",
    "        self.Vj1.weight.data.uniform_(-scale, scale) \n",
    "        self.bu1.data.fill_(0)\n",
    "        self.bv1.data.fill_(0)\n",
    "        \n",
    "        # conv2\n",
    "        scale = gain* np.sqrt( 2.0/ dim_out )\n",
    "        self.Ui2.weight.data.uniform_(-scale, scale) \n",
    "        self.Vi2.weight.data.uniform_(-scale, scale) \n",
    "        self.Vj2.weight.data.uniform_(-scale, scale) \n",
    "        self.bu2.data.fill_(0)\n",
    "        self.bv2.data.fill_(0)\n",
    "        \n",
    "        # RN\n",
    "        scale = gain* np.sqrt( 2.0/ dim_in )\n",
    "        self.R.weight.data.uniform_(-scale, scale)  \n",
    "        \n",
    "        \n",
    "    def forward(self, x, E_start, E_end):\n",
    "        x = F.dropout(x, self.dropout_fc, training=self.training)\n",
    "        xin = x\n",
    "        \n",
    "        # edge norm\n",
    "        norm = torch.sum(E_end.t(), 1).reshape(-1,1)\n",
    "#         norm = torch.max(norm, torch.ones(norm.shape).cuda())\n",
    "\n",
    "        # conv1\n",
    "        Uix = self.Ui1(x)  #  V x H_out\n",
    "        Vix = self.Vi1(x)  #  V x H_out\n",
    "        Vjx = self.Vj1(x)  #  V x H_out\n",
    "        x1 = torch.mm(E_end,Vix) + torch.mm(E_start,Vjx) + self.bv1  # E x H_out\n",
    "        x1 = torch.sigmoid(x1)\n",
    "#         x1 = F.dropout(x1, self.dropout_fc, training=self.training)\n",
    "\n",
    "        x2 = torch.mm(E_start, Uix)  #  E x H_out\n",
    "        x = torch.mm(E_end.t(), x1*x2) + self.bu1 #  V x H_out\n",
    "        \n",
    "        x = torch.div(x, norm)# norm\n",
    "        x = self.bn1(x) # bn1\n",
    "        x = torch.nn.LeakyReLU(0.2)(x) # relu1\n",
    "        \n",
    "        # conv2\n",
    "        Uix = self.Ui2(x)  #  V x H_out\n",
    "        Vix = self.Vi2(x)  #  V x H_out\n",
    "        Vjx = self.Vj2(x)  #  V x H_out\n",
    "        x1 = torch.mm(E_end,Vix) + torch.mm(E_start,Vjx) + self.bv2  # E x H_out\n",
    "        x1 = torch.sigmoid(x1)\n",
    "#         x1 = F.dropout(x1, self.dropout_fc, training=self.training)\n",
    "        \n",
    "        x2 = torch.mm(E_start, Uix)  #  V x H_out        \n",
    "        x = torch.mm(E_end.t(), x1*x2) + self.bu2 #  V x H_out\n",
    "        \n",
    "        x = torch.div(x, norm) # normalization\n",
    "        \n",
    "        x = self.bn2(x) # bn2\n",
    "        x = x + self.R(xin) # addition\n",
    "        x = torch.nn.LeakyReLU(0.2)(x) # relu2\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class Graph_OurConvNet(nn.Module):\n",
    "    def __init__(self, net_parameters, cora=False):\n",
    "        super(Graph_OurConvNet, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        Voc = net_parameters['Voc']\n",
    "        D = net_parameters['D']\n",
    "        nb_clusters_target = net_parameters['nb_clusters_target']\n",
    "        H = net_parameters['H']\n",
    "        L = net_parameters['L']\n",
    "        self.cora = cora\n",
    "        self.dropout_fc = net_parameters['Dropout_fc']\n",
    "        self.dropout_edge = net_parameters['Dropout_edge']\n",
    "        self.drop_in = net_parameters['Dropout_in']\n",
    "        \n",
    "        # vector of hidden dimensions\n",
    "        net_layers = []\n",
    "        for layer in range(L):\n",
    "            net_layers.append(H)\n",
    "        \n",
    "        # CL cells\n",
    "        # NOTE: Each graph convnet cell uses *TWO* convolutional operations\n",
    "        net_layers_extended = [net_parameters['features']] + net_layers \n",
    "        \n",
    "        L = len(net_layers)\n",
    "        list_of_gnn_cells = [] # list of NN cells\n",
    "        for layer in range(L//2):\n",
    "            Hin, Hout = net_layers_extended[2*layer], net_layers_extended[2*layer+2]\n",
    "            list_of_gnn_cells.append(OurConvNetcell(Hin,Hout, self.dropout_fc, self.dropout_edge))\n",
    "        \n",
    "        # register the cells for pytorch\n",
    "        self.gnn_cells = nn.ModuleList(list_of_gnn_cells)\n",
    "            \n",
    "        # fc\n",
    "        Hfinal = net_layers_extended[-1]\n",
    "        self.fc = nn.Linear(Hfinal,nb_clusters_target) \n",
    "        \n",
    "        # init\n",
    "        self.init_weights_Graph_OurConvNet(Voc,D,Hfinal,nb_clusters_target,1)\n",
    "        \n",
    "        # print\n",
    "#         print('\\nnb of hidden layers=',L)\n",
    "#         print('dim of layers (w/ embed dim)=',net_layers_extended)      \n",
    "#         print('\\n')\n",
    "        \n",
    "        # class variables\n",
    "        self.D = D\n",
    "        self.L = L\n",
    "        self.net_layers_extended = net_layers_extended      \n",
    "        \n",
    "        \n",
    "    def init_weights_Graph_OurConvNet(self, Fin_enc, Fout_enc, Fin_fc, Fout_fc, gain):\n",
    "        scale = gain* np.sqrt(2.0/ (Fin_fc+Fout_fc))\n",
    "        self.fc.weight.data.uniform_(-scale, scale)  \n",
    "        self.fc.bias.data.fill_(0)  \n",
    "        \n",
    "    def forward(self, x, E_start, E_end, E_identity, E_dropin):\n",
    "        if self.training:\n",
    "            # Edge Start+End Dropout for all layers\n",
    "            num_edges = E_start.shape[0]\n",
    "            dropout_idx = np.array([i for i in range(num_edges) if i not in E_identity])\n",
    "            np.random.shuffle(dropout_idx)\n",
    "            E_start = E_start.clone()\n",
    "            E_start[dropout_idx[:int(num_edges*self.dropout_edge)]] = 0\n",
    "            E_end = E_end.clone()\n",
    "            E_end[dropout_idx[:int(num_edges*self.dropout_edge)]] = 0\n",
    "            \n",
    "            # Dropin\n",
    "            D_start, D_end = dropin(E_dropin, self.drop_in)\n",
    "            E_start = torch.cat((E_start, D_start), 0)\n",
    "            E_end = torch.cat((E_end, D_end), 0)\n",
    "            \n",
    "        # convnet cells  \n",
    "        for layer in range(self.L//2):\n",
    "            gnn_layer = self.gnn_cells[layer]            \n",
    "            x = gnn_layer(x,E_start,E_end) # V x H\n",
    "            \n",
    "        x = F.dropout(x, self.dropout_fc, training=self.training) #FC Dropout\n",
    "        x = self.fc(x) # FC\n",
    "        return x\n",
    "         \n",
    "    def loss(self, y, y_target, weight):\n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "        return loss\n",
    "       \n",
    "    def update(self, lr, l2):\n",
    "        update = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "        return update\n",
    "    \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return optimizer\n",
    "    \n",
    "    def nb_param(self):\n",
    "        return self.nb_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_avg_accuracy(nb_classes, labels, pred_y):\n",
    "    S = labels.data.cpu().numpy()\n",
    "    C = np.argmax(torch.nn.Softmax(dim=1)(pred_y).data.cpu().numpy() , axis=1)\n",
    "    return np.sum(S==C)/S.shape[0]\n",
    "\n",
    "def update_lr(net, optimizer, average_loss, average_loss_old, lr, decay_rate, early_stopping, verbose):\n",
    "    # Update LR if > early_stopping and avg val loss is higher\n",
    "    if average_loss > average_loss_old and lr > early_stopping:\n",
    "        lr /= decay_rate\n",
    "        if verbose:\n",
    "            print('Updating LR to %.7f' % lr)\n",
    "    return net.update_learning_rate(optimizer, lr), lr\n",
    "\n",
    "def print_results(iteration, batch_iters, avg_train_acc, running_train_loss, val_accuracy, lr, t_start):\n",
    "    print('\\niteration= %d, train loss(%diter)= %.3f, lr= %.7f, time(%diter)= %.2f' % \n",
    "          (iteration, batch_iters, running_train_loss/batch_iters, lr, \n",
    "           batch_iters, time.time() - t_start))\n",
    "    print('val accuracy= %.3f' % (100* val_accuracy))\n",
    "    print('train accuracy= %.3f' % (100* avg_train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, lr, l2, batch_iters, early_stopping, verbose=False):\n",
    "    ### optimization parameters\n",
    "    nb_classes = 7 \n",
    "    max_iters = 1000\n",
    "    decay_rate = 1.25\n",
    "    SAVE_PATH = 'model_state'\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = net.update(lr, l2) \n",
    "    t_start = time.time()\n",
    "    t_start_total = time.time()\n",
    "    average_loss_old = torch.tensor(1e4).cuda()\n",
    "    best = running_train_acc = running_train_loss = running_val_loss = 0.0\n",
    "    tab_results = []\n",
    "    \n",
    "    features_x, train_y, E_start, E_end, E_identity, E_dropin = get_cora_dataset()\n",
    "\n",
    "    for iteration in range(1, max_iters):  # loop over the dataset multiple times\n",
    "        # forward, loss\n",
    "        net.train()\n",
    "        pred_y = net.forward(features_x, E_start, E_end, E_identity, E_dropin)\n",
    "        loss = net.loss(pred_y[idx_train], train_y[idx_train], None) \n",
    "        train_acc = calculate_avg_accuracy(nb_classes, train_y[idx_train], pred_y[idx_train]) # training acc\n",
    "        running_train_acc += train_acc    \n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "        # backward, update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # validation eval\n",
    "        net.eval()\n",
    "        y_eval = net.forward(features_x, E_start, E_end, E_identity, E_dropin)\n",
    "        val_loss = net.loss(y_eval[idx_val], train_y[idx_val], None) \n",
    "        running_val_loss += val_loss.item()\n",
    "\n",
    "        # learning rate, print results\n",
    "        if not iteration%batch_iters:\n",
    "            val_accuracy = calculate_avg_accuracy(nb_classes, train_y[idx_val], y_eval[idx_val])\n",
    "            average_val_loss = running_val_loss/ batch_iters\n",
    "            avg_train_acc = running_train_acc/ batch_iters\n",
    "\n",
    "            # update learning rate \n",
    "            if val_accuracy < avg_train_acc:\n",
    "                optimizer, lr = update_lr(net, optimizer, average_val_loss, average_loss_old, \n",
    "                                          lr, decay_rate, early_stopping, verbose)\n",
    "\n",
    "            # save intermediate results\n",
    "            if val_accuracy > best:\n",
    "                torch.save(net.state_dict(), SAVE_PATH)\n",
    "                best = val_accuracy\n",
    "            tab_results.append([iteration,average_val_loss,100* val_accuracy, time.time()-t_start_total])\n",
    "\n",
    "            if verbose:\n",
    "                print_results(iteration, batch_iters, avg_train_acc, running_train_loss, val_accuracy, lr, t_start)\n",
    "            if lr < torch.tensor(early_stopping).cuda() and avg_train_acc - val_accuracy > 0.05:\n",
    "                print(\"Early Stopping at %d. Highest Val: %.3f \" % (iteration, max([tab_results[i][2] for i in range(len(tab_results))])))\n",
    "                return max([tab_results[i][2] for i in range(len(tab_results))])\n",
    "                break\n",
    "\n",
    "            # reset counters\n",
    "            t_start = time.time()\n",
    "            running_train_acc = running_train_loss = running_val_loss = 0.0\n",
    "            average_loss_old = average_val_loss\n",
    "    return max([tab_results[i][2] for i in range(len(tab_results))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_x, _, _, _, _, _ = get_cora_dataset()\n",
    "\n",
    "CORA = 1\n",
    "net_parameters = {}\n",
    "net_parameters['features'] = features_x.shape[1]\n",
    "net_parameters['Voc'] = 7+1 \n",
    "net_parameters['nb_clusters_target'] = 7\n",
    "net_parameters['D'] = net_parameters['H'] = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Layers=2, L2=0.00500, LR=0.001000\n",
      "\n",
      "iteration= 20, train loss(20iter)= 1.694, lr= 0.0010000, time(20iter)= 19.53\n",
      "val accuracy= 31.600\n",
      "train accuracy= 43.679\n",
      "\n",
      "iteration= 40, train loss(20iter)= 1.137, lr= 0.0010000, time(20iter)= 17.21\n",
      "val accuracy= 32.600\n",
      "train accuracy= 82.071\n",
      "\n",
      "iteration= 60, train loss(20iter)= 0.713, lr= 0.0010000, time(20iter)= 17.23\n",
      "val accuracy= 44.400\n",
      "train accuracy= 91.857\n",
      "\n",
      "iteration= 80, train loss(20iter)= 0.445, lr= 0.0010000, time(20iter)= 17.49\n",
      "val accuracy= 65.800\n",
      "train accuracy= 96.107\n",
      "\n",
      "iteration= 100, train loss(20iter)= 0.315, lr= 0.0010000, time(20iter)= 17.52\n",
      "val accuracy= 73.200\n",
      "train accuracy= 96.964\n",
      "\n",
      "iteration= 120, train loss(20iter)= 0.254, lr= 0.0010000, time(20iter)= 20.37\n",
      "val accuracy= 72.600\n",
      "train accuracy= 97.714\n",
      "\n",
      "iteration= 140, train loss(20iter)= 0.198, lr= 0.0010000, time(20iter)= 19.38\n",
      "val accuracy= 71.400\n",
      "train accuracy= 97.750\n",
      "\n",
      "iteration= 160, train loss(20iter)= 0.174, lr= 0.0010000, time(20iter)= 17.56\n",
      "val accuracy= 72.600\n",
      "train accuracy= 97.714\n",
      "\n",
      "iteration= 180, train loss(20iter)= 0.180, lr= 0.0010000, time(20iter)= 17.37\n",
      "val accuracy= 73.000\n",
      "train accuracy= 97.964\n",
      "\n",
      "iteration= 200, train loss(20iter)= 0.151, lr= 0.0010000, time(20iter)= 17.28\n",
      "val accuracy= 74.400\n",
      "train accuracy= 97.929\n",
      "Updating LR to 0.0008000\n",
      "\n",
      "iteration= 220, train loss(20iter)= 0.158, lr= 0.0008000, time(20iter)= 17.79\n",
      "val accuracy= 73.400\n",
      "train accuracy= 97.571\n",
      "Updating LR to 0.0006400\n",
      "\n",
      "iteration= 240, train loss(20iter)= 0.136, lr= 0.0006400, time(20iter)= 17.86\n",
      "val accuracy= 70.600\n",
      "train accuracy= 98.036\n",
      "Updating LR to 0.0005120\n",
      "\n",
      "iteration= 260, train loss(20iter)= 0.130, lr= 0.0005120, time(20iter)= 17.86\n",
      "val accuracy= 72.600\n",
      "train accuracy= 98.143\n",
      "Updating LR to 0.0004096\n",
      "\n",
      "iteration= 280, train loss(20iter)= 0.136, lr= 0.0004096, time(20iter)= 17.86\n",
      "val accuracy= 69.600\n",
      "train accuracy= 97.786\n",
      "\n",
      "iteration= 300, train loss(20iter)= 0.122, lr= 0.0004096, time(20iter)= 17.85\n",
      "val accuracy= 70.600\n",
      "train accuracy= 98.107\n",
      "Updating LR to 0.0003277\n",
      "\n",
      "iteration= 320, train loss(20iter)= 0.109, lr= 0.0003277, time(20iter)= 18.17\n",
      "val accuracy= 70.400\n",
      "train accuracy= 98.357\n",
      "\n",
      "iteration= 340, train loss(20iter)= 0.114, lr= 0.0003277, time(20iter)= 17.87\n",
      "val accuracy= 72.600\n",
      "train accuracy= 98.107\n",
      "Updating LR to 0.0002621\n",
      "\n",
      "iteration= 360, train loss(20iter)= 0.111, lr= 0.0002621, time(20iter)= 17.86\n",
      "val accuracy= 71.600\n",
      "train accuracy= 98.250\n",
      "Updating LR to 0.0002097\n",
      "\n",
      "iteration= 380, train loss(20iter)= 0.101, lr= 0.0002097, time(20iter)= 17.92\n",
      "val accuracy= 70.400\n",
      "train accuracy= 98.143\n",
      "\n",
      "iteration= 400, train loss(20iter)= 0.103, lr= 0.0002097, time(20iter)= 18.01\n",
      "val accuracy= 71.400\n",
      "train accuracy= 98.357\n",
      "Updating LR to 0.0001678\n",
      "\n",
      "iteration= 420, train loss(20iter)= 0.118, lr= 0.0001678, time(20iter)= 17.79\n",
      "val accuracy= 71.800\n",
      "train accuracy= 98.393\n",
      "Updating LR to 0.0001342\n",
      "\n",
      "iteration= 440, train loss(20iter)= 0.118, lr= 0.0001342, time(20iter)= 17.81\n",
      "val accuracy= 71.400\n",
      "train accuracy= 97.964\n",
      "\n",
      "iteration= 460, train loss(20iter)= 0.107, lr= 0.0001342, time(20iter)= 17.83\n",
      "val accuracy= 73.000\n",
      "train accuracy= 98.107\n",
      "\n",
      "iteration= 480, train loss(20iter)= 0.116, lr= 0.0001342, time(20iter)= 17.81\n",
      "val accuracy= 72.800\n",
      "train accuracy= 98.393\n",
      "Updating LR to 0.0001074\n",
      "\n",
      "iteration= 500, train loss(20iter)= 0.106, lr= 0.0001074, time(20iter)= 17.66\n",
      "val accuracy= 73.000\n",
      "train accuracy= 98.107\n",
      "Updating LR to 0.0000859\n",
      "\n",
      "iteration= 520, train loss(20iter)= 0.099, lr= 0.0000859, time(20iter)= 17.33\n",
      "val accuracy= 72.800\n",
      "train accuracy= 98.536\n",
      "Updating LR to 0.0000687\n",
      "\n",
      "iteration= 540, train loss(20iter)= 0.101, lr= 0.0000687, time(20iter)= 17.28\n",
      "val accuracy= 72.400\n",
      "train accuracy= 98.500\n",
      "\n",
      "iteration= 560, train loss(20iter)= 0.100, lr= 0.0000687, time(20iter)= 17.42\n",
      "val accuracy= 73.000\n",
      "train accuracy= 98.750\n",
      "\n",
      "iteration= 580, train loss(20iter)= 0.104, lr= 0.0000687, time(20iter)= 17.36\n",
      "val accuracy= 72.800\n",
      "train accuracy= 98.286\n",
      "Updating LR to 0.0000550\n",
      "\n",
      "iteration= 600, train loss(20iter)= 0.106, lr= 0.0000550, time(20iter)= 17.26\n",
      "val accuracy= 72.800\n",
      "train accuracy= 98.393\n",
      "Updating LR to 0.0000440\n",
      "\n",
      "iteration= 620, train loss(20iter)= 0.109, lr= 0.0000440, time(20iter)= 17.27\n",
      "val accuracy= 73.000\n",
      "train accuracy= 98.143\n",
      "Early Stopping at 620. Highest Val: 74.400 \n",
      "Avg Val Accuracy 74.4\n",
      "Setting Layers=4, L2=0.00500, LR=0.001000\n",
      "\n",
      "iteration= 20, train loss(20iter)= 1.929, lr= 0.0010000, time(20iter)= 18.16\n",
      "val accuracy= 5.600\n",
      "train accuracy= 22.179\n",
      "Updating LR to 0.0008000\n",
      "\n",
      "iteration= 40, train loss(20iter)= 1.623, lr= 0.0008000, time(20iter)= 17.69\n",
      "val accuracy= 13.000\n",
      "train accuracy= 45.429\n",
      "Updating LR to 0.0006400\n",
      "\n",
      "iteration= 60, train loss(20iter)= 1.202, lr= 0.0006400, time(20iter)= 17.74\n",
      "val accuracy= 11.600\n",
      "train accuracy= 67.036\n",
      "\n",
      "iteration= 80, train loss(20iter)= 0.831, lr= 0.0006400, time(20iter)= 17.71\n",
      "val accuracy= 48.800\n",
      "train accuracy= 79.786\n",
      "\n",
      "iteration= 100, train loss(20iter)= 0.604, lr= 0.0006400, time(20iter)= 17.78\n",
      "val accuracy= 71.600\n",
      "train accuracy= 86.179\n",
      "\n",
      "iteration= 120, train loss(20iter)= 0.474, lr= 0.0006400, time(20iter)= 17.85\n",
      "val accuracy= 76.200\n",
      "train accuracy= 89.250\n",
      "\n",
      "iteration= 140, train loss(20iter)= 0.375, lr= 0.0006400, time(20iter)= 17.76\n",
      "val accuracy= 75.400\n",
      "train accuracy= 92.214\n",
      "Updating LR to 0.0005120\n",
      "\n",
      "iteration= 160, train loss(20iter)= 0.340, lr= 0.0005120, time(20iter)= 17.77\n",
      "val accuracy= 74.800\n",
      "train accuracy= 91.821\n",
      "\n",
      "iteration= 180, train loss(20iter)= 0.289, lr= 0.0005120, time(20iter)= 17.64\n",
      "val accuracy= 73.400\n",
      "train accuracy= 94.179\n",
      "Updating LR to 0.0004096\n",
      "\n",
      "iteration= 200, train loss(20iter)= 0.268, lr= 0.0004096, time(20iter)= 17.75\n",
      "val accuracy= 75.000\n",
      "train accuracy= 93.964\n",
      "Updating LR to 0.0003277\n",
      "\n",
      "iteration= 220, train loss(20iter)= 0.242, lr= 0.0003277, time(20iter)= 17.68\n",
      "val accuracy= 74.800\n",
      "train accuracy= 94.857\n",
      "Updating LR to 0.0002621\n",
      "\n",
      "iteration= 240, train loss(20iter)= 0.237, lr= 0.0002621, time(20iter)= 17.69\n",
      "val accuracy= 73.200\n",
      "train accuracy= 95.464\n",
      "Updating LR to 0.0002097\n",
      "\n",
      "iteration= 260, train loss(20iter)= 0.215, lr= 0.0002097, time(20iter)= 17.81\n",
      "val accuracy= 74.000\n",
      "train accuracy= 95.679\n",
      "\n",
      "iteration= 280, train loss(20iter)= 0.210, lr= 0.0002097, time(20iter)= 17.81\n",
      "val accuracy= 73.800\n",
      "train accuracy= 96.000\n",
      "Updating LR to 0.0001678\n",
      "\n",
      "iteration= 300, train loss(20iter)= 0.199, lr= 0.0001678, time(20iter)= 17.93\n",
      "val accuracy= 72.400\n",
      "train accuracy= 96.000\n",
      "Updating LR to 0.0001342\n",
      "\n",
      "iteration= 320, train loss(20iter)= 0.181, lr= 0.0001342, time(20iter)= 18.08\n",
      "val accuracy= 73.200\n",
      "train accuracy= 96.357\n",
      "\n",
      "iteration= 340, train loss(20iter)= 0.209, lr= 0.0001342, time(20iter)= 18.39\n",
      "val accuracy= 74.200\n",
      "train accuracy= 95.571\n",
      "\n",
      "iteration= 360, train loss(20iter)= 0.198, lr= 0.0001342, time(20iter)= 18.41\n",
      "val accuracy= 75.000\n",
      "train accuracy= 95.464\n",
      "\n",
      "iteration= 380, train loss(20iter)= 0.180, lr= 0.0001342, time(20iter)= 18.41\n",
      "val accuracy= 74.400\n",
      "train accuracy= 96.214\n",
      "Updating LR to 0.0001074\n",
      "\n",
      "iteration= 400, train loss(20iter)= 0.185, lr= 0.0001074, time(20iter)= 18.41\n",
      "val accuracy= 73.800\n",
      "train accuracy= 96.286\n",
      "Updating LR to 0.0000859\n",
      "\n",
      "iteration= 420, train loss(20iter)= 0.174, lr= 0.0000859, time(20iter)= 18.27\n",
      "val accuracy= 74.000\n",
      "train accuracy= 96.857\n",
      "\n",
      "iteration= 440, train loss(20iter)= 0.191, lr= 0.0000859, time(20iter)= 17.59\n",
      "val accuracy= 73.600\n",
      "train accuracy= 96.071\n",
      "Updating LR to 0.0000687\n",
      "\n",
      "iteration= 460, train loss(20iter)= 0.173, lr= 0.0000687, time(20iter)= 17.69\n",
      "val accuracy= 74.600\n",
      "train accuracy= 97.000\n",
      "Updating LR to 0.0000550\n",
      "\n",
      "iteration= 480, train loss(20iter)= 0.181, lr= 0.0000550, time(20iter)= 17.67\n",
      "val accuracy= 74.600\n",
      "train accuracy= 96.250\n",
      "Updating LR to 0.0000440\n",
      "\n",
      "iteration= 500, train loss(20iter)= 0.163, lr= 0.0000440, time(20iter)= 17.65\n",
      "val accuracy= 74.600\n",
      "train accuracy= 96.393\n",
      "Early Stopping at 500. Highest Val: 76.200 \n",
      "Avg Val Accuracy 76.2\n",
      "Setting Layers=6, L2=0.00500, LR=0.001000\n",
      "\n",
      "iteration= 20, train loss(20iter)= 2.009, lr= 0.0010000, time(20iter)= 18.61\n",
      "val accuracy= 14.600\n",
      "train accuracy= 19.107\n",
      "Updating LR to 0.0008000\n",
      "\n",
      "iteration= 40, train loss(20iter)= 1.797, lr= 0.0008000, time(20iter)= 18.01\n",
      "val accuracy= 12.200\n",
      "train accuracy= 29.071\n",
      "Updating LR to 0.0006400\n",
      "\n",
      "iteration= 60, train loss(20iter)= 1.584, lr= 0.0006400, time(20iter)= 18.11\n",
      "val accuracy= 13.200\n",
      "train accuracy= 42.393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration= 80, train loss(20iter)= 1.350, lr= 0.0006400, time(20iter)= 18.10\n",
      "val accuracy= 45.800\n",
      "train accuracy= 54.143\n",
      "\n",
      "iteration= 100, train loss(20iter)= 1.138, lr= 0.0006400, time(20iter)= 18.11\n",
      "val accuracy= 70.200\n",
      "train accuracy= 64.429\n",
      "\n",
      "iteration= 120, train loss(20iter)= 0.922, lr= 0.0006400, time(20iter)= 19.01\n",
      "val accuracy= 77.200\n",
      "train accuracy= 73.750\n",
      "\n",
      "iteration= 140, train loss(20iter)= 0.711, lr= 0.0006400, time(20iter)= 18.24\n",
      "val accuracy= 76.800\n",
      "train accuracy= 81.000\n",
      "\n",
      "iteration= 160, train loss(20iter)= 0.596, lr= 0.0006400, time(20iter)= 18.39\n",
      "val accuracy= 75.600\n",
      "train accuracy= 84.393\n",
      "\n",
      "iteration= 180, train loss(20iter)= 0.494, lr= 0.0006400, time(20iter)= 18.07\n",
      "val accuracy= 74.800\n",
      "train accuracy= 87.143\n",
      "Updating LR to 0.0005120\n",
      "\n",
      "iteration= 200, train loss(20iter)= 0.393, lr= 0.0005120, time(20iter)= 18.06\n",
      "val accuracy= 75.800\n",
      "train accuracy= 90.357\n",
      "Updating LR to 0.0004096\n",
      "\n",
      "iteration= 220, train loss(20iter)= 0.353, lr= 0.0004096, time(20iter)= 18.58\n",
      "val accuracy= 74.400\n",
      "train accuracy= 91.321\n",
      "Updating LR to 0.0003277\n",
      "\n",
      "iteration= 240, train loss(20iter)= 0.329, lr= 0.0003277, time(20iter)= 18.34\n",
      "val accuracy= 74.200\n",
      "train accuracy= 92.143\n",
      "\n",
      "iteration= 260, train loss(20iter)= 0.297, lr= 0.0003277, time(20iter)= 18.28\n",
      "val accuracy= 76.600\n",
      "train accuracy= 92.679\n",
      "Updating LR to 0.0002621\n",
      "\n",
      "iteration= 280, train loss(20iter)= 0.290, lr= 0.0002621, time(20iter)= 18.15\n",
      "val accuracy= 73.400\n",
      "train accuracy= 92.786\n",
      "Updating LR to 0.0002097\n",
      "\n",
      "iteration= 300, train loss(20iter)= 0.306, lr= 0.0002097, time(20iter)= 18.70\n",
      "val accuracy= 73.200\n",
      "train accuracy= 92.571\n",
      "\n",
      "iteration= 320, train loss(20iter)= 0.246, lr= 0.0002097, time(20iter)= 18.04\n",
      "val accuracy= 74.000\n",
      "train accuracy= 94.107\n",
      "Updating LR to 0.0001678\n",
      "\n",
      "iteration= 340, train loss(20iter)= 0.287, lr= 0.0001678, time(20iter)= 18.19\n",
      "val accuracy= 74.000\n",
      "train accuracy= 93.071\n",
      "\n",
      "iteration= 360, train loss(20iter)= 0.271, lr= 0.0001678, time(20iter)= 18.05\n",
      "val accuracy= 73.000\n",
      "train accuracy= 93.250\n",
      "Updating LR to 0.0001342\n",
      "\n",
      "iteration= 380, train loss(20iter)= 0.247, lr= 0.0001342, time(20iter)= 18.17\n",
      "val accuracy= 73.000\n",
      "train accuracy= 94.179\n",
      "Updating LR to 0.0001074\n",
      "\n",
      "iteration= 400, train loss(20iter)= 0.234, lr= 0.0001074, time(20iter)= 18.37\n",
      "val accuracy= 72.600\n",
      "train accuracy= 94.607\n",
      "Updating LR to 0.0000859\n",
      "\n",
      "iteration= 420, train loss(20iter)= 0.251, lr= 0.0000859, time(20iter)= 18.03\n",
      "val accuracy= 71.600\n",
      "train accuracy= 93.929\n",
      "\n",
      "iteration= 440, train loss(20iter)= 0.256, lr= 0.0000859, time(20iter)= 18.04\n",
      "val accuracy= 72.400\n",
      "train accuracy= 94.250\n",
      "\n",
      "iteration= 460, train loss(20iter)= 0.246, lr= 0.0000859, time(20iter)= 18.00\n",
      "val accuracy= 73.400\n",
      "train accuracy= 94.250\n",
      "\n",
      "iteration= 480, train loss(20iter)= 0.250, lr= 0.0000859, time(20iter)= 18.09\n",
      "val accuracy= 73.600\n",
      "train accuracy= 93.893\n",
      "\n",
      "iteration= 500, train loss(20iter)= 0.244, lr= 0.0000859, time(20iter)= 18.28\n",
      "val accuracy= 74.000\n",
      "train accuracy= 93.964\n",
      "Updating LR to 0.0000687\n",
      "\n",
      "iteration= 520, train loss(20iter)= 0.261, lr= 0.0000687, time(20iter)= 18.18\n",
      "val accuracy= 74.000\n",
      "train accuracy= 93.964\n",
      "Updating LR to 0.0000550\n",
      "\n",
      "iteration= 540, train loss(20iter)= 0.229, lr= 0.0000550, time(20iter)= 18.18\n",
      "val accuracy= 73.400\n",
      "train accuracy= 94.821\n",
      "\n",
      "iteration= 560, train loss(20iter)= 0.226, lr= 0.0000550, time(20iter)= 18.14\n",
      "val accuracy= 74.000\n",
      "train accuracy= 94.714\n",
      "Updating LR to 0.0000440\n",
      "\n",
      "iteration= 580, train loss(20iter)= 0.212, lr= 0.0000440, time(20iter)= 18.06\n",
      "val accuracy= 73.200\n",
      "train accuracy= 94.964\n",
      "Early Stopping at 580. Highest Val: 77.200 \n",
      "Avg Val Accuracy 77.2\n",
      "Setting Layers=8, L2=0.00500, LR=0.001000\n",
      "\n",
      "iteration= 20, train loss(20iter)= 2.041, lr= 0.0010000, time(20iter)= 19.10\n",
      "val accuracy= 16.200\n",
      "train accuracy= 15.393\n",
      "\n",
      "iteration= 40, train loss(20iter)= 2.007, lr= 0.0010000, time(20iter)= 18.47\n",
      "val accuracy= 15.200\n",
      "train accuracy= 15.964\n",
      "\n",
      "iteration= 60, train loss(20iter)= 1.915, lr= 0.0010000, time(20iter)= 18.44\n",
      "val accuracy= 7.200\n",
      "train accuracy= 22.607\n",
      "\n",
      "iteration= 80, train loss(20iter)= 1.819, lr= 0.0010000, time(20iter)= 18.52\n",
      "val accuracy= 19.600\n",
      "train accuracy= 28.071\n",
      "\n",
      "iteration= 100, train loss(20iter)= 1.626, lr= 0.0010000, time(20iter)= 18.71\n",
      "val accuracy= 40.000\n",
      "train accuracy= 36.179\n",
      "\n",
      "iteration= 120, train loss(20iter)= 1.393, lr= 0.0010000, time(20iter)= 21.73\n",
      "val accuracy= 48.000\n",
      "train accuracy= 44.893\n",
      "\n",
      "iteration= 140, train loss(20iter)= 1.180, lr= 0.0010000, time(20iter)= 21.72\n",
      "val accuracy= 70.000\n",
      "train accuracy= 57.214\n",
      "\n",
      "iteration= 160, train loss(20iter)= 0.987, lr= 0.0010000, time(20iter)= 19.88\n",
      "val accuracy= 74.000\n",
      "train accuracy= 68.071\n",
      "\n",
      "iteration= 180, train loss(20iter)= 0.772, lr= 0.0010000, time(20iter)= 19.57\n",
      "val accuracy= 76.200\n",
      "train accuracy= 75.964\n",
      "\n",
      "iteration= 200, train loss(20iter)= 0.621, lr= 0.0010000, time(20iter)= 18.50\n",
      "val accuracy= 77.400\n",
      "train accuracy= 80.393\n",
      "Updating LR to 0.0008000\n",
      "\n",
      "iteration= 220, train loss(20iter)= 0.506, lr= 0.0008000, time(20iter)= 18.40\n",
      "val accuracy= 74.400\n",
      "train accuracy= 86.286\n",
      "Updating LR to 0.0006400\n",
      "\n",
      "iteration= 240, train loss(20iter)= 0.449, lr= 0.0006400, time(20iter)= 18.74\n",
      "val accuracy= 75.000\n",
      "train accuracy= 87.571\n",
      "Updating LR to 0.0005120\n",
      "\n",
      "iteration= 260, train loss(20iter)= 0.420, lr= 0.0005120, time(20iter)= 18.38\n",
      "val accuracy= 75.400\n",
      "train accuracy= 88.357\n",
      "\n",
      "iteration= 280, train loss(20iter)= 0.376, lr= 0.0005120, time(20iter)= 18.59\n",
      "val accuracy= 77.200\n",
      "train accuracy= 90.357\n",
      "Updating LR to 0.0004096\n",
      "\n",
      "iteration= 300, train loss(20iter)= 0.332, lr= 0.0004096, time(20iter)= 18.83\n",
      "val accuracy= 76.400\n",
      "train accuracy= 91.143\n",
      "Updating LR to 0.0003277\n",
      "\n",
      "iteration= 320, train loss(20iter)= 0.324, lr= 0.0003277, time(20iter)= 18.77\n",
      "val accuracy= 76.200\n",
      "train accuracy= 91.393\n",
      "Updating LR to 0.0002621\n",
      "\n",
      "iteration= 340, train loss(20iter)= 0.329, lr= 0.0002621, time(20iter)= 18.72\n",
      "val accuracy= 75.000\n",
      "train accuracy= 90.500\n",
      "Updating LR to 0.0002097\n",
      "\n",
      "iteration= 360, train loss(20iter)= 0.320, lr= 0.0002097, time(20iter)= 18.67\n",
      "val accuracy= 76.200\n",
      "train accuracy= 90.714\n",
      "Updating LR to 0.0001678\n",
      "\n",
      "iteration= 380, train loss(20iter)= 0.333, lr= 0.0001678, time(20iter)= 18.37\n",
      "val accuracy= 75.800\n",
      "train accuracy= 92.036\n",
      "\n",
      "iteration= 400, train loss(20iter)= 0.324, lr= 0.0001678, time(20iter)= 18.51\n",
      "val accuracy= 76.600\n",
      "train accuracy= 91.429\n",
      "\n",
      "iteration= 420, train loss(20iter)= 0.292, lr= 0.0001678, time(20iter)= 18.74\n",
      "val accuracy= 76.800\n",
      "train accuracy= 92.571\n",
      "Updating LR to 0.0001342\n",
      "\n",
      "iteration= 440, train loss(20iter)= 0.299, lr= 0.0001342, time(20iter)= 18.79\n",
      "val accuracy= 75.600\n",
      "train accuracy= 92.357\n",
      "Updating LR to 0.0001074\n",
      "\n",
      "iteration= 460, train loss(20iter)= 0.286, lr= 0.0001074, time(20iter)= 18.81\n",
      "val accuracy= 75.600\n",
      "train accuracy= 92.286\n",
      "\n",
      "iteration= 480, train loss(20iter)= 0.301, lr= 0.0001074, time(20iter)= 18.76\n",
      "val accuracy= 76.200\n",
      "train accuracy= 93.036\n",
      "\n",
      "iteration= 500, train loss(20iter)= 0.280, lr= 0.0001074, time(20iter)= 18.64\n",
      "val accuracy= 76.200\n",
      "train accuracy= 92.964\n",
      "Updating LR to 0.0000859\n",
      "\n",
      "iteration= 520, train loss(20iter)= 0.293, lr= 0.0000859, time(20iter)= 18.40\n",
      "val accuracy= 76.600\n",
      "train accuracy= 93.000\n",
      "Updating LR to 0.0000687\n",
      "\n",
      "iteration= 540, train loss(20iter)= 0.265, lr= 0.0000687, time(20iter)= 18.65\n",
      "val accuracy= 75.600\n",
      "train accuracy= 93.821\n",
      "\n",
      "iteration= 560, train loss(20iter)= 0.287, lr= 0.0000687, time(20iter)= 18.71\n",
      "val accuracy= 76.000\n",
      "train accuracy= 93.893\n",
      "Updating LR to 0.0000550\n",
      "\n",
      "iteration= 580, train loss(20iter)= 0.260, lr= 0.0000550, time(20iter)= 18.81\n",
      "val accuracy= 76.600\n",
      "train accuracy= 93.429\n",
      "Updating LR to 0.0000440\n",
      "\n",
      "iteration= 600, train loss(20iter)= 0.270, lr= 0.0000440, time(20iter)= 18.78\n",
      "val accuracy= 75.400\n",
      "train accuracy= 93.429\n",
      "Early Stopping at 600. Highest Val: 77.400 \n",
      "Avg Val Accuracy 77.4\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "l2 = 0.005\n",
    "layers = [2,4,6,8]\n",
    "iters = 1\n",
    "batch_iters = 20\n",
    "early_stopping = 5e-5\n",
    "    \n",
    "for i in range(len(layers)):\n",
    "    L = layers[i]\n",
    "    net_parameters['L'] = L\n",
    "    net_parameters['Dropout_fc'] = 0.5\n",
    "    net_parameters['Dropout_edge'] = 0.5\n",
    "    net_parameters['Dropout_in'] = 0.0001\n",
    "    print(\"Setting Layers=%d, L2=%.5f, LR=%f\" % (L, l2, lr))\n",
    "\n",
    "    val_total = 0.\n",
    "    for iteration in range(iters):\n",
    "        net = Graph_OurConvNet(net_parameters, 1)\n",
    "        net.cuda()\n",
    "        val_total += train(net, lr, l2, batch_iters, early_stopping, verbose=True)\n",
    "    print(\"Avg Val Accuracy\", val_total)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEXCAYAAACDChKsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a3b810198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = [2,4,6,8,10, 12, 14]\n",
    "y = [74.4, 76.2, 77.2, 77.4, 73.4, 65, 59.8]\n",
    "x = np.array(x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=14)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=17)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=17)    # fontsize of the x and y labels\n",
    "plt.rc('legend', fontsize=14)    # legend fontsize\n",
    "plt.rc('xtick', labelsize=15)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=15)    # fontsize of the tick labels\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.xlabel('Layers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout \n",
    "FC = [67.8, 67.4, 64.8, 68.2, 68.4, 69.2, 71.8, 70.4]\n",
    "Edge = [63.8, 67.8, 71.4, 70.4, 71.8, 72.2, 73.8, 73.8]\n",
    "EFC = [67, 67.6, 71.2, 72.4, 73.6, 77.8, 77.6, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEXCAYAAACDChKsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbaf48cfe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "plt.plot(x, FC, label='Conv')\n",
    "plt.plot(x, Edge, label='Edge')\n",
    "plt.plot(x, EFC, label='Conv+Edge')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
