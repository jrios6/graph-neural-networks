{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import LogReg\n",
    "from utils import process\n",
    "\n",
    "dataset = 'cora'\n",
    "\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 2000\n",
    "patience = 100\n",
    "lr = 0.001\n",
    "l2_coef = 0.0\n",
    "drop_prob = 0.0\n",
    "hid_units = 50\n",
    "sparse = False\n",
    "nonlinearity = 'prelu' # special name to separate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = process.load_data(dataset)\n",
    "features, _ = process.preprocess_features(features)\n",
    "\n",
    "nb_nodes = features.shape[0]\n",
    "ft_size = features.shape[1]\n",
    "nb_classes = labels.shape[1]\n",
    "\n",
    "# adj = process.normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "# if sparse:\n",
    "#     sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
    "# else:\n",
    "#     adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "\n",
    "features = torch.FloatTensor(features[np.newaxis])\n",
    "# if not sparse:\n",
    "#     adj = torch.FloatTensor(adj[np.newaxis])\n",
    "labels = torch.FloatTensor(labels[np.newaxis])\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = adj.toarray().astype(float)\n",
    "adj += np.eye(adj.shape[0])\n",
    "\n",
    "new_edges = []\n",
    "for v1 in idx_train:\n",
    "    for v2 in idx_train:\n",
    "        if v1 != v2 and adj[v1, v2] != 1: # and labels[v1] == labels[v2]:\n",
    "            new_edges.append((v1,v2))\n",
    "new_edges = np.array(new_edges)\n",
    "\n",
    "\n",
    "def dropin(new_edges, rate):\n",
    "    np.random.shuffle(new_edges)\n",
    "    v = new_edges.shape[0]\n",
    "    E_start = np.zeros((v, 2708))\n",
    "    E_end = np.zeros((v, 2708))\n",
    "    for i in range(0, int(v*rate), 2):\n",
    "        v1, v2 = new_edges[i]\n",
    "        E_start[i,v1] = E_end[i,v2] = E_start[i+1,v1] = E_end[i+1,v2] = 1\n",
    "    E_start = Variable(torch.from_numpy(E_start[:i+2,:]).float())\n",
    "    E_end = Variable(torch.from_numpy(E_end[:i+2,:]).float())\n",
    "    return E_start.cuda(), E_end.cuda()\n",
    "\n",
    "cora_Estart = np.zeros((20000, 2708))\n",
    "cora_Eend = np.zeros((20000, 2708))\n",
    "cora_Eidentity = [] # idx of identity edges\n",
    "\n",
    "# converting adjacency matrix to edge-to-start, edge-to-end vertex matrix\n",
    "count = 0\n",
    "for i in range(adj.shape[0]):\n",
    "    for j in range(adj.shape[1]):\n",
    "        if adj[i,j] == 1:\n",
    "            cora_Estart[count,i] = 1\n",
    "            cora_Eend[count,j] = 1\n",
    "            if i == j:\n",
    "                cora_Eidentity.append(count)\n",
    "            count += 1\n",
    "cora_Estart = cora_Estart[:count]\n",
    "cora_Eend = cora_Eend[:count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cora_dataset():\n",
    "    E_start = Variable(torch.from_numpy(cora_Estart).float())\n",
    "    E_end = Variable(torch.from_numpy(cora_Eend).float())\n",
    "    \n",
    "    return E_start.cuda(), E_end.cuda(), cora_Eidentity, new_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurConvNetcell(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dropout_fc=0, dropout_edge=0):\n",
    "        super(OurConvNetcell, self).__init__()\n",
    "    \n",
    "        # conv1\n",
    "        self.Ui1 = nn.Linear(dim_in, dim_out, bias=False) \n",
    "        self.Vi1 = nn.Linear(dim_in, dim_out, bias=False) \n",
    "        self.Vj1 = nn.Linear(dim_in, dim_out, bias=False)  \n",
    "        self.bu1 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        self.bv1 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        \n",
    "        self.dropout_fc = dropout_fc\n",
    "        self.dropout_edge = dropout_edge\n",
    "        \n",
    "        # conv2\n",
    "        self.Ui2 = nn.Linear(dim_out, dim_out, bias=False) \n",
    "        self.Vi2 = nn.Linear(dim_out, dim_out, bias=False) \n",
    "        self.Vj2 = nn.Linear(dim_out, dim_out, bias=False)  \n",
    "        self.bu2 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        self.bv2 = torch.nn.Parameter( torch.FloatTensor(dim_out), requires_grad=True )\n",
    "        \n",
    "        # bn1, bn2\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim_out)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim_out)\n",
    "        \n",
    "        # resnet\n",
    "        self.R = nn.Linear(dim_in, dim_out, bias=False) \n",
    "        \n",
    "        # init\n",
    "        self.init_weights_OurConvNetcell(dim_in, dim_out, 1)\n",
    "        \n",
    "         \n",
    "    def init_weights_OurConvNetcell(self, dim_in, dim_out, gain):   \n",
    "        # conv1\n",
    "        scale = gain* np.sqrt( 2.0/ dim_in )\n",
    "        self.Ui1.weight.data.uniform_(-scale, scale) \n",
    "        self.Vi1.weight.data.uniform_(-scale, scale) \n",
    "        self.Vj1.weight.data.uniform_(-scale, scale) \n",
    "        self.bu1.data.fill_(0)\n",
    "        self.bv1.data.fill_(0)\n",
    "        \n",
    "        # conv2\n",
    "        scale = gain* np.sqrt( 2.0/ dim_out )\n",
    "        self.Ui2.weight.data.uniform_(-scale, scale) \n",
    "        self.Vi2.weight.data.uniform_(-scale, scale) \n",
    "        self.Vj2.weight.data.uniform_(-scale, scale) \n",
    "        self.bu2.data.fill_(0)\n",
    "        self.bv2.data.fill_(0)\n",
    "        \n",
    "        # RN\n",
    "        scale = gain* np.sqrt( 2.0/ dim_in )\n",
    "        self.R.weight.data.uniform_(-scale, scale)  \n",
    "        \n",
    "        \n",
    "    def forward(self, x, E_start, E_end):\n",
    "        x = F.dropout(x, self.dropout_fc, training=self.training)\n",
    "        xin = x\n",
    "        \n",
    "        # edge norm\n",
    "        norm = torch.sum(E_end.t(), 1).reshape(-1,1)\n",
    "#         norm = torch.max(norm, torch.ones(norm.shape).cuda())\n",
    "\n",
    "        # conv1\n",
    "        Uix = self.Ui1(x)  #  V x H_out\n",
    "        Vix = self.Vi1(x)  #  V x H_out\n",
    "        Vjx = self.Vj1(x)  #  V x H_out\n",
    "        x1 = torch.mm(E_end,Vix) + torch.mm(E_start,Vjx) + self.bv1  # E x H_out\n",
    "        x1 = torch.sigmoid(x1)\n",
    "\n",
    "        x2 = torch.mm(E_start, Uix)  #  E x H_out\n",
    "        x = torch.mm(E_end.t(), x1*x2) + self.bu1 #  V x H_out\n",
    "        \n",
    "        x = torch.div(x, norm)# norm\n",
    "        x = self.bn1(x) # bn1\n",
    "        x = torch.nn.LeakyReLU(0.1)(x) # relu1\n",
    "        \n",
    "        # conv2\n",
    "        Uix = self.Ui2(x)  #  V x H_out\n",
    "        Vix = self.Vi2(x)  #  V x H_out\n",
    "        Vjx = self.Vj2(x)  #  V x H_out\n",
    "        x1 = torch.mm(E_end,Vix) + torch.mm(E_start,Vjx) + self.bv2  # E x H_out\n",
    "        x1 = torch.sigmoid(x1)\n",
    "        \n",
    "        x2 = torch.mm(E_start, Uix)  #  V x H_out        \n",
    "        x = torch.mm(E_end.t(), x1*x2) + self.bu2 #  V x H_out\n",
    "        \n",
    "        x = torch.div(x, norm) # normalization\n",
    "        \n",
    "        x = self.bn2(x) # bn2\n",
    "        x = x + self.R(xin) # addition\n",
    "        x = torch.nn.LeakyReLU(0.1)(x) # relu2\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class Graph_OurConvNet(nn.Module):\n",
    "    def __init__(self, net_parameters, cora=False):\n",
    "        super(Graph_OurConvNet, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        Voc = net_parameters['Voc']\n",
    "        D = net_parameters['D']\n",
    "        nb_clusters_target = net_parameters['nb_clusters_target']\n",
    "        H = net_parameters['H']\n",
    "        L = net_parameters['L']\n",
    "        self.cora = cora\n",
    "        self.dropout_fc = net_parameters['Dropout_fc']\n",
    "        self.dropout_edge = net_parameters['Dropout_edge']\n",
    "        self.drop_in = net_parameters['Dropout_in']\n",
    "        \n",
    "        # vector of hidden dimensions\n",
    "        net_layers = []\n",
    "        for layer in range(L):\n",
    "            net_layers.append(H)\n",
    "        \n",
    "        # CL cells\n",
    "        # NOTE: Each graph convnet cell uses *TWO* convolutional operations\n",
    "        net_layers_extended = [net_parameters['features']] + net_layers \n",
    "        \n",
    "        L = len(net_layers)\n",
    "        list_of_gnn_cells = [] # list of NN cells\n",
    "        for layer in range(L//2):\n",
    "            Hin, Hout = net_layers_extended[2*layer], net_layers_extended[2*layer+2]\n",
    "            list_of_gnn_cells.append(OurConvNetcell(Hin,Hout, self.dropout_fc, self.dropout_edge))\n",
    "        \n",
    "        # register the cells for pytorch\n",
    "        self.gnn_cells = nn.ModuleList(list_of_gnn_cells)\n",
    "            \n",
    "        # fc\n",
    "        Hfinal = net_layers_extended[-1]\n",
    "        self.fc = nn.Linear(Hfinal,nb_clusters_target) \n",
    "        \n",
    "        # init\n",
    "        self.init_weights_Graph_OurConvNet(Voc,D,Hfinal,nb_clusters_target,1)\n",
    "        \n",
    "        # print\n",
    "        print('\\nnb of hidden layers=',L)\n",
    "        print('dim of layers (w/ embed dim)=',net_layers_extended)      \n",
    "        print('\\n')\n",
    "        \n",
    "        # class variables\n",
    "        self.D = D\n",
    "        self.L = L\n",
    "        self.net_layers_extended = net_layers_extended      \n",
    "        \n",
    "        \n",
    "    def init_weights_Graph_OurConvNet(self, Fin_enc, Fout_enc, Fin_fc, Fout_fc, gain):\n",
    "        scale = gain* np.sqrt(2.0/ (Fin_fc+Fout_fc))\n",
    "        self.fc.weight.data.uniform_(-scale, scale)  \n",
    "        self.fc.bias.data.fill_(0)  \n",
    "        \n",
    "    def forward(self, x, E_start, E_end, E_identity, E_dropin):\n",
    "        if self.training:\n",
    "            # Edge Start+End Dropout for all layers\n",
    "            num_edges = E_start.shape[0]\n",
    "            dropout_idx = np.array([i for i in range(num_edges) if i not in E_identity])\n",
    "            np.random.shuffle(dropout_idx)\n",
    "            E_start = E_start.clone()\n",
    "            E_start[dropout_idx[:int(num_edges*self.dropout_edge)]] = 0\n",
    "            E_end = E_end.clone()\n",
    "            E_end[dropout_idx[:int(num_edges*self.dropout_edge)]] = 0\n",
    "            \n",
    "            # Dropin\n",
    "            D_start, D_end = dropin(E_dropin, self.drop_in)\n",
    "            E_start = torch.cat((E_start, D_start), 0)\n",
    "            E_end = torch.cat((E_end, D_end), 0)\n",
    "            \n",
    "        # convnet cells  \n",
    "        for layer in range(self.L//2):\n",
    "            gnn_layer = self.gnn_cells[layer]            \n",
    "            x = gnn_layer(x,E_start,E_end) # V x H\n",
    "            \n",
    "#         x = F.dropout(x, self.dropout_fc, training=self.training) #FC Dropout\n",
    "#         x = self.fc(x) # FC\n",
    "        return x\n",
    "         \n",
    "    def loss(self, y, y_target, weight):\n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "        return loss\n",
    "       \n",
    "    def update(self, lr, l2):\n",
    "        update = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "        return update\n",
    "    \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return optimizer\n",
    "    \n",
    "    def nb_param(self):\n",
    "        return self.nb_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_h):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.f_k = nn.Bilinear(n_h, n_h, 8)\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Bilinear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
    "        c_x = torch.unsqueeze(c, 1)\n",
    "        c_x = c_x.expand_as(h_pl)\n",
    "\n",
    "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x))\n",
    "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x))\n",
    "\n",
    "        if s_bias1 is not None:\n",
    "            sc_1 += s_bias1\n",
    "        if s_bias2 is not None:\n",
    "            sc_2 += s_bias2\n",
    "\n",
    "        logits = torch.cat((sc_1, sc_2), 0)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import GCN, AvgReadout\n",
    "\n",
    "class DGI(nn.Module):\n",
    "    def __init__(self, n_in, n_h, activation, net_parameters):\n",
    "        super(DGI, self).__init__()\n",
    "        self.gcn = Graph_OurConvNet(net_parameters, True)\n",
    "        self.read = AvgReadout()\n",
    "        self.drop_prob = net_parameters['Dropout_fc']\n",
    "\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "        self.disc = Discriminator(n_h)\n",
    "\n",
    "    def forward(self, seq1, seq2, E_start, E_end, E_identity, E_dropin, adj, sparse, msk, samp_bias1, samp_bias2):\n",
    "        h_1 = self.gcn(seq1, E_start, E_end, E_identity, E_dropin)\n",
    "\n",
    "        c = self.read(h_1, msk)\n",
    "        c = self.sigm(c)\n",
    "\n",
    "        h_2 = self.gcn(seq2, E_start, E_end, E_identity, E_dropin)\n",
    "        \n",
    "#         h_1 = F.dropout(h_1, self.drop_prob, training=self.training)\n",
    "#         h_2 = F.dropout(h_2, self.drop_prob, training=self.training)\n",
    "        \n",
    "        ret = self.disc(c, h_1, h_2, samp_bias1, samp_bias2)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    # Detach the return variables\n",
    "    def embed(self, seq,  E_start, E_end, E_identity, E_dropin, adj, sparse, msk):\n",
    "        h_1 = self.gcn(seq,  E_start, E_end, E_identity, E_dropin)\n",
    "        c = self.read(h_1, msk)\n",
    "\n",
    "        return h_1.detach(), c.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.reshape(2708, -1)\n",
    "\n",
    "net_parameters = {}\n",
    "net_parameters['D'] = net_parameters['H'] = hid_units\n",
    "net_parameters['features'] = features.shape[1]\n",
    "net_parameters['Voc'] = 7+1 \n",
    "net_parameters['nb_clusters_target'] = 7\n",
    "net_parameters['L'] = 10\n",
    "net_parameters['Dropout_fc'] = 0.0\n",
    "net_parameters['Dropout_edge'] = 0.0\n",
    "net_parameters['Dropout_in'] = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nb of hidden layers= 10\n",
      "dim of layers (w/ embed dim)= [1433, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
      "\n",
      "\n",
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "model = DGI(ft_size, hid_units, nonlinearity, net_parameters)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_coef)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Using CUDA')\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "#     if sparse:\n",
    "#         sp_adj = sp_adj.cuda()\n",
    "#     else:\n",
    "#         adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "    \n",
    "b_xent = nn.BCEWithLogitsLoss()\n",
    "xent = nn.CrossEntropyLoss()\n",
    "nll = nn.NLLLoss()\n",
    "cnt_wait = 0\n",
    "best = 1e9\n",
    "best_t = 0\n",
    "\n",
    "train_lbls = torch.argmax(labels[0, idx_train], dim=1)\n",
    "val_lbls = torch.argmax(labels[0, idx_val], dim=1)\n",
    "test_lbls = torch.argmax(labels[0, idx_test], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.5886, UL: 8.5886, Sup: 0.4334\n",
      "0 Test Acc: 0.1490, Train Acc: 0.1429\n",
      "\n",
      "Loss: 8.4745, UL: 8.4745, Sup: 0.4207\n",
      "1 Test Acc: 0.1490, Train Acc: 0.1429\n",
      "\n",
      "Loss: 8.4133, UL: 8.4133, Sup: 0.4111\n",
      "2 Test Acc: 0.1490, Train Acc: 0.1429\n",
      "\n",
      "Loss: 8.3711, UL: 8.3711, Sup: 0.4129\n",
      "3 Test Acc: 0.1490, Train Acc: 0.1429\n",
      "\n",
      "Loss: 8.3314, UL: 8.3314, Sup: 0.4167\n",
      "4 Test Acc: 0.1490, Train Acc: 0.1429\n",
      "\n",
      "Loss: 8.3081, UL: 8.3081, Sup: 0.4199\n",
      "5 Test Acc: 0.1490, Train Acc: 0.1429\n",
      "\n",
      "Loss: 8.3051, UL: 8.3051, Sup: 0.4230\n",
      "6 Test Acc: 0.1600, Train Acc: 0.1214\n",
      "\n",
      "Loss: 8.2864, UL: 8.2864, Sup: 0.4217\n",
      "7 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 8.2611, UL: 8.2611, Sup: 0.4199\n",
      "8 Test Acc: 0.1650, Train Acc: 0.1286\n",
      "\n",
      "Loss: 8.2387, UL: 8.2387, Sup: 0.4160\n",
      "9 Test Acc: 0.1490, Train Acc: 0.1429\n",
      "\n",
      "Loss: 8.1938, UL: 8.1938, Sup: 0.4144\n",
      "10 Test Acc: 0.1490, Train Acc: 0.1429\n",
      "\n",
      "Loss: 8.1624, UL: 8.1624, Sup: 0.4122\n",
      "11 Test Acc: 0.1490, Train Acc: 0.1429\n",
      "\n",
      "Loss: 8.1264, UL: 8.1264, Sup: 0.4112\n",
      "12 Test Acc: 0.3140, Train Acc: 0.1357\n",
      "\n",
      "Loss: 8.0948, UL: 8.0948, Sup: 0.4098\n",
      "13 Test Acc: 0.3190, Train Acc: 0.1429\n",
      "\n",
      "Loss: 8.0569, UL: 8.0569, Sup: 0.4108\n",
      "14 Test Acc: 0.1300, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.9710, UL: 7.9710, Sup: 0.4080\n",
      "15 Test Acc: 0.1300, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.9548, UL: 7.9548, Sup: 0.4081\n",
      "16 Test Acc: 0.1300, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.8801, UL: 7.8801, Sup: 0.4054\n",
      "17 Test Acc: 0.1300, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.8220, UL: 7.8220, Sup: 0.4032\n",
      "18 Test Acc: 0.0910, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.7865, UL: 7.7865, Sup: 0.4012\n",
      "19 Test Acc: 0.0910, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.6558, UL: 7.6558, Sup: 0.4020\n",
      "20 Test Acc: 0.0910, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.6008, UL: 7.6008, Sup: 0.4018\n",
      "21 Test Acc: 0.0910, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.5210, UL: 7.5210, Sup: 0.4007\n",
      "22 Test Acc: 0.1400, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.4455, UL: 7.4455, Sup: 0.4004\n",
      "23 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.3900, UL: 7.3900, Sup: 0.4024\n",
      "24 Test Acc: 0.1260, Train Acc: 0.1143\n",
      "\n",
      "Loss: 7.3232, UL: 7.3232, Sup: 0.4007\n",
      "25 Test Acc: 0.0840, Train Acc: 0.0929\n",
      "\n",
      "Loss: 7.2150, UL: 7.2150, Sup: 0.4002\n",
      "26 Test Acc: 0.1430, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.1533, UL: 7.1533, Sup: 0.3983\n",
      "27 Test Acc: 0.1430, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.1402, UL: 7.1402, Sup: 0.3988\n",
      "28 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.0590, UL: 7.0590, Sup: 0.3937\n",
      "29 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 7.0146, UL: 7.0146, Sup: 0.3917\n",
      "30 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.9942, UL: 6.9942, Sup: 0.3901\n",
      "31 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.9491, UL: 6.9491, Sup: 0.3904\n",
      "32 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.9388, UL: 6.9388, Sup: 0.3895\n",
      "33 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.9151, UL: 6.9151, Sup: 0.3906\n",
      "34 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8897, UL: 6.8897, Sup: 0.3911\n",
      "35 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8749, UL: 6.8749, Sup: 0.3901\n",
      "36 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8694, UL: 6.8694, Sup: 0.3901\n",
      "37 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8571, UL: 6.8571, Sup: 0.3905\n",
      "38 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8569, UL: 6.8569, Sup: 0.3896\n",
      "39 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8387, UL: 6.8387, Sup: 0.3890\n",
      "40 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8393, UL: 6.8393, Sup: 0.3887\n",
      "41 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8342, UL: 6.8342, Sup: 0.3889\n",
      "42 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8285, UL: 6.8285, Sup: 0.3894\n",
      "43 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8281, UL: 6.8281, Sup: 0.3892\n",
      "44 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8242, UL: 6.8242, Sup: 0.3894\n",
      "45 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8230, UL: 6.8230, Sup: 0.3890\n",
      "46 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8216, UL: 6.8216, Sup: 0.3889\n",
      "47 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8219, UL: 6.8219, Sup: 0.3883\n",
      "48 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8192, UL: 6.8192, Sup: 0.3886\n",
      "49 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8183, UL: 6.8183, Sup: 0.3892\n",
      "50 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8245, UL: 6.8245, Sup: 0.3897\n",
      "51 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8172, UL: 6.8172, Sup: 0.3897\n",
      "52 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8181, UL: 6.8181, Sup: 0.3896\n",
      "53 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8164, UL: 6.8164, Sup: 0.3895\n",
      "54 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8173, UL: 6.8173, Sup: 0.3892\n",
      "55 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8189, UL: 6.8189, Sup: 0.3891\n",
      "56 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8153, UL: 6.8153, Sup: 0.3891\n",
      "57 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8158, UL: 6.8158, Sup: 0.3891\n",
      "58 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8143, UL: 6.8143, Sup: 0.3896\n",
      "59 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8152, UL: 6.8152, Sup: 0.3896\n",
      "60 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8148, UL: 6.8148, Sup: 0.3892\n",
      "61 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8145, UL: 6.8145, Sup: 0.3900\n",
      "62 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8142, UL: 6.8142, Sup: 0.3895\n",
      "63 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8138, UL: 6.8138, Sup: 0.3896\n",
      "64 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8138, UL: 6.8138, Sup: 0.3895\n",
      "65 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8181, UL: 6.8181, Sup: 0.3904\n",
      "66 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8141, UL: 6.8141, Sup: 0.3887\n",
      "67 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8135, UL: 6.8135, Sup: 0.3886\n",
      "68 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8134, UL: 6.8134, Sup: 0.3888\n",
      "69 Test Acc: 0.1490, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8133, UL: 6.8133, Sup: 0.3892\n",
      "70 Test Acc: 0.1490, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8134, UL: 6.8134, Sup: 0.3891\n",
      "71 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8135, UL: 6.8135, Sup: 0.3889\n",
      "72 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8134, UL: 6.8134, Sup: 0.3891\n",
      "73 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8132, UL: 6.8132, Sup: 0.3897\n",
      "74 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8134, UL: 6.8134, Sup: 0.3900\n",
      "75 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8127, UL: 6.8127, Sup: 0.3896\n",
      "76 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8136, UL: 6.8136, Sup: 0.3893\n",
      "77 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8131, UL: 6.8131, Sup: 0.3892\n",
      "78 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8133, UL: 6.8133, Sup: 0.3893\n",
      "79 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8129, UL: 6.8129, Sup: 0.3893\n",
      "80 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8127, UL: 6.8127, Sup: 0.3893\n",
      "81 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8128, UL: 6.8128, Sup: 0.3890\n",
      "82 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8125, UL: 6.8125, Sup: 0.3893\n",
      "83 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8223, UL: 6.8223, Sup: 0.3933\n",
      "84 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8133, UL: 6.8133, Sup: 0.3893\n",
      "85 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8129, UL: 6.8129, Sup: 0.3892\n",
      "86 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8129, UL: 6.8129, Sup: 0.3895\n",
      "87 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8127, UL: 6.8127, Sup: 0.3896\n",
      "88 Test Acc: 0.1470, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8165, UL: 6.8165, Sup: 0.3896\n",
      "89 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8130, UL: 6.8130, Sup: 0.3892\n",
      "90 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8128, UL: 6.8128, Sup: 0.3893\n",
      "91 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8131, UL: 6.8131, Sup: 0.3893\n",
      "92 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8133, UL: 6.8133, Sup: 0.3894\n",
      "93 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8130, UL: 6.8130, Sup: 0.3894\n",
      "94 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8137, UL: 6.8137, Sup: 0.3893\n",
      "95 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8128, UL: 6.8128, Sup: 0.3889\n",
      "96 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8132, UL: 6.8132, Sup: 0.3892\n",
      "97 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8127, UL: 6.8127, Sup: 0.3891\n",
      "98 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8124, UL: 6.8124, Sup: 0.3892\n",
      "99 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8124, UL: 6.8124, Sup: 0.3890\n",
      "100 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8128, UL: 6.8128, Sup: 0.3891\n",
      "101 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8124, UL: 6.8124, Sup: 0.3893\n",
      "102 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8122, UL: 6.8122, Sup: 0.3895\n",
      "103 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8123, UL: 6.8123, Sup: 0.3896\n",
      "104 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8120, UL: 6.8120, Sup: 0.3894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8122, UL: 6.8122, Sup: 0.3894\n",
      "106 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8127, UL: 6.8127, Sup: 0.3893\n",
      "107 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8121, UL: 6.8121, Sup: 0.3893\n",
      "108 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8122, UL: 6.8122, Sup: 0.3894\n",
      "109 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8145, UL: 6.8145, Sup: 0.3893\n",
      "110 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8121, UL: 6.8121, Sup: 0.3894\n",
      "111 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8119, UL: 6.8119, Sup: 0.3894\n",
      "112 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8119, UL: 6.8119, Sup: 0.3892\n",
      "113 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8120, UL: 6.8120, Sup: 0.3893\n",
      "114 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8118, UL: 6.8118, Sup: 0.3892\n",
      "115 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8119, UL: 6.8119, Sup: 0.3893\n",
      "116 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8119, UL: 6.8119, Sup: 0.3893\n",
      "117 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8118, UL: 6.8118, Sup: 0.3893\n",
      "118 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8119, UL: 6.8119, Sup: 0.3894\n",
      "119 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8117, UL: 6.8117, Sup: 0.3894\n",
      "120 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8118, UL: 6.8118, Sup: 0.3893\n",
      "121 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8121, UL: 6.8121, Sup: 0.3893\n",
      "122 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8128, UL: 6.8128, Sup: 0.3893\n",
      "123 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8118, UL: 6.8118, Sup: 0.3892\n",
      "124 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8118, UL: 6.8118, Sup: 0.3893\n",
      "125 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8120, UL: 6.8120, Sup: 0.3892\n",
      "126 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8118, UL: 6.8118, Sup: 0.3893\n",
      "127 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8116, UL: 6.8116, Sup: 0.3894\n",
      "128 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8117, UL: 6.8117, Sup: 0.3895\n",
      "129 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8115, UL: 6.8115, Sup: 0.3893\n",
      "130 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8119, UL: 6.8119, Sup: 0.3895\n",
      "131 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8119, UL: 6.8119, Sup: 0.3895\n",
      "132 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8119, UL: 6.8119, Sup: 0.3890\n",
      "133 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8120, UL: 6.8120, Sup: 0.3891\n",
      "134 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8115, UL: 6.8115, Sup: 0.3891\n",
      "135 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8128, UL: 6.8128, Sup: 0.3896\n",
      "136 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8116, UL: 6.8116, Sup: 0.3893\n",
      "137 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8116, UL: 6.8116, Sup: 0.3892\n",
      "138 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8117, UL: 6.8117, Sup: 0.3894\n",
      "139 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8118, UL: 6.8118, Sup: 0.3894\n",
      "140 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3895\n",
      "141 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8120, UL: 6.8120, Sup: 0.3896\n",
      "142 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8116, UL: 6.8116, Sup: 0.3893\n",
      "143 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8115, UL: 6.8115, Sup: 0.3891\n",
      "144 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8115, UL: 6.8115, Sup: 0.3894\n",
      "145 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3893\n",
      "146 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8124, UL: 6.8124, Sup: 0.3893\n",
      "147 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8115, UL: 6.8115, Sup: 0.3891\n",
      "148 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8123, UL: 6.8123, Sup: 0.3892\n",
      "149 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8116, UL: 6.8116, Sup: 0.3893\n",
      "150 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3894\n",
      "151 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8116, UL: 6.8116, Sup: 0.3894\n",
      "152 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8115, UL: 6.8115, Sup: 0.3893\n",
      "153 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3894\n",
      "154 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3894\n",
      "155 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3893\n",
      "156 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8115, UL: 6.8115, Sup: 0.3893\n",
      "157 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3892\n",
      "158 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8115, UL: 6.8115, Sup: 0.3892\n",
      "159 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3893\n",
      "160 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3894\n",
      "161 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8121, UL: 6.8121, Sup: 0.3901\n",
      "162 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3892\n",
      "163 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3893\n",
      "164 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3894\n",
      "165 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8117, UL: 6.8117, Sup: 0.3896\n",
      "166 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3892\n",
      "167 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8115, UL: 6.8115, Sup: 0.3893\n",
      "168 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3894\n",
      "169 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3893\n",
      "170 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8115, UL: 6.8115, Sup: 0.3895\n",
      "171 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3893\n",
      "172 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3893\n",
      "173 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8116, UL: 6.8116, Sup: 0.3896\n",
      "174 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3894\n",
      "175 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3893\n",
      "176 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8116, UL: 6.8116, Sup: 0.3896\n",
      "177 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3892\n",
      "178 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3893\n",
      "179 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8117, UL: 6.8117, Sup: 0.3897\n",
      "180 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3894\n",
      "181 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "182 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3894\n",
      "183 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8115, UL: 6.8115, Sup: 0.3897\n",
      "184 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3893\n",
      "185 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3893\n",
      "186 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3894\n",
      "187 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3893\n",
      "188 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3892\n",
      "189 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3892\n",
      "190 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3893\n",
      "191 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3893\n",
      "192 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3894\n",
      "193 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8116, UL: 6.8116, Sup: 0.3896\n",
      "194 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3893\n",
      "195 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "196 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8121, UL: 6.8121, Sup: 0.3899\n",
      "197 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3894\n",
      "198 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "199 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3894\n",
      "200 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3894\n",
      "201 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3894\n",
      "202 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3894\n",
      "203 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3892\n",
      "204 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3894\n",
      "205 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3894\n",
      "206 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3894\n",
      "207 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3894\n",
      "208 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3894\n",
      "210 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3895\n",
      "211 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3894\n",
      "212 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "213 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8115, UL: 6.8115, Sup: 0.3897\n",
      "214 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3894\n",
      "215 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "216 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "217 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3893\n",
      "218 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3893\n",
      "219 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3894\n",
      "220 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3891\n",
      "221 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "222 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3895\n",
      "223 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3895\n",
      "224 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3892\n",
      "225 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "226 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3894\n",
      "227 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3891\n",
      "228 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "229 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3893\n",
      "230 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3891\n",
      "231 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3894\n",
      "232 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3895\n",
      "233 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3892\n",
      "234 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3892\n",
      "235 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3895\n",
      "236 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3892\n",
      "237 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3890\n",
      "238 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3894\n",
      "239 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3894\n",
      "240 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "241 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "242 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3894\n",
      "243 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3890\n",
      "244 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3892\n",
      "245 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3895\n",
      "246 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "247 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "248 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3895\n",
      "249 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3893\n",
      "250 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3891\n",
      "251 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "252 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3895\n",
      "253 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3892\n",
      "254 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3893\n",
      "255 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "256 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3893\n",
      "257 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3892\n",
      "258 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3894\n",
      "259 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "260 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "261 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "262 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "263 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "264 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "265 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "266 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "267 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "268 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "269 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3894\n",
      "270 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "271 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "272 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "273 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "274 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "275 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3895\n",
      "276 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3892\n",
      "277 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "278 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "279 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "280 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "281 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "282 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "283 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "284 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "285 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "286 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3894\n",
      "287 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3894\n",
      "288 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3894\n",
      "289 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3890\n",
      "290 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "291 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3894\n",
      "292 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "293 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "294 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3894\n",
      "295 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3890\n",
      "296 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "297 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3894\n",
      "298 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3890\n",
      "299 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3894\n",
      "300 Test Acc: 0.1460, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3894\n",
      "301 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3890\n",
      "302 Test Acc: 0.1460, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3893\n",
      "303 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3895\n",
      "304 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3889\n",
      "305 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "306 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3894\n",
      "307 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3891\n",
      "308 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3892\n",
      "309 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3893\n",
      "310 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3891\n",
      "311 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3894\n",
      "312 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3891\n",
      "314 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3895\n",
      "315 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3895\n",
      "316 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3888\n",
      "317 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3894\n",
      "318 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3895\n",
      "319 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3890\n",
      "320 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3894\n",
      "321 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3894\n",
      "322 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3890\n",
      "323 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3894\n",
      "324 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3891\n",
      "325 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "326 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "327 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "328 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "329 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "330 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "331 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "332 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "333 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "334 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "335 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "336 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "337 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "338 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "339 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "340 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "341 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "342 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "343 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "344 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "345 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "346 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "347 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "348 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "349 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3894\n",
      "350 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "351 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "352 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "353 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "354 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "355 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3894\n",
      "356 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3890\n",
      "357 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3894\n",
      "358 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "359 Test Acc: 0.1460, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "360 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3894\n",
      "361 Test Acc: 0.1460, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "362 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3890\n",
      "363 Test Acc: 0.1470, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3896\n",
      "364 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3889\n",
      "365 Test Acc: 0.1460, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3895\n",
      "366 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "367 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3891\n",
      "368 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3895\n",
      "369 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3890\n",
      "370 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "371 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3894\n",
      "372 Test Acc: 0.1460, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "373 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "374 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3894\n",
      "375 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "376 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "377 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "378 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "379 Test Acc: 0.1460, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "380 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "381 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "382 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "383 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "384 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "385 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3895\n",
      "386 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "387 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "388 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "389 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "390 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "391 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "392 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "393 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "394 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "395 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "396 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "397 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "398 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "399 Test Acc: 0.1570, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "400 Test Acc: 0.1470, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3892\n",
      "401 Test Acc: 0.1670, Train Acc: 0.1714\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3894\n",
      "402 Test Acc: 0.1550, Train Acc: 0.1643\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3891\n",
      "403 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8114, UL: 6.8114, Sup: 0.3893\n",
      "404 Test Acc: 0.1630, Train Acc: 0.1786\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3893\n",
      "405 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3891\n",
      "406 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3894\n",
      "407 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "408 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "409 Test Acc: 0.1470, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "410 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "411 Test Acc: 0.1460, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "412 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "413 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "414 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "415 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "416 Test Acc: 0.1460, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "418 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "419 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "420 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "421 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "422 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "423 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "424 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "425 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3893\n",
      "426 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "427 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "428 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "429 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "430 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "431 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "432 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "433 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "434 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "435 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "436 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "437 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "438 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "439 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "440 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "441 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "442 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "443 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "444 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "445 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "446 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "447 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "448 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "449 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "450 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "451 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "452 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "453 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "454 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "455 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "456 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "457 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "458 Test Acc: 0.1450, Train Acc: 0.1571\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "459 Test Acc: 0.1470, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "460 Test Acc: 0.1460, Train Acc: 0.1643\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "461 Test Acc: 0.1490, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3894\n",
      "462 Test Acc: 0.1510, Train Acc: 0.1714\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3890\n",
      "463 Test Acc: 0.1490, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3895\n",
      "464 Test Acc: 0.1740, Train Acc: 0.2000\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3890\n",
      "465 Test Acc: 0.1520, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3894\n",
      "466 Test Acc: 0.1530, Train Acc: 0.1786\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "467 Test Acc: 0.1530, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3894\n",
      "468 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "469 Test Acc: 0.1470, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "470 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "471 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "472 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "473 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "474 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "475 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "476 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "477 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3890\n",
      "478 Test Acc: 0.1460, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3894\n",
      "479 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "480 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "481 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "482 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "483 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "484 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "485 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "486 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "487 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3894\n",
      "488 Test Acc: 0.1440, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "489 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "490 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "491 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "492 Test Acc: 0.1520, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "493 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3894\n",
      "494 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8115, UL: 6.8115, Sup: 0.3891\n",
      "495 Test Acc: 0.1700, Train Acc: 0.1714\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3895\n",
      "496 Test Acc: 0.1640, Train Acc: 0.1929\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3890\n",
      "497 Test Acc: 0.1800, Train Acc: 0.1929\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3894\n",
      "498 Test Acc: 0.1700, Train Acc: 0.2000\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3892\n",
      "499 Test Acc: 0.1750, Train Acc: 0.1857\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3894\n",
      "500 Test Acc: 0.1610, Train Acc: 0.1571\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3890\n",
      "501 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3894\n",
      "502 Test Acc: 0.1690, Train Acc: 0.1714\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "503 Test Acc: 0.1510, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3892\n",
      "504 Test Acc: 0.1980, Train Acc: 0.2214\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3893\n",
      "505 Test Acc: 0.1590, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3893\n",
      "506 Test Acc: 0.1900, Train Acc: 0.1929\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3892\n",
      "507 Test Acc: 0.1620, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3894\n",
      "508 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3891\n",
      "509 Test Acc: 0.1470, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "510 Test Acc: 0.1470, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "511 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "512 Test Acc: 0.1520, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "513 Test Acc: 0.1480, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "514 Test Acc: 0.1530, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "515 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "516 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "517 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "518 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "519 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "520 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521 Test Acc: 0.1500, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3894\n",
      "522 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "523 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "524 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "525 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "526 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "527 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "528 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "529 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "530 Test Acc: 0.1500, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "531 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "532 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "533 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3894\n",
      "534 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "535 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "536 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "537 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "538 Test Acc: 0.1470, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "539 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "540 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "541 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "542 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "543 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "544 Test Acc: 0.1470, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "545 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "546 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "547 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "548 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "549 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "550 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "551 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "552 Test Acc: 0.1460, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "553 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "554 Test Acc: 0.1460, Train Acc: 0.1571\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "555 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "556 Test Acc: 0.1580, Train Acc: 0.1857\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "557 Test Acc: 0.1460, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "558 Test Acc: 0.2150, Train Acc: 0.2357\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3892\n",
      "559 Test Acc: 0.1510, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "560 Test Acc: 0.2320, Train Acc: 0.2571\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "561 Test Acc: 0.1770, Train Acc: 0.2071\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "562 Test Acc: 0.2250, Train Acc: 0.2500\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "563 Test Acc: 0.1910, Train Acc: 0.2214\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "564 Test Acc: 0.1670, Train Acc: 0.2143\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "565 Test Acc: 0.1950, Train Acc: 0.2286\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "566 Test Acc: 0.1460, Train Acc: 0.1571\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "567 Test Acc: 0.1600, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8111, UL: 6.8111, Sup: 0.3890\n",
      "568 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3894\n",
      "569 Test Acc: 0.1560, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "570 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "571 Test Acc: 0.1500, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "572 Test Acc: 0.1470, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3890\n",
      "573 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3895\n",
      "574 Test Acc: 0.1470, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3889\n",
      "575 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3895\n",
      "576 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3890\n",
      "577 Test Acc: 0.1480, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3894\n",
      "578 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "579 Test Acc: 0.1510, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "580 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "581 Test Acc: 0.1580, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "582 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "583 Test Acc: 0.1560, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3893\n",
      "584 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "585 Test Acc: 0.1530, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3891\n",
      "586 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "587 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "588 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "589 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "590 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3890\n",
      "591 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3894\n",
      "592 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3890\n",
      "593 Test Acc: 0.1490, Train Acc: 0.1571\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "594 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3893\n",
      "595 Test Acc: 0.1830, Train Acc: 0.2143\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3890\n",
      "596 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3895\n",
      "597 Test Acc: 0.1980, Train Acc: 0.2286\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3889\n",
      "598 Test Acc: 0.1390, Train Acc: 0.1357\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3896\n",
      "599 Test Acc: 0.1780, Train Acc: 0.2357\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3890\n",
      "600 Test Acc: 0.1620, Train Acc: 0.1643\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3893\n",
      "601 Test Acc: 0.1130, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3892\n",
      "602 Test Acc: 0.1780, Train Acc: 0.1929\n",
      "\n",
      "Loss: 6.8113, UL: 6.8113, Sup: 0.3893\n",
      "603 Test Acc: 0.2120, Train Acc: 0.2429\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3891\n",
      "604 Test Acc: 0.1770, Train Acc: 0.1857\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3892\n",
      "605 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3892\n",
      "606 Test Acc: 0.1600, Train Acc: 0.1929\n",
      "\n",
      "Loss: 6.8112, UL: 6.8112, Sup: 0.3891\n",
      "607 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3894\n",
      "608 Test Acc: 0.1950, Train Acc: 0.2071\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3890\n",
      "609 Test Acc: 0.1470, Train Acc: 0.1714\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3895\n",
      "610 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3890\n",
      "611 Test Acc: 0.1450, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "612 Test Acc: 0.1450, Train Acc: 0.1500\n",
      "\n",
      "Loss: 6.8108, UL: 6.8108, Sup: 0.3893\n",
      "613 Test Acc: 0.1440, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8109, UL: 6.8109, Sup: 0.3891\n",
      "614 Test Acc: 0.1430, Train Acc: 0.1429\n",
      "\n",
      "Loss: 6.8110, UL: 6.8110, Sup: 0.3894\n",
      "615 Test Acc: 0.1480, Train Acc: 0.1571\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-cc37565a8947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuf_fts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mE_identity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_dropin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp_adj\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# SSL Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-78a37c88f6bb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq1, seq2, E_start, E_end, E_identity, E_dropin, adj, sparse, msk, samp_bias1, samp_bias2)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_identity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_dropin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamp_bias1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamp_bias2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mh_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_identity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_dropin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-178-e709a2f6250a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, E_start, E_end, E_identity, E_dropin)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# Edge Start+End Dropout for all layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mnum_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mdropout_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_edges\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mE_identity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mE_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-178-e709a2f6250a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# Edge Start+End Dropout for all layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mnum_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mdropout_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_edges\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mE_identity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mE_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "E_start, E_end, E_identity, E_dropin = get_cora_dataset()\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    idx = np.random.permutation(nb_nodes)\n",
    "    shuf_fts = features[idx, :]\n",
    "\n",
    "    \n",
    "    lbl_1 = torch.zeros(nb_nodes, dtype=torch.long)\n",
    "    lbl_2 = torch.ones(nb_nodes, dtype=torch.long)\n",
    "    lbl = torch.cat((lbl_1, lbl_2), 0)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        shuf_fts = shuf_fts.cuda()\n",
    "        lbl = lbl.cuda()\n",
    "    \n",
    "    logits = model(features, shuf_fts, E_start, E_end,E_identity, E_dropin, sp_adj if sparse else adj, sparse, None, None, None) \n",
    "    \n",
    "    # SSL Loss\n",
    "    sup_loss = xent(logits[idx_train], train_lbls)/5.\n",
    "\n",
    "    log = nn.functional.log_softmax(logits, dim=1)\n",
    "    ul_logits = torch.cat((torch.sum(log[:,:-1], dim=1).reshape(-1,1),log[:,-1].reshape(-1,1)),1)\n",
    "    ul_loss = nll(ul_logits, lbl)\n",
    "\n",
    "    loss = ul_loss\n",
    "    \n",
    "    print('Loss: %.4f, UL: %.4f, Sup: %.4f' % (loss.item(), ul_loss.item(), sup_loss.item()))\n",
    "\n",
    "    if loss < best:\n",
    "        best = loss\n",
    "        best_t = epoch\n",
    "        cnt_wait = 0\n",
    "        torch.save(model.state_dict(), 'best_dgi.pkl')\n",
    "    else:\n",
    "        cnt_wait += 1\n",
    "\n",
    "    if cnt_wait == patience:\n",
    "        print('Early stopping!')\n",
    "        break\n",
    "\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    model.eval()\n",
    "    logits = model(features, shuf_fts, E_start, E_end, E_identity, E_dropin, sp_adj if sparse else adj, sparse, None, None, None) \n",
    "    log = nn.functional.log_softmax(logits, dim=1)\n",
    "    preds = torch.argmax(log[:,:-1], dim=1)\n",
    "    train_acc = torch.sum(preds[idx_train] == train_lbls).float() / train_lbls.shape[0]\n",
    "    acc = torch.sum(preds[idx_test] == test_lbls).float() / test_lbls.shape[0]\n",
    "    print(\"%d Test Acc: %.4f, Train Acc: %.4f\\n\" % (epoch, acc.item(), train_acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 613th epoch\n"
     ]
    }
   ],
   "source": [
    "print('Loading {}th epoch'.format(best_t))\n",
    "model.load_state_dict(torch.load('best_dgi.pkl'))\n",
    "\n",
    "model.eval()\n",
    "logits = model(features, shuf_fts,  E_start, E_end, E_identity, E_dropin, sp_adj if sparse else adj, sparse, None, None, None) \n",
    "log = nn.functional.log_softmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.argmax(log, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0   15    0    0    0    0 2693]\n",
      "[   0    0    2    0    0    0    0 2706]\n"
     ]
    }
   ],
   "source": [
    "count = np.zeros(8, dtype=int)\n",
    "for i in range(2708):\n",
    "    count[preds[i]] += 1\n",
    "print(count)\n",
    "\n",
    "count = np.zeros(8, dtype=int)\n",
    "for i in range(2708,5416):\n",
    "    count[preds[i]] += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc tensor(0.1450, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "preds = torch.argmax(log[:,:-1], dim=1)\n",
    "acc = torch.sum(preds[idx_test] == test_lbls).float() / test_lbls.shape[0]\n",
    "print(\"Test Acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 613th epoch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1450, device='cuda:0')"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Loading {}th epoch'.format(best_t))\n",
    "model.load_state_dict(torch.load('best_dgi.pkl'))\n",
    "\n",
    "logits = model(features, shuf_fts,  E_start, E_end, E_identity, E_dropin, sp_adj if sparse else adj, sparse, None, None, None) \n",
    "log = nn.functional.log_softmax(logits, dim=1)\n",
    "preds = torch.argmax(log[:,:-1], dim=1)\n",
    "acc = torch.sum(preds[idx_test] == test_lbls).float() / test_lbls.shape[0]\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 613th epoch\n",
      "Test Acc: 0.5450, Train Acc: 0.6643\n",
      "Test Acc: 0.5400, Train Acc: 0.6571\n",
      "Test Acc: 0.5460, Train Acc: 0.6571\n",
      "Test Acc: 0.5250, Train Acc: 0.6429\n",
      "Test Acc: 0.5270, Train Acc: 0.6643\n",
      "Test Acc: 0.5540, Train Acc: 0.6714\n",
      "Test Acc: 0.5550, Train Acc: 0.6500\n",
      "Test Acc: 0.5300, Train Acc: 0.6643\n",
      "Test Acc: 0.5470, Train Acc: 0.6571\n",
      "Test Acc: 0.5390, Train Acc: 0.6429\n",
      "Test Acc: 0.5490, Train Acc: 0.6500\n",
      "Test Acc: 0.5590, Train Acc: 0.6786\n",
      "Test Acc: 0.5560, Train Acc: 0.6857\n",
      "Test Acc: 0.5430, Train Acc: 0.6643\n",
      "Test Acc: 0.5430, Train Acc: 0.6643\n",
      "Test Acc: 0.5260, Train Acc: 0.6429\n",
      "Test Acc: 0.5540, Train Acc: 0.6857\n",
      "Test Acc: 0.5460, Train Acc: 0.6429\n",
      "Test Acc: 0.5280, Train Acc: 0.6500\n",
      "Test Acc: 0.5400, Train Acc: 0.6429\n",
      "Test Acc: 0.5480, Train Acc: 0.6500\n",
      "Test Acc: 0.5380, Train Acc: 0.6571\n",
      "Test Acc: 0.5330, Train Acc: 0.6429\n",
      "Test Acc: 0.5540, Train Acc: 0.6643\n",
      "Test Acc: 0.5510, Train Acc: 0.6643\n",
      "Test Acc: 0.5500, Train Acc: 0.6643\n",
      "Test Acc: 0.5580, Train Acc: 0.6429\n",
      "Test Acc: 0.5400, Train Acc: 0.6571\n",
      "Test Acc: 0.5590, Train Acc: 0.6500\n",
      "Test Acc: 0.5530, Train Acc: 0.6714\n",
      "Test Acc: 0.5320, Train Acc: 0.6357\n",
      "Test Acc: 0.5270, Train Acc: 0.6500\n",
      "Test Acc: 0.5530, Train Acc: 0.6571\n",
      "Test Acc: 0.5380, Train Acc: 0.6786\n",
      "Test Acc: 0.5400, Train Acc: 0.6500\n",
      "Test Acc: 0.5260, Train Acc: 0.6500\n",
      "Test Acc: 0.5440, Train Acc: 0.6571\n",
      "Test Acc: 0.5400, Train Acc: 0.6714\n",
      "Test Acc: 0.5230, Train Acc: 0.6357\n",
      "Test Acc: 0.5360, Train Acc: 0.6714\n",
      "Test Acc: 0.5460, Train Acc: 0.6857\n",
      "Test Acc: 0.5380, Train Acc: 0.6571\n",
      "Test Acc: 0.5470, Train Acc: 0.6357\n",
      "Test Acc: 0.5510, Train Acc: 0.6714\n",
      "Test Acc: 0.5490, Train Acc: 0.6500\n",
      "Test Acc: 0.5440, Train Acc: 0.6929\n",
      "Test Acc: 0.5310, Train Acc: 0.6286\n",
      "Test Acc: 0.5500, Train Acc: 0.6857\n",
      "Test Acc: 0.5430, Train Acc: 0.6500\n",
      "Test Acc: 0.5200, Train Acc: 0.6429\n",
      "Average accuracy: tensor([0.5423], device='cuda:0')\n",
      "tensor(54.2280, device='cuda:0')\n",
      "tensor(1.0363, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('Loading {}th epoch'.format(best_t))\n",
    "model.load_state_dict(torch.load('best_dgi.pkl'))\n",
    "\n",
    "embeds, _ = model.embed(features,  E_start, E_end, E_identity, E_dropin, sp_adj if sparse else adj, sparse, None)\n",
    "train_embs = embeds[idx_train]\n",
    "val_embs = embeds[idx_val]\n",
    "test_embs = embeds[idx_test]\n",
    "\n",
    "train_lbls = torch.argmax(labels[0, idx_train], dim=1)\n",
    "val_lbls = torch.argmax(labels[0, idx_val], dim=1)\n",
    "test_lbls = torch.argmax(labels[0, idx_test], dim=1)\n",
    "\n",
    "tot = torch.zeros(1)\n",
    "tot = tot.cuda()\n",
    "\n",
    "accs = []\n",
    "\n",
    "for _ in range(50):\n",
    "    log = LogReg(hid_units, nb_classes)\n",
    "    opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)\n",
    "    log.cuda()\n",
    "\n",
    "    pat_steps = 0\n",
    "    best_acc = torch.zeros(1)\n",
    "    best_acc = best_acc.cuda()\n",
    "    for _ in range(100):\n",
    "        log.train()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = log(train_embs)\n",
    "        loss = xent(logits, train_lbls)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        \n",
    "    tlogits = log(train_embs)\n",
    "    tpreds = torch.argmax(tlogits, dim=1)\n",
    "    train_acc = torch.sum(tpreds == train_lbls).float() / train_lbls.shape[0]\n",
    "\n",
    "    logits = log(test_embs)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = torch.sum(preds == test_lbls).float() / test_lbls.shape[0]\n",
    "    print(\"Test Acc: %.4f, Train Acc: %.4f\" % (acc.item(), train_acc.item()))\n",
    "    accs.append(acc * 100)\n",
    "    tot += acc\n",
    "\n",
    "print('Average accuracy:', tot / 50)\n",
    "\n",
    "accs = torch.stack(accs)\n",
    "print(accs.mean())\n",
    "print(accs.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
