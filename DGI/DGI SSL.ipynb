{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import LogReg\n",
    "from utils import process\n",
    "\n",
    "dataset = 'cora'\n",
    "\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 2000\n",
    "patience = 100\n",
    "lr = 0.001\n",
    "l2_coef = 0.0\n",
    "drop_prob = 0.5\n",
    "hid_units = 256\n",
    "sparse = True\n",
    "nonlinearity = 'prelu' # special name to separate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = process.load_data(dataset)\n",
    "features, _ = process.preprocess_features(features)\n",
    "\n",
    "nb_nodes = features.shape[0]\n",
    "ft_size = features.shape[1]\n",
    "nb_classes = labels.shape[1]\n",
    "\n",
    "adj = process.normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "if sparse:\n",
    "    sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
    "else:\n",
    "    adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "\n",
    "features = torch.FloatTensor(features[np.newaxis])\n",
    "if not sparse:\n",
    "    adj = torch.FloatTensor(adj[np.newaxis])\n",
    "labels = torch.FloatTensor(labels[np.newaxis])\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_h):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.f_k = nn.Bilinear(n_h, n_h, 8)\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Bilinear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
    "        c_x = torch.unsqueeze(c, 1)\n",
    "        c_x = c_x.expand_as(h_pl)\n",
    "\n",
    "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x))\n",
    "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x))\n",
    "\n",
    "        if s_bias1 is not None:\n",
    "            sc_1 += s_bias1\n",
    "        if s_bias2 is not None:\n",
    "            sc_2 += s_bias2\n",
    "\n",
    "        logits = torch.cat((sc_1, sc_2), 0)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import GCN, AvgReadout\n",
    "\n",
    "class DGI(nn.Module):\n",
    "    def __init__(self, n_in, n_h, activation):\n",
    "        super(DGI, self).__init__()\n",
    "        self.gcn = GCN(n_in, n_h, activation)\n",
    "        self.gcn2 = GCN(n_h, n_h, activation)\n",
    "        self.gcn3 = GCN(n_h, n_h, activation)\n",
    "        \n",
    "        self.read = AvgReadout()\n",
    "\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "        self.disc = Discriminator(n_h)\n",
    "\n",
    "    def forward(self, seq1, seq2, adj, sparse, msk, samp_bias1, samp_bias2):\n",
    "        h_1 = self.gcn(seq1, adj, sparse)\n",
    "        h_1 = self.gcn2(h_1, adj, sparse)\n",
    "        h_1 = self.gcn3(h_1, adj, sparse)\n",
    "\n",
    "        c = self.read(h_1, msk)\n",
    "        c = self.sigm(c)\n",
    "\n",
    "        h_2 = self.gcn(seq2, adj, sparse)\n",
    "        h_2 = self.gcn2(h_2, adj, sparse)\n",
    "        h_2 = self.gcn3(h_2, adj, sparse)\n",
    "        \n",
    "        h_1 = F.dropout(h_1, drop_prob, training=self.training)\n",
    "        h_2 = F.dropout(h_2, drop_prob, training=self.training)\n",
    "        \n",
    "        ret = self.disc(c, h_1, h_2, samp_bias1, samp_bias2)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    # Detach the return variables\n",
    "    def embed(self, seq, adj, sparse, msk):\n",
    "        h_1 = self.gcn(seq, adj, sparse)\n",
    "        h_1 = self.gcn2(h_1, adj, sparse)\n",
    "        c = self.read(h_1, msk)\n",
    "\n",
    "        return h_1.detach(), c.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "model = DGI(ft_size, hid_units, nonlinearity)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_coef)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Using CUDA')\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    if sparse:\n",
    "        sp_adj = sp_adj.cuda()\n",
    "    else:\n",
    "        adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "    \n",
    "b_xent = nn.BCEWithLogitsLoss()\n",
    "xent = nn.CrossEntropyLoss()\n",
    "nll = nn.NLLLoss()\n",
    "cnt_wait = 0\n",
    "best = 1e9\n",
    "best_t = 0\n",
    "\n",
    "train_lbls = torch.argmax(labels[0, idx_train], dim=1)\n",
    "val_lbls = torch.argmax(labels[0, idx_val], dim=1)\n",
    "test_lbls = torch.argmax(labels[0, idx_test], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.5778, UL: 8.3178, Sup: 0.2600\n",
      "0 Test Acc: 0.4960, Train Acc: 0.6214\n",
      "\n",
      "Loss: 8.5747, UL: 8.3162, Sup: 0.2585\n",
      "1 Test Acc: 0.6390, Train Acc: 0.7357\n",
      "\n",
      "Loss: 8.5703, UL: 8.3136, Sup: 0.2567\n",
      "2 Test Acc: 0.5980, Train Acc: 0.8143\n",
      "\n",
      "Loss: 8.5632, UL: 8.3073, Sup: 0.2559\n",
      "3 Test Acc: 0.6130, Train Acc: 0.8500\n",
      "\n",
      "Loss: 8.5518, UL: 8.2978, Sup: 0.2539\n",
      "4 Test Acc: 0.6190, Train Acc: 0.8429\n",
      "\n",
      "Loss: 8.5337, UL: 8.2820, Sup: 0.2517\n",
      "5 Test Acc: 0.6690, Train Acc: 0.8714\n",
      "\n",
      "Loss: 8.5109, UL: 8.2624, Sup: 0.2485\n",
      "6 Test Acc: 0.6800, Train Acc: 0.8714\n",
      "\n",
      "Loss: 8.4791, UL: 8.2332, Sup: 0.2459\n",
      "7 Test Acc: 0.6920, Train Acc: 0.8929\n",
      "\n",
      "Loss: 8.4430, UL: 8.2030, Sup: 0.2399\n",
      "8 Test Acc: 0.6860, Train Acc: 0.8714\n",
      "\n",
      "Loss: 8.4373, UL: 8.1900, Sup: 0.2473\n",
      "9 Test Acc: 0.7140, Train Acc: 0.8786\n",
      "\n",
      "Loss: 8.3908, UL: 8.1600, Sup: 0.2308\n",
      "10 Test Acc: 0.7220, Train Acc: 0.9286\n",
      "\n",
      "Loss: 8.3198, UL: 8.0883, Sup: 0.2316\n",
      "11 Test Acc: 0.7440, Train Acc: 0.9214\n",
      "\n",
      "Loss: 8.3177, UL: 8.0777, Sup: 0.2400\n",
      "12 Test Acc: 0.7410, Train Acc: 0.9214\n",
      "\n",
      "Loss: 8.2113, UL: 7.9840, Sup: 0.2273\n",
      "13 Test Acc: 0.7620, Train Acc: 0.9214\n",
      "\n",
      "Loss: 8.1858, UL: 7.9630, Sup: 0.2227\n",
      "14 Test Acc: 0.7710, Train Acc: 0.9143\n",
      "\n",
      "Loss: 8.1268, UL: 7.8951, Sup: 0.2317\n",
      "15 Test Acc: 0.7580, Train Acc: 0.9143\n",
      "\n",
      "Loss: 8.0258, UL: 7.8000, Sup: 0.2257\n",
      "16 Test Acc: 0.7550, Train Acc: 0.9143\n",
      "\n",
      "Loss: 7.9976, UL: 7.7803, Sup: 0.2173\n",
      "17 Test Acc: 0.7400, Train Acc: 0.9143\n",
      "\n",
      "Loss: 7.9332, UL: 7.7078, Sup: 0.2254\n",
      "18 Test Acc: 0.7150, Train Acc: 0.9071\n",
      "\n",
      "Loss: 7.8031, UL: 7.5832, Sup: 0.2199\n",
      "19 Test Acc: 0.7220, Train Acc: 0.9143\n",
      "\n",
      "Loss: 7.7562, UL: 7.5399, Sup: 0.2163\n",
      "20 Test Acc: 0.7630, Train Acc: 0.9429\n",
      "\n",
      "Loss: 7.7404, UL: 7.5156, Sup: 0.2248\n",
      "21 Test Acc: 0.7780, Train Acc: 0.9214\n",
      "\n",
      "Loss: 7.6625, UL: 7.4463, Sup: 0.2162\n",
      "22 Test Acc: 0.7560, Train Acc: 0.9286\n",
      "\n",
      "Loss: 7.5580, UL: 7.3402, Sup: 0.2178\n",
      "23 Test Acc: 0.7420, Train Acc: 0.9357\n",
      "\n",
      "Loss: 7.5858, UL: 7.3632, Sup: 0.2226\n",
      "24 Test Acc: 0.7390, Train Acc: 0.9286\n",
      "\n",
      "Loss: 7.5482, UL: 7.3337, Sup: 0.2145\n",
      "25 Test Acc: 0.7270, Train Acc: 0.9286\n",
      "\n",
      "Loss: 7.4419, UL: 7.2251, Sup: 0.2168\n",
      "26 Test Acc: 0.7330, Train Acc: 0.9286\n",
      "\n",
      "Loss: 7.5284, UL: 7.3064, Sup: 0.2220\n",
      "27 Test Acc: 0.7710, Train Acc: 0.9214\n",
      "\n",
      "Loss: 7.5513, UL: 7.3374, Sup: 0.2139\n",
      "28 Test Acc: 0.7590, Train Acc: 0.9286\n",
      "\n",
      "Loss: 7.4570, UL: 7.2439, Sup: 0.2131\n",
      "29 Test Acc: 0.7190, Train Acc: 0.9000\n",
      "\n",
      "Loss: 7.4719, UL: 7.2515, Sup: 0.2204\n",
      "30 Test Acc: 0.7470, Train Acc: 0.9143\n",
      "\n",
      "Loss: 7.3775, UL: 7.1630, Sup: 0.2145\n",
      "31 Test Acc: 0.7700, Train Acc: 0.9286\n",
      "\n",
      "Loss: 7.4304, UL: 7.2165, Sup: 0.2139\n",
      "32 Test Acc: 0.7630, Train Acc: 0.9286\n",
      "\n",
      "Loss: 7.3155, UL: 7.0993, Sup: 0.2163\n",
      "33 Test Acc: 0.7430, Train Acc: 0.9071\n",
      "\n",
      "Loss: 7.3661, UL: 7.1460, Sup: 0.2201\n",
      "34 Test Acc: 0.7460, Train Acc: 0.9214\n",
      "\n",
      "Loss: 7.3356, UL: 7.1172, Sup: 0.2184\n",
      "35 Test Acc: 0.7390, Train Acc: 0.9143\n",
      "\n",
      "Loss: 7.3395, UL: 7.1230, Sup: 0.2165\n",
      "36 Test Acc: 0.7710, Train Acc: 0.9286\n",
      "\n",
      "Loss: 7.3156, UL: 7.0981, Sup: 0.2175\n",
      "37 Test Acc: 0.7700, Train Acc: 0.9357\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-222-8257cd653906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuf_fts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp_adj\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# SSL Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-220-01b0553f8af9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq1, seq2, adj, sparse, msk, samp_bias1, samp_bias2)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mh_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mh_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mh_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/graph-neural-networks/DGI/layers/gcn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq, adj, sparse)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Shape of seq: (batch, nodes, features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mseq_fts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_fts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epochs):\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    idx = np.random.permutation(nb_nodes)\n",
    "    shuf_fts = features[:, idx, :]\n",
    "\n",
    "    lbl_1 = torch.zeros(nb_nodes, dtype=torch.long)\n",
    "    lbl_2 = torch.ones(nb_nodes, dtype=torch.long)\n",
    "    lbl = torch.cat((lbl_1, lbl_2), 0)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        shuf_fts = shuf_fts.cuda()\n",
    "        lbl = lbl.cuda()\n",
    "    \n",
    "    logits = model(features, shuf_fts, sp_adj if sparse else adj, sparse, None, None, None) \n",
    "    \n",
    "    # SSL Loss\n",
    "    sup_loss = xent(logits[idx_train], train_lbls)/8.\n",
    "\n",
    "    log = nn.functional.log_softmax(logits, dim=1)\n",
    "    ul_logits = torch.cat((torch.sum(log[:,:-1], dim=1).reshape(-1,1),log[:,-1].reshape(-1,1)),1)\n",
    "    ul_loss = nll(ul_logits, lbl)\n",
    "\n",
    "    loss = sup_loss + ul_loss\n",
    "    \n",
    "    print('Loss: %.4f, UL: %.4f, Sup: %.4f' % (loss.item(), ul_loss.item(), sup_loss.item()))\n",
    "\n",
    "    if loss < best:\n",
    "        best = loss\n",
    "        best_t = epoch\n",
    "        cnt_wait = 0\n",
    "        torch.save(model.state_dict(), 'best_dgi.pkl')\n",
    "    else:\n",
    "        cnt_wait += 1\n",
    "\n",
    "    if cnt_wait == patience:\n",
    "        print('Early stopping!')\n",
    "        break\n",
    "\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    model.eval()\n",
    "    logits = model(features, shuf_fts, sp_adj if sparse else adj, sparse, None, None, None) \n",
    "    log = nn.functional.log_softmax(logits, dim=1)\n",
    "    preds = torch.argmax(log[:,:-1], dim=1)\n",
    "    train_acc = torch.sum(preds[idx_train] == train_lbls).float() / train_lbls.shape[0]\n",
    "    acc = torch.sum(preds[idx_test] == test_lbls).float() / test_lbls.shape[0]\n",
    "    print(\"%d Test Acc: %.4f, Train Acc: %.4f\\n\" % (epoch, acc.item(), train_acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 130th epoch\n"
     ]
    }
   ],
   "source": [
    "print('Loading {}th epoch'.format(best_t))\n",
    "model.load_state_dict(torch.load('best_dgi.pkl'))\n",
    "\n",
    "model.eval()\n",
    "logits = model(features, shuf_fts, sp_adj if sparse else adj, sparse, None, None, None) \n",
    "log = nn.functional.log_softmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.argmax(log, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[360 334 447 580 435 279 273   0]\n",
      "[   8    3    5   18   18    8    0 2648]\n"
     ]
    }
   ],
   "source": [
    "count = np.zeros(8, dtype=int)\n",
    "for i in range(2708):\n",
    "    count[preds[i]] += 1\n",
    "print(count)\n",
    "\n",
    "count = np.zeros(8, dtype=int)\n",
    "for i in range(2708,5416):\n",
    "    count[preds[i]] += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc tensor(0.7850, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "preds = torch.argmax(log[:,:-1], dim=1)\n",
    "acc = torch.sum(preds[idx_test] == test_lbls).float() / test_lbls.shape[0]\n",
    "print(\"Test Acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 130th epoch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7850, device='cuda:0')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Loading {}th epoch'.format(best_t))\n",
    "model.load_state_dict(torch.load('best_dgi.pkl'))\n",
    "\n",
    "logits = model(features, shuf_fts, sp_adj if sparse else adj, sparse, None, None, None) \n",
    "log = nn.functional.log_softmax(logits, dim=1)\n",
    "preds = torch.argmax(log[:,:-1], dim=1)\n",
    "acc = torch.sum(preds[idx_test] == test_lbls).float() / test_lbls.shape[0]\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 130th epoch\n",
      "Test Acc: 0.8090, Train Acc: 0.9286\n",
      "Test Acc: 0.8110, Train Acc: 0.9286\n",
      "Test Acc: 0.8070, Train Acc: 0.9214\n",
      "Test Acc: 0.8120, Train Acc: 0.9286\n",
      "Test Acc: 0.8110, Train Acc: 0.9214\n",
      "Test Acc: 0.8150, Train Acc: 0.9286\n",
      "Test Acc: 0.8110, Train Acc: 0.9286\n",
      "Test Acc: 0.8130, Train Acc: 0.9286\n",
      "Test Acc: 0.8130, Train Acc: 0.9286\n",
      "Test Acc: 0.8090, Train Acc: 0.9214\n",
      "Test Acc: 0.8100, Train Acc: 0.9286\n",
      "Test Acc: 0.8090, Train Acc: 0.9214\n",
      "Test Acc: 0.8140, Train Acc: 0.9286\n",
      "Test Acc: 0.8140, Train Acc: 0.9214\n",
      "Test Acc: 0.8140, Train Acc: 0.9214\n",
      "Test Acc: 0.8150, Train Acc: 0.9286\n",
      "Test Acc: 0.8150, Train Acc: 0.9214\n",
      "Test Acc: 0.8100, Train Acc: 0.9214\n",
      "Test Acc: 0.8100, Train Acc: 0.9286\n",
      "Test Acc: 0.8150, Train Acc: 0.9214\n",
      "Test Acc: 0.8080, Train Acc: 0.9286\n",
      "Test Acc: 0.8160, Train Acc: 0.9286\n",
      "Test Acc: 0.8140, Train Acc: 0.9214\n",
      "Test Acc: 0.8130, Train Acc: 0.9286\n",
      "Test Acc: 0.8080, Train Acc: 0.9214\n",
      "Test Acc: 0.8100, Train Acc: 0.9286\n",
      "Test Acc: 0.8130, Train Acc: 0.9286\n",
      "Test Acc: 0.8090, Train Acc: 0.9214\n",
      "Test Acc: 0.8080, Train Acc: 0.9286\n",
      "Test Acc: 0.8130, Train Acc: 0.9214\n",
      "Test Acc: 0.8120, Train Acc: 0.9214\n",
      "Test Acc: 0.8140, Train Acc: 0.9286\n",
      "Test Acc: 0.8110, Train Acc: 0.9286\n",
      "Test Acc: 0.8110, Train Acc: 0.9214\n",
      "Test Acc: 0.8160, Train Acc: 0.9214\n",
      "Test Acc: 0.8100, Train Acc: 0.9286\n",
      "Test Acc: 0.8100, Train Acc: 0.9214\n",
      "Test Acc: 0.8100, Train Acc: 0.9214\n",
      "Test Acc: 0.8120, Train Acc: 0.9286\n",
      "Test Acc: 0.8120, Train Acc: 0.9214\n",
      "Test Acc: 0.8110, Train Acc: 0.9214\n",
      "Test Acc: 0.8110, Train Acc: 0.9286\n",
      "Test Acc: 0.8130, Train Acc: 0.9286\n",
      "Test Acc: 0.8110, Train Acc: 0.9286\n",
      "Test Acc: 0.8150, Train Acc: 0.9286\n",
      "Test Acc: 0.8120, Train Acc: 0.9286\n",
      "Test Acc: 0.8090, Train Acc: 0.9286\n",
      "Test Acc: 0.8130, Train Acc: 0.9214\n",
      "Test Acc: 0.8070, Train Acc: 0.9214\n",
      "Test Acc: 0.8110, Train Acc: 0.9286\n",
      "Average accuracy: tensor([0.8116], device='cuda:0')\n",
      "tensor(81.1600, device='cuda:0')\n",
      "tensor(0.2373, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('Loading {}th epoch'.format(best_t))\n",
    "model.load_state_dict(torch.load('best_dgi.pkl'))\n",
    "\n",
    "embeds, _ = model.embed(features, sp_adj if sparse else adj, sparse, None)\n",
    "train_embs = embeds[0, idx_train]\n",
    "val_embs = embeds[0, idx_val]\n",
    "test_embs = embeds[0, idx_test]\n",
    "\n",
    "train_lbls = torch.argmax(labels[0, idx_train], dim=1)\n",
    "val_lbls = torch.argmax(labels[0, idx_val], dim=1)\n",
    "test_lbls = torch.argmax(labels[0, idx_test], dim=1)\n",
    "\n",
    "tot = torch.zeros(1)\n",
    "tot = tot.cuda()\n",
    "\n",
    "accs = []\n",
    "\n",
    "for _ in range(50):\n",
    "    log = LogReg(hid_units, nb_classes)\n",
    "    opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)\n",
    "    log.cuda()\n",
    "\n",
    "    pat_steps = 0\n",
    "    best_acc = torch.zeros(1)\n",
    "    best_acc = best_acc.cuda()\n",
    "    for _ in range(100):\n",
    "        log.train()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = log(train_embs)\n",
    "        loss = xent(logits, train_lbls)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        \n",
    "    tlogits = log(train_embs)\n",
    "    tpreds = torch.argmax(tlogits, dim=1)\n",
    "    train_acc = torch.sum(tpreds == train_lbls).float() / train_lbls.shape[0]\n",
    "\n",
    "    logits = log(test_embs)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = torch.sum(preds == test_lbls).float() / test_lbls.shape[0]\n",
    "    print(\"Test Acc: %.4f, Train Acc: %.4f\" % (acc.item(), train_acc.item()))\n",
    "    accs.append(acc * 100)\n",
    "    tot += acc\n",
    "\n",
    "print('Average accuracy:', tot / 50)\n",
    "\n",
    "accs = torch.stack(accs)\n",
    "print(accs.mean())\n",
    "print(accs.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
